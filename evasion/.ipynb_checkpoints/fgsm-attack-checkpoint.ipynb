{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evasion Attack: Fast Gradient Sign Method (FGSM)\n",
    "\n",
    "This notebook demonstrates how to perform the Fast Gradient Sign Method (FGSM) evasion attack against our deployed MNIST digit classifier. FGSM is one of the four evasion attack algorithms mentioned in the Security and Privacy of AI Knowledge Guide.\n",
    "\n",
    "## Overview of FGSM\n",
    "\n",
    "FGSM, introduced by Goodfellow et al. in 2014, is a one-step attack that creates adversarial examples by perturbing the input in the direction of the gradient of the loss function with respect to the input. The perturbation is scaled by a small epsilon value and the sign of the gradient is used to determine the direction of the perturbation.\n",
    "\n",
    "The mathematical formulation is:\n",
    "\n",
    "$$x_{adv} = x + \\epsilon \\cdot \\text{sign}(\\nabla_x J(\\theta, x, y))$$\n",
    "\n",
    "where:\n",
    "- $x_{adv}$ is the adversarial example\n",
    "- $x$ is the original input\n",
    "- $\\epsilon$ is the perturbation magnitude\n",
    "- $\\nabla_x J$ is the gradient of the loss function with respect to the input\n",
    "- $\\theta$ represents the model parameters\n",
    "- $y$ is the true label\n",
    "\n",
    "## Steps in this notebook:\n",
    "1. Import required libraries\n",
    "2. Set up connection to the deployed model\n",
    "3. Load test data\n",
    "4. Implement the FGSM attack\n",
    "5. Generate adversarial examples\n",
    "6. Evaluate attack success rate\n",
    "7. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up connection to the deployed model\n",
    "\n",
    "We'll create functions to interact with the deployed model API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# API endpoint of the deployed model\n",
    "API_URL = \"http://localhost:5000\"\n",
    "\n",
    "def get_model_info():\n",
    "    \"\"\"Get information about the deployed model\"\"\"\n",
    "    response = requests.get(f\"{API_URL}/info\")\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Error fetching model info: {response.text}\")\n",
    "\n",
    "def get_prediction(pixels):\n",
    "    \"\"\"Get prediction for an image from the deployed model\"\"\"\n",
    "    # Ensure pixels are flattened\n",
    "    pixels_flat = pixels.reshape(-1).tolist()\n",
    "    \n",
    "    # Prepare the request data\n",
    "    data = {\n",
    "        'pixels': pixels_flat\n",
    "    }\n",
    "    \n",
    "    # Send the request\n",
    "    response = requests.post(f\"{API_URL}/predict_raw\", json=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Error getting prediction: {response.text}\")\n",
    "\n",
    "def get_gradient(pixels, label):\n",
    "    \"\"\"Get the gradient of the loss with respect to the input from the deployed model\"\"\"\n",
    "    # Ensure pixels are flattened\n",
    "    pixels_flat = pixels.reshape(-1).tolist()\n",
    "    \n",
    "    # Prepare the request data\n",
    "    data = {\n",
    "        'pixels': pixels_flat,\n",
    "        'label': int(label)\n",
    "    }\n",
    "    \n",
    "    # Send the request\n",
    "    response = requests.post(f\"{API_URL}/get_gradient\", json=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        gradient = np.array(result['gradient'])\n",
    "        gradient_shape = result['gradient_shape']\n",
    "        return gradient.reshape(gradient_shape)\n",
    "    else:\n",
    "        raise Exception(f\"Error getting gradient: {response.text}\")\n",
    "        \n",
    "# Test the API connection\n",
    "try:\n",
    "    model_info = get_model_info()\n",
    "    print(\"Successfully connected to the model API!\")\n",
    "    print(f\"Model: {model_info['model_name']}\")\n",
    "    print(f\"Input shape: {model_info['input_shape']}\")\n",
    "    print(f\"Classes: {model_info['classes']}\")\n",
    "    print(f\"Test accuracy: {model_info['test_accuracy']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to the model API: {e}\")\n",
    "    print(\"Make sure the Flask server is running at http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Test Data\n",
    "\n",
    "We'll load the MNIST test dataset to use for our attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load MNIST dataset\n",
    "print(\"Loading MNIST dataset...\")\n",
    "(_, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data (normalize to 0-1)\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape to include channel dimension (MNIST is grayscale, so 1 channel)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "print(f\"Loaded {len(x_test)} test images\")\n",
    "print(f\"Data shape: {x_test.shape}\")\n",
    "print(f\"Labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Some Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display some test images\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"Label: {y_test[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Model Predictions\n",
    "\n",
    "Before attempting any attack, let's verify that the deployed model is correctly classifying the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test model predictions on a few original images\n",
    "num_test_samples = 5\n",
    "test_indices = np.random.choice(len(x_test), num_test_samples, replace=False)\n",
    "\n",
    "print(\"Testing model predictions on original images...\")\n",
    "for i, idx in enumerate(test_indices):\n",
    "    # Get the image and true label\n",
    "    image = x_test[idx]\n",
    "    true_label = y_test[idx]\n",
    "    \n",
    "    # Get model prediction\n",
    "    result = get_prediction(image)\n",
    "    predicted_class = result['predicted_class']\n",
    "    confidence = result['confidence']\n",
    "    \n",
    "    print(f\"Sample {i+1}: True label = {true_label}, Predicted class = {predicted_class}, Confidence = {confidence:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement the FGSM Attack\n",
    "\n",
    "Now, let's implement the Fast Gradient Sign Method (FGSM) attack. The key steps are:\n",
    "1. Calculate the gradient of the loss with respect to the input image\n",
    "2. Take the sign of the gradient\n",
    "3. Perturb the input in the direction of the sign of the gradient, scaled by epsilon\n",
    "4. Clip the perturbed image to ensure it's within the valid range (0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def fgsm_attack(image, label, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Generates an adversarial example using the Fast Gradient Sign Method\n",
    "    \n",
    "    Args:\n",
    "        image: Input image to be perturbed (numpy array)\n",
    "        label: True label of the image\n",
    "        epsilon: Perturbation magnitude\n",
    "        \n",
    "    Returns:\n",
    "        Adversarial example\n",
    "    \"\"\"\n",
    "    # Step 1: Get the gradient of the loss with respect to the input\n",
    "    gradient = get_gradient(image, label)\n",
    "    \n",
    "    # Step 2: Take the sign of the gradient\n",
    "    sign_gradient = np.sign(gradient)\n",
    "    \n",
    "    # Step 3: Perturb the input in the direction of the sign of the gradient\n",
    "    perturbed_image = image + epsilon * sign_gradient\n",
    "    \n",
    "    # Step 4: Clip the perturbed image to ensure it's within the valid range (0-1)\n",
    "    perturbed_image = np.clip(perturbed_image, 0, 1)\n",
    "    \n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Adversarial Examples\n",
    "\n",
    "Now let's generate adversarial examples for a subset of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define the number of samples to attack and the perturbation strength\n",
    "num_samples = 20\n",
    "epsilon = 0.2  # Perturbation magnitude\n",
    "\n",
    "# Randomly select samples from the test set\n",
    "sample_indices = np.random.choice(len(x_test), num_samples, replace=False)\n",
    "\n",
    "# Lists to store results\n",
    "original_images = []\n",
    "adversarial_images = []\n",
    "original_preds = []\n",
    "adversarial_preds = []\n",
    "true_labels = []\n",
    "perturbations = []\n",
    "\n",
    "print(f\"Generating adversarial examples using FGSM with epsilon={epsilon}...\")\n",
    "\n",
    "# Generate adversarial examples\n",
    "for i, idx in enumerate(tqdm(sample_indices)):\n",
    "    # Get the original image and label\n",
    "    original_image = x_test[idx]\n",
    "    true_label = y_test[idx]\n",
    "    \n",
    "    # Get original prediction\n",
    "    try:\n",
    "        orig_result = get_prediction(original_image)\n",
    "        original_pred = orig_result['predicted_class']\n",
    "        \n",
    "        # Only attack correctly classified images\n",
    "        if int(original_pred) == true_label:\n",
    "            # Generate adversarial example\n",
    "            adversarial_image = fgsm_attack(original_image, true_label, epsilon)\n",
    "            \n",
    "            # Calculate perturbation\n",
    "            perturbation = adversarial_image - original_image\n",
    "            \n",
    "            # Get adversarial prediction\n",
    "            adv_result = get_prediction(adversarial_image)\n",
    "            adversarial_pred = adv_result['predicted_class']\n",
    "            \n",
    "            # Store results\n",
    "            original_images.append(original_image)\n",
    "            adversarial_images.append(adversarial_image)\n",
    "            original_preds.append(original_pred)\n",
    "            adversarial_preds.append(adversarial_pred)\n",
    "            true_labels.append(true_label)\n",
    "            perturbations.append(perturbation)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample {idx}: {e}\")\n",
    "        continue\n",
    "        \n",
    "# Convert lists to numpy arrays for easier handling\n",
    "original_images = np.array(original_images)\n",
    "adversarial_images = np.array(adversarial_images)\n",
    "original_preds = np.array(original_preds)\n",
    "adversarial_preds = np.array(adversarial_preds)\n",
    "true_labels = np.array(true_labels)\n",
    "perturbations = np.array(perturbations)\n",
    "\n",
    "print(f\"Generated {len(adversarial_images)} adversarial examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Attack Success Rate\n",
    "\n",
    "Let's evaluate how successful our FGSM attack was by calculating the attack success rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate attack success rate\n",
    "successful_attacks = (adversarial_preds != true_labels)\n",
    "attack_success_rate = np.mean(successful_attacks) * 100\n",
    "\n",
    "print(f\"Attack Success Rate: {attack_success_rate:.2f}%\")\n",
    "print(f\"Number of successful attacks: {np.sum(successful_attacks)} out of {len(adversarial_images)}\")\n",
    "\n",
    "# Calculate average perturbation magnitude\n",
    "avg_perturbation_l2 = np.mean([np.linalg.norm(p) for p in perturbations])\n",
    "avg_perturbation_linf = np.mean([np.max(np.abs(p)) for p in perturbations])\n",
    "\n",
    "print(f\"Average L2 perturbation magnitude: {avg_perturbation_l2:.4f}\")\n",
    "print(f\"Average L∞ perturbation magnitude: {avg_perturbation_linf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results\n",
    "\n",
    "Now, let's visualize the original images, perturbations, and adversarial examples to better understand the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display a subset of the results\n",
    "num_to_display = min(10, len(adversarial_images))\n",
    "\n",
    "plt.figure(figsize=(15, 5 * num_to_display))\n",
    "\n",
    "for i in range(num_to_display):\n",
    "    # Original image\n",
    "    plt.subplot(num_to_display, 3, 3*i + 1)\n",
    "    plt.imshow(original_images[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"Original\\nTrue: {true_labels[i]}, Pred: {original_preds[i]}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Perturbation\n",
    "    plt.subplot(num_to_display, 3, 3*i + 2)\n",
    "    plt.imshow(perturbations[i].reshape(28, 28), cmap='RdBu_r')\n",
    "    plt.title(f\"Perturbation\\nEpsilon: {epsilon}\")\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Adversarial image\n",
    "    plt.subplot(num_to_display, 3, 3*i + 3)\n",
    "    plt.imshow(adversarial_images[i].reshape(28, 28), cmap='gray')\n",
    "    color = 'green' if successful_attacks[i] else 'red'\n",
    "    result = 'Success' if successful_attacks[i] else 'Failure'\n",
    "    plt.title(f\"Adversarial\\nPred: {adversarial_preds[i]}\\nAttack: {result}\", color=color)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Epsilon on Attack Success Rate\n",
    "\n",
    "Let's investigate how the perturbation magnitude (epsilon) affects the attack success rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test various epsilon values\n",
    "epsilon_values = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "test_samples = 10  # Use a smaller subset for efficiency\n",
    "success_rates = []\n",
    "\n",
    "# Sample indices for testing\n",
    "test_indices = np.random.choice(len(x_test), test_samples, replace=False)\n",
    "\n",
    "print(\"Testing attack success rate for different epsilon values...\")\n",
    "for eps in tqdm(epsilon_values):\n",
    "    successful = 0\n",
    "    total = 0\n",
    "    \n",
    "    for idx in test_indices:\n",
    "        original_image = x_test[idx]\n",
    "        true_label = y_test[idx]\n",
    "        \n",
    "        try:\n",
    "            # Get original prediction\n",
    "            orig_result = get_prediction(original_image)\n",
    "            original_pred = orig_result['predicted_class']\n",
    "            \n",
    "            # Only attack correctly classified images\n",
    "            if int(original_pred) == true_label:\n",
    "                # Generate adversarial example\n",
    "                adversarial_image = fgsm_attack(original_image, true_label, eps)\n",
    "                \n",
    "                # Get adversarial prediction\n",
    "                adv_result = get_prediction(adversarial_image)\n",
    "                adversarial_pred = adv_result['predicted_class']\n",
    "                \n",
    "                # Count successful attacks\n",
    "                if int(adversarial_pred) != true_label:\n",
    "                    successful += 1\n",
    "                total += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx} with epsilon={eps}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate and store success rate\n",
    "    if total > 0:\n",
    "        success_rate = (successful / total) * 100\n",
    "        success_rates.append(success_rate)\n",
    "        print(f\"Epsilon: {eps}, Success Rate: {success_rate:.2f}%\")\n",
    "    else:\n",
    "        success_rates.append(0)\n",
    "        print(f\"Epsilon: {eps}, No valid samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot the relationship between epsilon and attack success rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epsilon_values, success_rates, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epsilon (Perturbation Magnitude)', fontsize=12)\n",
    "plt.ylabel('Attack Success Rate (%)', fontsize=12)\n",
    "plt.title('Effect of Epsilon on FGSM Attack Success Rate', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(epsilon_values)\n",
    "plt.ylim([0, 105])\n",
    "\n",
    "# Add data labels\n",
    "for i, rate in enumerate(success_rates):\n",
    "    plt.text(epsilon_values[i], rate + 2, f\"{rate:.1f}%\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have demonstrated the Fast Gradient Sign Method (FGSM) evasion attack against a deployed MNIST digit classifier. We have shown how to:\n",
    "\n",
    "1. Connect to a deployed model API\n",
    "2. Implement the FGSM attack algorithm\n",
    "3. Generate adversarial examples\n",
    "4. Evaluate the attack success rate\n",
    "5. Visualize the results\n",
    "6. Study the impact of the perturbation magnitude (epsilon) on the attack effectiveness\n",
    "\n",
    "Key findings:\n",
    "- The FGSM attack can successfully fool the model with relatively small perturbations\n",
    "- The attack success rate increases with larger epsilon values\n",
    "- There is a trade-off between the perturbation visibility (how noticeable the changes are) and attack success rate\n",
    "\n",
    "These results demonstrate the vulnerability of neural networks to adversarial examples, even with simple one-step attack methods like FGSM. This highlights the importance of developing robust models that can withstand such attacks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
