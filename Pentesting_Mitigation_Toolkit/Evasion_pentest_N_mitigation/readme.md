# AI/ML Evasion Attack Penetration Testing Tool

This tool implements evasion attacks against machine learning models as described in the CyBOK Security and Privacy of AI Knowledge Guide. The tool focuses on testing model robustness against adversarial examples using state-of-the-art attack methods.

## Overview

This penetration testing framework implements three major evasion attack techniques:

1. **Fast Gradient Sign Method (FGSM)** - A simple yet effective attack that generates adversarial examples by taking a single step in the direction that maximizes the loss.

2. **Projected Gradient Descent (PGD)** - A stronger iterative attack that takes multiple steps, projecting back to an epsilon ball around the original input at each step.

3. **Carlini & Wagner (C&W) Attack** - A powerful optimization-based attack that generates adversarial examples with minimal distortion.

## Requirements

- Python 3.7+
- TensorFlow 2.x
- NumPy
- Matplotlib
- scikit-learn

You can install the required packages using:

```bash
pip install tensorflow numpy matplotlib scikit-learn
```

## Usage

### Running the Tool

To run the penetration testing tool, use the following command:

```bash
python evasion_attack_pentest.py --model-path your_model.h5 --output-dir results
```

For a quick demonstration, you can use the provided script:

```bash
bash run_pentest.sh
```

This script will:
1. Create a results directory
2. Train a simple MNIST model if one doesn't exist
3. Run the evasion attack penetration testing tool
4. Generate a comprehensive security assessment report

### Command-line Arguments

The tool accepts the following command-line arguments:

- `--model-path`: Path to the target model file (.h5) [required]
- `--output-dir`: Directory to save results (default: results)
- `--fgsm-epsilon`: Epsilon for FGSM attack (default: 0.1)
- `--pgd-epsilon`: Epsilon for PGD attack (default: 0.1)
- `--pgd-alpha`: Step size for PGD attack (default: 0.01)
- `--pgd-iterations`: Iterations for PGD attack (default: 40)
- `--cw-confidence`: Confidence for C&W attack (default: 0)
- `--cw-iterations`: Iterations for C&W attack (default: 100)
- `--skip-cw`: Skip C&W attack (it's computationally expensive)

### Output

The tool generates the following outputs in the specified output directory:

1. **Adversarial Example Visualizations**: PNG files showing original images, adversarial examples, and the differences between them.
2. **HTML Report**: A comprehensive security assessment report with findings and recommendations.
3. **JSON Report**: Raw attack results and metrics in JSON format.

## Interpreting Results

### Attack Success Rate

The primary metric for evaluating an attack's effectiveness is the **Attack Success Rate (ASR)**, which measures the percentage of adversarial examples that successfully cause misclassification.

### Risk Assessment

The tool categorizes risks based on the attack success rate:

- **High Risk** (>70% success rate): Critical vulnerability requiring immediate attention
- **Medium Risk** (30-70% success rate): Significant vulnerability requiring remediation
- **Low Risk** (<30% success rate): Minor vulnerability that should be monitored

### Recommendations

Based on the findings, the tool generates tailored recommendations for improving model robustness, such as:

- Implementing adversarial training
- Adding input preprocessing defenses
- Using ensemble methods
- Considering certified defenses

## Understanding the Attacks

### FGSM (Fast Gradient Sign Method)

FGSM perturbs the input in the direction that maximizes the loss:

```
x_adv = x + ε * sign(∇_x J(θ, x, y))
```

Where:
- x_adv is the adversarial example
- x is the original input
- ε is the perturbation magnitude
- J is the loss function
- θ represents the model parameters
- y is the true label

### PGD (Projected Gradient Descent)

PGD is an iterative version of FGSM that takes multiple small steps:

```
x_adv^(t+1) = Proj_ε(x_adv^(t) + α * sign(∇_x J(θ, x_adv^(t), y)))
```

Where:
- x_adv^(t) is the adversarial example at iteration t
- α is the step size
- Proj_ε projects the perturbation back to an ε-ball around the original input

### C&W (Carlini & Wagner)

C&W formulates adversarial example generation as an optimization problem:

```
minimize ||δ||_p + c * f(x + δ)
```

Where:
- δ is the perturbation
- c is a constant balancing the two objectives
- f is a function that is negative when the model misclassifies x + δ

## Security Considerations

This tool is for educational and authorized testing purposes only. Always ensure you have proper authorization before conducting security testing on any system.

## References

1. CyBOK Security and Privacy of AI Knowledge Guide
2. Goodfellow, I. J., Shlens, J., & Szegedy, C. (2015). Explaining and Harnessing Adversarial Examples.
3. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., & Vladu, A. (2018). Towards Deep Learning Models Resistant to Adversarial Attacks.
4. Carlini, N., & Wagner, D. (2017). Towards Evaluating the Robustness of Neural Networks.
