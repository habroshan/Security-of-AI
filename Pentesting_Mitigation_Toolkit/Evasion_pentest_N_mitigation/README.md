# AI/ML Security Penetration Testing - Evasion Attacks

This toolkit demonstrates how to perform evasion attacks (adversarial examples) against machine learning models and how to implement defenses against these attacks. The implementation is based on the concepts outlined in the [CyBOK Security and Privacy of AI Knowledge Guide](https://www.cybok.org/).

## Table of Contents

1. [Introduction](#introduction)
2. [Installation](#installation)
3. [Usage](#usage)
4. [Attack Methods](#attack-methods)
5. [Defense Strategies](#defense-strategies)
6. [Example Results](#example-results)
7. [Educational Purpose](#educational-purpose)

## Introduction

Evasion attacks are a type of adversarial attack that involves crafting inputs to cause a machine learning model to make incorrect predictions. These attacks occur during the inference phase and do not require modifications to the training process. As outlined in the CyBOK Security and Privacy of AI Knowledge Guide (Section 3.1), evasion attacks work by solving an optimization problem to find perturbations that, when added to input data, cause the model to produce incorrect outputs.

This toolkit demonstrates two common evasion attack methods:

1. **Fast Gradient Sign Method (FGSM)**: A single-step attack that uses the sign of the gradient to create adversarial examples.
2. **Projected Gradient Descent (PGD)**: An iterative attack that is more powerful than FGSM, making multiple small steps in the direction of the gradient.

It also implements three defense strategies:

1. **Adversarial Training**: Training models on a mix of clean and adversarial examples.
2. **Input Preprocessing**: Applying transformations to inputs to remove or reduce adversarial perturbations.
3. **Gradient Masking**: Obfuscating gradients to make it harder for attackers to compute effective perturbations.

## Installation

### Prerequisites

- Python 3.6 or higher
- TensorFlow 2.x
- NumPy
- Matplotlib

### Setup

1. Clone this repository:
   ```bash
   git clone https://github.com/yourusername/ai-security-pentest.git
   cd ai-security-pentest
   ```

2. Install the required packages:
   ```bash
   pip install tensorflow numpy matplotlib
   ```

3. Make the run script executable:
   ```bash
   chmod +x run_evasion_demo.sh
   ```

## Usage

You can run the demo in two ways:

### Interactive Demo Script

The easiest way to get started is to use the provided shell script:

```bash
./run_evasion_demo.sh
```

This interactive script will guide you through:
- Training a new model
- Running FGSM attacks with different parameters
- Running PGD attacks with different parameters
- Comparing different defense strategies

### Direct Python Commands

For more advanced usage, you can directly use the Python script:

```bash
# Train a new model
python evasion_attack_demo.py --train

# Run FGSM attack with default parameters
python evasion_attack_demo.py --attack fgsm --epsilon 0.1 --defense adversarial_training

# Run PGD attack with custom parameters
python evasion_attack_demo.py --attack pgd --epsilon 0.2 --defense input_preprocessing

# Only evaluate defense without implementing it
python evasion_attack_demo.py --attack fgsm --epsilon 0.1 --evaluate-only
```

## Attack Methods

### Fast Gradient Sign Method (FGSM)

FGSM is a simple, one-step attack that perturbs the input in the direction of the gradient of the loss with respect to the input:

```
x_adv = x + ε * sign(∇ₓJ(x, y))
```

Where:
- x is the original input
- x_adv is the adversarial example
- ε is the perturbation size
- ∇ₓJ(x, y) is the gradient of the loss with respect to the input
- sign() is the sign function

### Projected Gradient Descent (PGD)

PGD is an iterative attack that takes multiple small steps in the direction of the gradient:

```
x⁰ = x
x^(t+1) = Proj(x^t + α * sign(∇ₓJ(x^t, y)))
```

Where:
- x⁰ is the original input
- x^t is the adversarial example at step t
- α is the step size
- Proj() projects the perturbation back onto the ε-ball around x

PGD is generally more effective than FGSM but requires more computational resources.

## Defense Strategies

### Adversarial Training

Adversarial training involves incorporating adversarial examples into the training process. The model is trained on both clean and adversarial examples, making it more robust to adversarial attacks.

The implementation in this toolkit:
1. Generates adversarial examples for each training batch
2. Combines clean and adversarial examples
3. Trains the model on the combined data

Adversarial training is one of the most effective defenses but can increase training time and may slightly reduce performance on clean data.

### Input Preprocessing

Input preprocessing applies transformations to inputs before feeding them to the model. These transformations can reduce or remove adversarial perturbations.

The implementation includes:
1. Gaussian noise addition
2. Spatial smoothing (average pooling)
3. Value clipping

This defense is computationally efficient but may be vulnerable to adaptive attacks where the adversary is aware of the preprocessing.

### Gradient Masking (Defensive Distillation)

Gradient masking works by obfuscating the gradients, making it harder for attackers to compute effective perturbations. Defensive distillation trains a second model on the soft outputs of the first model.

The implementation:
1. Trains an initial model
2. Uses the soft outputs (probabilities) of the initial model as training labels
3. Trains a second model on these soft labels

While gradient masking can appear effective against gradient-based attacks, it may be vulnerable to black-box attacks or attacks that don't rely on gradients.

## Example Results

After running an attack and defense, the toolkit will generate:

1. Visualizations of original and adversarial examples
2. Quantitative results on clean and adversarial accuracy
3. Assessment of defense effectiveness
4. Recommendations for improving the defense

Example output:
```
EVASION ATTACK DEMO SUMMARY
========================================================================
Attack: FGSM, Epsilon: 0.1
Defense: adversarial_training

Results:
  Clean Accuracy (Original): 0.9921
  Clean Accuracy (Defended): 0.9876
  Adversarial Accuracy (Original): 0.0832
  Adversarial Accuracy (Defended): 0.8943
  Improvement: 0.8111 (81.1%)
  Adaptive Attack Accuracy: 0.6754

Defense Effectiveness: HIGH

Recommendations:
  - Increase the diversity of adversarial examples during training
  - Try ensemble adversarial training with multiple attack types
  - Consider combining with input preprocessing for better robustness
```

## Educational Purpose

This toolkit is provided for educational purposes to demonstrate the concepts discussed in the CyBOK Security and Privacy of AI Knowledge Guide. Some key learning points:

1. **Understanding Attack Mechanisms**: Learn how adversarial examples work and how they exploit model vulnerabilities.
2. **Defense Effectiveness**: Compare different defense strategies and understand their strengths and limitations.
3. **Security-Performance Trade-offs**: Observe how defenses can affect model performance on clean data.
4. **Adaptive Attacks**: See how attackers can adapt to defenses, highlighting the need for robust, multi-layered security.

For practical applications in security-critical environments, additional measures beyond what is demonstrated here would be necessary.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- This toolkit is based on the concepts outlined in the CyBOK Security and Privacy of AI Knowledge Guide.
- The implementations of attack and defense methods are inspired by research in the field of adversarial machine learning.
