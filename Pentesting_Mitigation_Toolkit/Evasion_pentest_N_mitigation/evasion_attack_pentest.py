#!/usr/bin/env python3
"""
MNIST Evasion Attack Penetration Testing Tool
==============================================

This script demonstrates how to perform evasion attacks against an ML model
as described in the CyBOK Security and Privacy of AI Knowledge Guide.

The tool performs the following evasion attacks:
1. Fast Gradient Sign Method (FGSM)
2. Projected Gradient Descent (PGD)
3. Carlini & Wagner (C&W) Attack

Author: CyBOK Security Assessment Team
License: Educational Use Only
"""

import os
import sys
import argparse
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from datetime import datetime
import json
import requests
from sklearn.metrics import accuracy_score, classification_report

# Set TensorFlow logging level
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings

# ANSI colors for terminal output
class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    GREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'

def print_banner():
    """Print the tool banner"""
    banner = f"""
    {Colors.BLUE}╔══════════════════════════════════════════════════════════════╗
    ║ {Colors.HEADER}MNIST Evasion Attack Penetration Testing Tool{Colors.BLUE}                ║
    ║                                                              ║
    ║ {Colors.WARNING}Based on CyBOK Security and Privacy of AI Knowledge Guide{Colors.BLUE}    ║
    ╚══════════════════════════════════════════════════════════════╝{Colors.ENDC}
    """
    print(banner)
    print(f"{Colors.WARNING}This tool is for educational and authorized testing purposes only.{Colors.ENDC}\n")

class EvasionAttackPenTest:
    """Class to perform evasion attack penetration testing"""
    
    def __init__(self, model_path=None, api_url=None, output_dir="results"):
        """Initialize the penetration testing environment"""
        self.model_path = model_path
        self.api_url = api_url
        self.output_dir = output_dir
        self.model = None
        self.x_test = None
        self.y_test = None
        self.class_names = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]  # Default for MNIST
        self.attack_results = {}
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
    def load_model(self):
        """Load the target model"""
        print(f"{Colors.BLUE}[*] Loading model from {self.model_path}...{Colors.ENDC}")
        
        try:
            self.model = tf.keras.models.load_model(self.model_path)
            print(f"{Colors.GREEN}[✓] Model loaded successfully{Colors.ENDC}")
            
            # Print model summary
            self.model.summary()
            
            return True
        except Exception as e:
            print(f"{Colors.FAIL}[✗] Failed to load model: {str(e)}{Colors.ENDC}")
            return False
    
    def load_test_data(self):
        """Load MNIST test data"""
        print(f"{Colors.BLUE}[*] Loading MNIST test data...{Colors.ENDC}")
        
        try:
            # Load MNIST dataset
            (_, _), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
            
            # Preprocess the data
            x_test = x_test.astype('float32') / 255.0
            x_test = x_test.reshape(-1, 28, 28, 1)
            
            # Convert to one-hot encoding for attacks
            self.y_test_categorical = tf.keras.utils.to_categorical(y_test, 10)
            
            # Keep original labels
            self.y_test = y_test
            
            # Keep only a subset for efficiency
            subset_size = 100  # Using 100 test samples is sufficient for demonstration
            self.x_test = x_test[:subset_size]
            self.y_test = y_test[:subset_size]
            self.y_test_categorical = self.y_test_categorical[:subset_size]
            
            print(f"{Colors.GREEN}[✓] Loaded {len(self.x_test)} test samples{Colors.ENDC}")
            
            return True
        except Exception as e:
            print(f"{Colors.FAIL}[✗] Failed to load test data: {str(e)}{Colors.ENDC}")
            return False
    
    def evaluate_baseline(self):
        """Evaluate the model on clean test data to establish baseline performance"""
        print(f"{Colors.BLUE}[*] Evaluating baseline model performance...{Colors.ENDC}")
        
        predictions = self.model.predict(self.x_test)
        predicted_classes = np.argmax(predictions, axis=1)
        true_classes = self.y_test
        
        accuracy = accuracy_score(true_classes, predicted_classes)
        
        print(f"{Colors.GREEN}[✓] Baseline accuracy: {accuracy:.4f}{Colors.ENDC}")
        print("\nClassification Report (Baseline):")
        print(classification_report(true_classes, predicted_classes))
        
        return accuracy, predicted_classes
    
    def fgsm_attack(self, epsilon=0.1, batch_size=32):
        """
        Perform Fast Gradient Sign Method (FGSM) attack
        
        Args:
            epsilon: Perturbation magnitude
            batch_size: Batch size for attack
            
        Returns:
            Dictionary containing attack results
        """
        print(f"{Colors.BLUE}[*] Performing FGSM attack with epsilon={epsilon}...{Colors.ENDC}")
        
        # Create TensorFlow variables
        x_test_tf = tf.convert_to_tensor(self.x_test)
        y_test_tf = tf.convert_to_tensor(self.y_test_categorical)
        
        # Store original and adversarial examples
        original_examples = []
        adversarial_examples = []
        original_predictions = []
        adversarial_predictions = []
        
        # Get baseline predictions
        baseline_preds = self.model.predict(self.x_test)
        baseline_pred_classes = np.argmax(baseline_preds, axis=1)
        
        # FGSM attack
        success_count = 0
        
        for i in range(0, len(self.x_test), batch_size):
            batch_end = min(i + batch_size, len(self.x_test))
            x_batch = x_test_tf[i:batch_end]
            y_batch = y_test_tf[i:batch_end]
            
            # Perform FGSM attack
            with tf.GradientTape() as tape:
                tape.watch(x_batch)
                predictions = self.model(x_batch)
                loss = tf.keras.losses.categorical_crossentropy(y_batch, predictions)
            
            # Get gradients
            gradients = tape.gradient(loss, x_batch)
            
            # Create adversarial examples
            signed_gradients = tf.sign(gradients)
            adversarial_batch = x_batch + epsilon * signed_gradients
            adversarial_batch = tf.clip_by_value(adversarial_batch, 0, 1)
            
            # Get predictions for adversarial examples
            adv_predictions = self.model.predict(adversarial_batch)
            adv_pred_classes = np.argmax(adv_predictions, axis=1)
            
            # Check if the attack was successful for each example
            for j in range(len(adv_pred_classes)):
                if baseline_pred_classes[i+j] != adv_pred_classes[j]:
                    success_count += 1
                
                # Add to original and adversarial examples (for visualization)
                if len(original_examples) < 10:  # Only keep 10 examples for visualization
                    original_examples.append(self.x_test[i+j])
                    adversarial_examples.append(adversarial_batch[j].numpy())
                    original_predictions.append(int(baseline_pred_classes[i+j]))
                    adversarial_predictions.append(int(adv_pred_classes[j]))
        
        # Calculate attack success rate
        success_rate = success_count / len(self.x_test)
        
        print(f"{Colors.GREEN}[✓] FGSM attack completed{Colors.ENDC}")
        print(f"{Colors.GREEN}[✓] Attack success rate: {success_rate:.4f}{Colors.ENDC}")
        
        # Save results
        result = {
            "attack_type": "FGSM",
            "parameters": {
                "epsilon": epsilon
            },
            "success_rate": success_rate,
            "original_examples": [x.tolist() for x in original_examples],
            "adversarial_examples": [x.tolist() for x in adversarial_examples],
            "original_predictions": original_predictions,
            "adversarial_predictions": adversarial_predictions
        }
        
        self.attack_results["fgsm"] = result
        
        # Create and save visualization
        self._save_adversarial_examples_visualization(
            original_examples, adversarial_examples,
            original_predictions, adversarial_predictions,
            "FGSM"
        )
        
        return result
    
    def pgd_attack(self, epsilon=0.1, alpha=0.01, iterations=40, batch_size=32):
        """
        Perform Projected Gradient Descent (PGD) attack
        
        Args:
            epsilon: Perturbation magnitude
            alpha: Step size
            iterations: Number of iterations
            batch_size: Batch size for attack
            
        Returns:
            Dictionary containing attack results
        """
        print(f"{Colors.BLUE}[*] Performing PGD attack with epsilon={epsilon}, alpha={alpha}, iterations={iterations}...{Colors.ENDC}")
        
        # Create TensorFlow variables
        x_test_tf = tf.convert_to_tensor(self.x_test)
        y_test_tf = tf.convert_to_tensor(self.y_test_categorical)
        
        # Store original and adversarial examples
        original_examples = []
        adversarial_examples = []
        original_predictions = []
        adversarial_predictions = []
        
        # Get baseline predictions
        baseline_preds = self.model.predict(self.x_test)
        baseline_pred_classes = np.argmax(baseline_preds, axis=1)
        
        # PGD attack
        success_count = 0
        
        for i in range(0, len(self.x_test), batch_size):
            batch_end = min(i + batch_size, len(self.x_test))
            x_batch = x_test_tf[i:batch_end]
            y_batch = y_test_tf[i:batch_end]
            
            # Initialize adversarial examples with small random noise
            adversarial_batch = x_batch + tf.random.uniform(x_batch.shape, -epsilon/10, epsilon/10)
            adversarial_batch = tf.clip_by_value(adversarial_batch, 0, 1)
            
            # Iterative attack
            for _ in range(iterations):
                with tf.GradientTape() as tape:
                    tape.watch(adversarial_batch)
                    predictions = self.model(adversarial_batch)
                    loss = tf.keras.losses.categorical_crossentropy(y_batch, predictions)
                
                # Get gradients
                gradients = tape.gradient(loss, adversarial_batch)
                
                # Update adversarial examples
                signed_gradients = tf.sign(gradients)
                adversarial_batch = adversarial_batch + alpha * signed_gradients
                
                # Project back to epsilon ball
                delta = adversarial_batch - x_batch
                delta = tf.clip_by_value(delta, -epsilon, epsilon)
                adversarial_batch = x_batch + delta
                
                # Ensure valid pixel values
                adversarial_batch = tf.clip_by_value(adversarial_batch, 0, 1)
            
            # Get predictions for adversarial examples
            adv_predictions = self.model.predict(adversarial_batch)
            adv_pred_classes = np.argmax(adv_predictions, axis=1)
            
            # Check if the attack was successful for each example
            for j in range(len(adv_pred_classes)):
                if baseline_pred_classes[i+j] != adv_pred_classes[j]:
                    success_count += 1
                
                # Add to original and adversarial examples (for visualization)
                if len(original_examples) < 10:  # Only keep 10 examples for visualization
                    original_examples.append(self.x_test[i+j])
                    adversarial_examples.append(adversarial_batch[j].numpy())
                    original_predictions.append(int(baseline_pred_classes[i+j]))
                    adversarial_predictions.append(int(adv_pred_classes[j]))
        
        # Calculate attack success rate
        success_rate = success_count / len(self.x_test)
        
        print(f"{Colors.GREEN}[✓] PGD attack completed{Colors.ENDC}")
        print(f"{Colors.GREEN}[✓] Attack success rate: {success_rate:.4f}{Colors.ENDC}")
        
        # Save results
        result = {
            "attack_type": "PGD",
            "parameters": {
                "epsilon": epsilon,
                "alpha": alpha,
                "iterations": iterations
            },
            "success_rate": success_rate,
            "original_examples": [x.tolist() for x in original_examples],
            "adversarial_examples": [x.tolist() for x in adversarial_examples],
            "original_predictions": original_predictions,
            "adversarial_predictions": adversarial_predictions
        }
        
        self.attack_results["pgd"] = result
        
        # Create and save visualization
        self._save_adversarial_examples_visualization(
            original_examples, adversarial_examples,
            original_predictions, adversarial_predictions,
            "PGD"
        )
        
        return result
    
    def cw_attack(self, confidence=0, learning_rate=0.01, iterations=100, num_samples=10):
        """
        Perform Carlini & Wagner (C&W) attack (simplified version)
        
        Args:
            confidence: Confidence parameter
            learning_rate: Learning rate for optimization
            iterations: Number of iterations
            num_samples: Number of samples to attack (CW is computationally expensive)
            
        Returns:
            Dictionary containing attack results
        """
        print(f"{Colors.BLUE}[*] Performing C&W attack with confidence={confidence}, iterations={iterations}...{Colors.ENDC}")
        print(f"{Colors.WARNING}[!] Note: Using simplified C&W implementation for demonstration{Colors.ENDC}")
        
        # For C&W attack, we'll use a smaller subset due to computational complexity
        x_test_subset = self.x_test[:num_samples]
        y_test_subset = self.y_test[:num_samples]
        
        # Get baseline predictions
        baseline_preds = self.model.predict(x_test_subset)
        baseline_pred_classes = np.argmax(baseline_preds, axis=1)
        
        # Store original and adversarial examples
        original_examples = []
        adversarial_examples = []
        original_predictions = []
        adversarial_predictions = []
        
        # C&W attack
        success_count = 0
        
        for i in range(len(x_test_subset)):
            image = x_test_subset[i:i+1]
            true_label = y_test_subset[i]
            
            # Using a simplified approach for demonstration
            # In a real implementation, the C&W attack would use more sophisticated optimization
            
            # Convert to TensorFlow tensor
            image_tensor = tf.convert_to_tensor(image)
            
            # Initialize perturbation
            w = tf.Variable(tf.zeros_like(image_tensor), trainable=True)
            
            # Optimizer
            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
            
            # Optimization loop
            best_adv_example = None
            best_adv_class = None
            best_loss = float('inf')
            
            for iteration in range(iterations):
                with tf.GradientTape() as tape:
                    # tanh parameterization to constrain adversarial example to [0, 1]
                    adv_example = 0.5 * (tf.tanh(w) + 1)
                    
                    # Predict
                    predictions = self.model(adv_example)
                    predicted_class = tf.argmax(predictions[0])
                    
                    # Confidence of the true class
                    target_confidence = predictions[0, true_label]
                    
                    # Highest confidence among other classes
                    other_confidence = tf.reduce_max(
                        predictions[0] - tf.one_hot(true_label, 10) * 1e9
                    )
                    
                    # L2 distance between original and adversarial example
                    l2_dist = tf.reduce_sum(tf.square(adv_example - image_tensor))
                    
                    # C&W loss function
                    loss = l2_dist + tf.maximum(0.0, target_confidence - other_confidence + confidence)
                
                # Update perturbation
                gradients = tape.gradient(loss, w)
                optimizer.apply_gradients([(gradients, w)])
                
                # Check if attack is successful and better than previous best
                current_loss = loss.numpy()
                current_class = predicted_class.numpy()
                
                if current_class != true_label and current_loss < best_loss:
                    best_loss = current_loss
                    best_adv_example = adv_example.numpy()
                    best_adv_class = current_class
            
            # Check if attack was successful
            if best_adv_example is not None and best_adv_class != true_label:
                success_count += 1
                
                # Add to visualization examples
                original_examples.append(image[0])
                adversarial_examples.append(best_adv_example[0])
                original_predictions.append(int(baseline_pred_classes[i]))
                adversarial_predictions.append(int(best_adv_class))
        
        # Calculate attack success rate
        success_rate = success_count / len(x_test_subset)
        
        print(f"{Colors.GREEN}[✓] C&W attack completed{Colors.ENDC}")
        print(f"{Colors.GREEN}[✓] Attack success rate: {success_rate:.4f}{Colors.ENDC}")
        
        # Save results
        result = {
            "attack_type": "C&W",
            "parameters": {
                "confidence": confidence,
                "iterations": iterations,
                "num_samples": num_samples
            },
            "success_rate": success_rate,
            "original_examples": [x.tolist() for x in original_examples],
            "adversarial_examples": [x.tolist() for x in adversarial_examples],
            "original_predictions": original_predictions,
            "adversarial_predictions": adversarial_predictions
        }
        
        self.attack_results["cw"] = result
        
        # Create and save visualization
        if original_examples:
            self._save_adversarial_examples_visualization(
                original_examples, adversarial_examples,
                original_predictions, adversarial_predictions,
                "C&W"
            )
        
        return result
    
    def _save_adversarial_examples_visualization(self, original_examples, adversarial_examples, 
                                               original_predictions, adversarial_predictions, attack_name):
        """Create and save visualization of adversarial examples"""
        if not original_examples:
            return
            
        num_examples = min(len(original_examples), 5)  # Show up to 5 examples
        
        plt.figure(figsize=(12, 4 * num_examples))
        for i in range(num_examples):
            # Original image
            plt.subplot(num_examples, 3, i*3+1)
            plt.imshow(original_examples[i].reshape(28, 28), cmap='gray')
            plt.title(f"Original\nPredicted: {original_predictions[i]}")
            plt.axis('off')
            
            # Adversarial image
            plt.subplot(num_examples, 3, i*3+2)
            plt.imshow(adversarial_examples[i].reshape(28, 28), cmap='gray')
            plt.title(f"Adversarial\nPredicted: {adversarial_predictions[i]}")
            plt.axis('off')
            
            # Difference (magnified)
            plt.subplot(num_examples, 3, i*3+3)
            difference = np.abs(adversarial_examples[i] - original_examples[i])
            plt.imshow(difference.reshape(28, 28) * 10, cmap='hot')  # Magnify by 10x for visibility
            plt.title("Difference (10x)")
            plt.axis('off')
        
        plt.suptitle(f"{attack_name} Adversarial Examples")
        plt.tight_layout()
        
        # Save figure
        output_path = os.path.join(self.output_dir, f"{attack_name.lower()}_examples.png")
        plt.savefig(output_path)
        plt.close()
        
        print(f"{Colors.GREEN}[✓] Saved adversarial examples visualization to {output_path}{Colors.ENDC}")
    
    def generate_report(self):
        """Generate a comprehensive security assessment report"""
        print(f"{Colors.BLUE}[*] Generating security assessment report...{Colors.ENDC}")
        
        if not self.attack_results:
            print(f"{Colors.FAIL}[✗] No attack results available. Run attacks first.{Colors.ENDC}")
            return
        
        # Basic report data
        report = {
            "report_title": "AI/ML Evasion Attack Security Assessment",
            "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "model_path": self.model_path,
            "model_summary": str(self.model.summary()) if self.model else "Not available",
            "attack_results": self.attack_results,
            "risk_assessment": self._assess_risk(),
            "recommendations": self._generate_recommendations()
        }
        
        # Save JSON report
        report_path = os.path.join(self.output_dir, "security_assessment_report.json")
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=4)
        
        # Create HTML report (simplified)
        html_report_path = os.path.join(self.output_dir, "security_assessment_report.html")
        self._generate_html_report(report, html_report_path)
        
        print(f"{Colors.GREEN}[✓] Security assessment report generated{Colors.ENDC}")
        print(f"{Colors.GREEN}[✓] JSON report saved to {report_path}{Colors.ENDC}")
        print(f"{Colors.GREEN}[✓] HTML report saved to {html_report_path}{Colors.ENDC}")
    
    def _assess_risk(self):
        """Assess risk based on attack success rates"""
        risk_assessment = {
            "overall_risk": "Low",
            "attack_risks": {}
        }
        
        success_rates = []
        for attack_name, result in self.attack_results.items():
            success_rate = result.get('success_rate', 0)
            success_rates.append(success_rate)
            
            # Determine risk level for this attack
            if success_rate >= 0.7:
                risk_level = "High"
            elif success_rate >= 0.3:
                risk_level = "Medium"
            else:
                risk_level = "Low"
            
            risk_assessment["attack_risks"][attack_name] = {
                "success_rate": success_rate,
                "risk_level": risk_level
            }
        
        # Determine overall risk level
        if any(rate >= 0.7 for rate in success_rates):
            risk_assessment["overall_risk"] = "High"
        elif any(rate >= 0.3 for rate in success_rates):
            risk_assessment["overall_risk"] = "Medium"
        
        return risk_assessment
    
    def _generate_recommendations(self):
        """Generate recommendations based on attack results"""
        recommendations = []
        
        # Check which attacks were successful and provide recommendations
        high_risk_attacks = []
        for attack_name, result in self.attack_results.items():
            if result.get('success_rate', 0) >= 0.3:
                high_risk_attacks.append(attack_name)
        
        # General recommendation for adversarial training
        if high_risk_attacks:
            recommendations.append({
                "title": "Implement Adversarial Training",
                "description": "Train the model with adversarial examples to improve robustness.",
                "details": "Adversarial training involves augmenting the training dataset with adversarial examples. This helps the model learn to be robust against small perturbations.",
                "priority": "High"
            })
        
        # FGSM-specific recommendations
        if "fgsm" in high_risk_attacks:
            recommendations.append({
                "title": "Add Input Preprocessing Defense",
                "description": "Implement input preprocessing techniques to detect or neutralize adversarial perturbations.",
                "details": "Techniques such as JPEG compression, bit-depth reduction, or Gaussian noise can help reduce the effectiveness of gradient-based attacks like FGSM.",
                "priority": "Medium"
            })
        
        # PGD-specific recommendations
        if "pgd" in high_risk_attacks:
            recommendations.append({
                "title": "Use Ensemble Methods",
                "description": "Implement ensemble prediction using multiple model architectures.",
                "details": "Ensemble methods can improve robustness as different models may be vulnerable to different types of adversarial examples.",
                "priority": "Medium"
            })
        
        # C&W-specific recommendations
        if "cw" in high_risk_attacks:
            recommendations.append({
                "title": "Consider Certified Defenses",
                "description": "Implement certified robustness techniques for critical applications.",
                "details": "Techniques like randomized smoothing can provide mathematical guarantees about the model's robustness against adversarial perturbations.",
                "priority": "High"
            })
        
        # Add general recommendations
        recommendations.append({
            "title": "Regular Security Testing",
            "description": "Implement regular adversarial testing as part of the ML development lifecycle.",
            "details": "Continuously test for new vulnerabilities, especially after model updates or retraining.",
            "priority": "Medium"
        })
        
        recommendations.append({
            "title": "Monitor Model Predictions",
            "description": "Implement monitoring for detecting potential adversarial examples in production.",
            "details": "Look for unusual patterns in input data or unexpected model behavior that could indicate an attack.",
            "priority": "Medium"
        })
        
        return recommendations
    
    def _generate_html_report(self, report_data, output_path):
        """Generate a simple HTML report"""
        # Get risk level for color coding
        risk_level = report_data["risk_assessment"]["overall_risk"]
        risk_color = {
            "Low": "#28a745",      # Green
            "Medium": "#ffc107",   # Yellow
            "High": "#dc3545"      # Red
        }[risk_level]
        
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>AI/ML Security Assessment Report</title>
            <style>
                body {{
                    font-family: Arial, sans-serif;
                    line-height: 1.6;
                    max-width: 1000px;
                    margin: 0 auto;
                    padding: 20px;
                }}
                h1, h2, h3 {{
                    color: #333;
                }}
                .header {{
                    text-align: center;
                    margin-bottom: 30px;
                }}
                .risk-badge {{
                    display: inline-block;
                    padding: 5px 10px;
                    border-radius: 4px;
                    color: white;
                    background-color: {risk_color};
                    font-weight: bold;
                }}
                .section {{
                    margin-bottom: 30px;
                    border: 1px solid #ddd;
                    border-radius: 5px;
                    padding: 20px;
                }}
                table {{
                    width: 100%;
                    border-collapse: collapse;
                }}
                th, td {{
                    border: 1px solid #ddd;
                    padding: 8px;
                    text-align: left;
                }}
                th {{
                    background-color: #f2f2f2;
                }}
                .recommendation {{
                    margin-bottom: 15px;
                    padding: 15px;
                    border-left: 4px solid #2196F3;
                    background-color: #f8f9fa;
                }}
                .high-priority {{
                    border-left-color: #dc3545;
                }}
                .medium-priority {{
                    border-left-color: #ffc107;
                }}
                .low-priority {{
                    border-left-color: #28a745;
                }}
                img {{
                    max-width: 100%;
                    height: auto;
                    margin: 20px 0;
                }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>AI/ML Security Assessment Report</h1>
                <p>Generated on: {report_data["date"]}</p>
            </div>
            
            <div class="section">
                <h2>Executive Summary</h2>
                <p>This report presents the findings of a security assessment focused on evasion attacks against the machine learning model.</p>
                <p>Overall risk level: <span class="risk-badge">{risk_level}</span></p>
            </div>
            
            <div class="section">
                <h2>Attack Results</h2>
                <table>
                    <tr>
                        <th>Attack Type</th>
                        <th>Success Rate</th>
                        <th>Risk Level</th>
                    </tr>
        """
        
        # Add attack results
        for attack_name, risk_info in report_data["risk_assessment"]["attack_risks"].items():
            risk_level = risk_info["risk_level"]
            risk_color = {
                "Low": "#28a745",      # Green
                "Medium": "#ffc107",   # Yellow
                "High": "#dc3545"      # Red
            }[risk_level]
            
            html += f"""
                    <tr>
                        <td>{attack_name.upper()}</td>
                        <td>{risk_info["success_rate"]:.2%}</td>
                        <td style="color: {risk_color}; font-weight: bold;">{risk_level}</td>
                    </tr>
            """
        
        html += """
                </table>
            </div>
            
            <div class="section">
                <h2>Attack Visualizations</h2>
        """
        
        # Add attack visualizations
        for attack_name in report_data["attack_results"].keys():
            img_path = f"{attack_name.lower()}_examples.png"
            html += f"""
                <h3>{attack_name.upper()} Attack</h3>
                <img src="{img_path}" alt="{attack_name} Adversarial Examples">
            """
        
        html += """
            </div>
            
            <div class="section">
                <h2>Recommendations</h2>
        """
        
        # Add recommendations
        for recommendation in report_data["recommendations"]:
            priority_class = f"{recommendation['priority'].lower()}-priority"
            html += f"""
                <div class="recommendation {priority_class}">
                    <h3>{recommendation["title"]} (Priority: {recommendation["priority"]})</h3>
                    <p>{recommendation["description"]}</p>
                    <p><strong>Details:</strong> {recommendation["details"]}</p>
                </div>
            """
        
        html += """
            </div>
            
            <div class="section">
                <h2>Methodology</h2>
                <p>This security assessment was conducted using the following methodology:</p>
                <ol>
                    <li>Established baseline model performance on clean test data.</li>
                    <li>Performed FGSM attack to generate adversarial examples.</li>
                    <li>Performed PGD attack to generate stronger adversarial examples.</li>
                    <li>Performed C&W attack to generate optimization-based adversarial examples.</li>
                    <li>Analyzed attack success rates and model vulnerability.</li>
                    <li>Generated security recommendations based on findings.</li>
                </ol>
                <p>All testing was conducted in accordance with the CyBOK Security and Privacy of AI Knowledge Guide.</p>
            </div>
            
            <div style="margin-top: 50px; text-align: center; color: #666; font-size: 14px;">
                <p>This report is for educational and authorized testing purposes only.</p>
                <p>Based on CyBOK Security and Privacy of AI Knowledge Guide</p>
            </div>
        </body>
        </html>
        """
        
        # Write HTML report to file
        with open(output_path, 'w') as f:
            f.write(html)
    
    def run_all_attacks(self, options=None):
        """Run all evasion attacks with given options"""
        if options is None:
            options = {}
        
        # Default options if not specified
        fgsm_eps = options.get("fgsm_epsilon", 0.1)
        pgd_eps = options.get("pgd_epsilon", 0.1)
        pgd_alpha = options.get("pgd_alpha", 0.01)
        pgd_iter = options.get("pgd_iterations", 40)
        cw_conf = options.get("cw_confidence", 0)
        cw_iter = options.get("cw_iterations", 100)
        
        # Run attacks
        self.fgsm_attack(epsilon=fgsm_eps)
        self.pgd_attack(epsilon=pgd_eps, alpha=pgd_alpha, iterations=pgd_iter)
        self.cw_attack(confidence=cw_conf, iterations=cw_iter)
        
        # Generate report
        self.generate_report()

def main():
    """Main function to run the evasion attack penetration testing tool"""
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="MNIST Evasion Attack Penetration Testing Tool")
    parser.add_argument("--model-path", type=str, required=True, help="Path to the target model file (.h5)")
    parser.add_argument("--output-dir", type=str, default="results", help="Directory to save results (default: results)")
    parser.add_argument("--fgsm-epsilon", type=float, default=0.1, help="Epsilon for FGSM attack (default: 0.1)")
    parser.add_argument("--pgd-epsilon", type=float, default=0.1, help="Epsilon for PGD attack (default: 0.1)")
    parser.add_argument("--pgd-alpha", type=float, default=0.01, help="Step size for PGD attack (default: 0.01)")
    parser.add_argument("--pgd-iterations", type=int, default=40, help="Iterations for PGD attack (default: 40)")
    parser.add_argument("--cw-confidence", type=float, default=0, help="Confidence for C&W attack (default: 0)")
    parser.add_argument("--cw-iterations", type=int, default=100, help="Iterations for C&W attack (default: 100)")
    parser.add_argument("--skip-cw", action="store_true", help="Skip C&W attack (it's computationally expensive)")
    
    args = parser.parse_args()
    
    # Print banner
    print_banner()
    
    # Initialize the pentesting environment
    pentester = EvasionAttackPenTest(model_path=args.model_path, output_dir=args.output_dir)
    
    # Load model and test data
    if not pentester.load_model():
        sys.exit(1)
    
    if not pentester.load_test_data():
        sys.exit(1)
    
    # Evaluate baseline performance
    pentester.evaluate_baseline()
    
    # Run attacks
    print(f"\n{Colors.BLUE}[*] Beginning evasion attack penetration testing...{Colors.ENDC}")
    
    # FGSM Attack
    pentester.fgsm_attack(epsilon=args.fgsm_epsilon)
    
    # PGD Attack
    pentester.pgd_attack(epsilon=args.pgd_epsilon, alpha=args.pgd_alpha, iterations=args.pgd_iterations)
    
    # C&W Attack (optional, can be skipped as it's computationally expensive)
    if not args.skip_cw:
        pentester.cw_attack(confidence=args.cw_confidence, iterations=args.cw_iterations)
    else:
        print(f"{Colors.WARNING}[!] Skipping C&W attack as requested{Colors.ENDC}")
    
    # Generate report
    pentester.generate_report()
    
    print(f"\n{Colors.GREEN}[✓] Penetration testing completed{Colors.ENDC}")
    print(f"{Colors.GREEN}[✓] Results saved to {args.output_dir}{Colors.ENDC}")

if __name__ == "__main__":
    main()