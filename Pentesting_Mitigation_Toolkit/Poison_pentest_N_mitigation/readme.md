# ML Poisoning Attack Penetration Testing Toolkit

This toolkit provides implementations of poisoning attacks and defense mechanisms for machine learning models, based on the CyBOK Security and Privacy of AI Knowledge Guide. It allows security professionals and researchers to test the resilience of machine learning systems against poisoning attacks.

## Table of Contents

- [Overview](#overview)
- [Installation](#installation)
- [Usage](#usage)
  - [Running Poisoning Attacks](#running-poisoning-attacks)
  - [Implementing Defense Mechanisms](#implementing-defense-mechanisms)
- [Technical Background](#technical-background)
- [Attack Implementation](#attack-implementation)
- [Defense Implementation](#defense-implementation)
- [Visualizations](#visualizations)
- [References](#references)
- [License](#license)

## Overview

Poisoning attacks target the training phase of machine learning models by manipulating training data to compromise the model's integrity. This toolkit implements both attacks and defenses to help security professionals test and improve ML systems.

Key features:
- **Poisoning Attack Implementation**: Test how label flipping affects model behavior
- **Multiple Defense Strategies**: Test different detection and mitigation approaches
- **Comprehensive Evaluation**: Detailed metrics and visualizations
- **Command-line Interface**: Easy to use in testing environments

## Installation

1. Clone this repository:
```bash
git clone https://github.com/yourusername/ml-poisoning-toolkit.git
cd ml-poisoning-toolkit
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

Required dependencies:
- tensorflow
- numpy
- matplotlib
- scikit-learn
- pandas
- seaborn

## Usage

### Running Poisoning Attacks

The `poisoning_attack.py` script implements a label flipping attack against an MNIST classifier. This represents a typical poisoning attack as described in Section 3.2 of the CyBOK guide.

Basic usage:
```bash
python poisoning_attack.py
```

With custom parameters:
```bash
python poisoning_attack.py --source-class 7 --target-class 1 --poison-percent 0.05 --epochs 5 --output-dir results
```

Parameters:
- `--source-class`: Class to select samples from (default: 7)
- `--target-class`: Class to flip labels to (default: 1)
- `--poison-percent`: Percentage of training data to poison (default: 0.05)
- `--model-save-path`: Path to save the poisoned model (default: poisoned_model.h5)
- `--epochs`: Number of training epochs (default: 5)
- `--output-dir`: Directory to save results (default: results)

The script will:
1. Load the MNIST dataset
2. Create a poisoned version by flipping labels from source to target class
3. Train both clean and poisoned models
4. Evaluate the attack effectiveness
5. Generate visualizations
6. Save the poisoned model

### Implementing Defense Mechanisms

The `poisoning_defense.py` script implements defense mechanisms against poisoning attacks as described in Section 4.4 of the CyBOK guide.

Basic usage:
```bash
python poisoning_defense.py
```

With custom parameters:
```bash
python poisoning_defense.py --model-path results/poisoned_model.h5 --detection-method iforest --contamination 0.05 --epochs 5 --output-dir defense_results
```

Parameters:
- `--model-path`: Path to the poisoned model (default: results/poisoned_model.h5)
- `--source-class`: Source class used in poisoning (default: 7)
- `--target-class`: Target class used in poisoning (default: 1)
- `--detection-method`: Method for detecting poisoned samples (choices: iforest, lof, label) (default: iforest)
- `--contamination`: Expected proportion of poisoned samples (default: 0.05)
- `--epochs`: Number of training epochs for the robust model (default: 5)
- `--output-dir`: Directory to save defense results (default: defense_results)

The script will:
1. Load the poisoned model
2. Detect potentially poisoned samples using the specified method
3. Create a sanitized dataset
4. Train a robust model on the sanitized data
5. Evaluate defense effectiveness
6. Generate visualizations
7. Save the robust model

## Technical Background

### Poisoning Attacks

Poisoning attacks target the training phase of machine learning models. As described in the CyBOK Security and Privacy of AI Knowledge Guide (Section 3.2), these attacks can be formulated as an optimization problem:

```
maximize L(θ*, {x*_i, y*_i})
subject to θ* = argmin L(θ, {x_i, y_i} ∪ {x*_i, y*_i})
           constraints on {x*_i, y*_i}
```

Where:
- `L` is the model's loss function
- `θ` represents the model parameters
- `{x_i, y_i}` is the original training dataset
- `{x*_i, y*_i}` are the poisoned samples

In our implementation, we focus on label flipping attacks where a subset of samples from one class has their labels changed to another class.

### Defense Strategies

Defending against poisoning attacks involves:
1. **Data Sanitization**: Detecting and removing suspicious training samples
2. **Robust Learning**: Training models that are less sensitive to poisoned data
3. **Anomaly Detection**: Identifying samples that deviate from expected distributions

## Attack Implementation

The poisoning attack implementation:
1. Selects samples from a source class
2. Changes their labels to a target class
3. Optionally adds subtle noise to make detection harder
4. Trains a model on the poisoned data
5. Evaluates how effectively the poisoned model misclassifies samples from the source class as the target class

## Defense Implementation

The defense implementation provides three detection methods:
1. **Isolation Forest**: An anomaly detection algorithm that isolates outliers by recursively partitioning the data
2. **Local Outlier Factor**: A density-based method that compares the local density of a point to its neighbors
3. **Label Outlier Detection**: A neighborhood-based approach that identifies samples whose labels differ from their neighbors

After detection, the defense:
1. Removes suspicious samples from the training data
2. Trains a new, robust model on the sanitized data
3. Evaluates how well the defense mitigates the poisoning attack

## Visualizations

Both scripts generate visualizations to help understand the attack and defense:

### Attack Visualizations
- Poisoned samples
- Confusion matrices for clean and poisoned models
- Impact on classification of the source class

### Defense Visualizations
- Detected outliers
- Confusion matrices for robust and poisoned models
- Classification differences before and after defense

## References

- CyBOK Security and Privacy of AI Knowledge Guide (2023), Section 3.2: Poisoning Attacks
- CyBOK Security and Privacy of AI Knowledge Guide (2023), Section 4.4: Defenses against Poisoning and Backdoor Attacks

## License

This toolkit is provided for educational and research purposes only. Use of this toolkit for attacking production systems without explicit permission is illegal and unethical.

[Your License Information Here]
