#!/usr/bin/env python3
"""
Poisoning Attack Implementation for ML Models

This script implements a poisoning attack against machine learning models
as described in the CyBOK Security and Privacy of AI Knowledge Guide (Section 3.2).

Usage:
    python poisoning_attack.py [--source-class 7] [--target-class 1] [--poison-percent 0.05]
                              [--model-save-path poisoned_model.h5] [--epochs 5]

Author: Based on CyBOK Security and Privacy of AI Knowledge Guide
"""

import os
import argparse
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential, save_model, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

class PoisoningAttack:
    """
    Implementation of a poisoning attack against an MNIST classifier.
    
    This attack manipulates training data by:
    1. Selecting a subset of samples from a source class
    2. Flipping their labels to a target class
    3. Optionally adding subtle modifications to make detection harder
    """
    
    def __init__(self, x_train, y_train, x_test, y_test):
        """Initialize the attack with training and test data"""
        self.x_train = x_train
        self.y_train = y_train
        self.x_test = x_test
        self.y_test = y_test
        
        # Convert labels to categorical format if they aren't already
        if len(self.y_train.shape) == 1:
            self.y_train_cat = to_categorical(self.y_train, 10)
        else:
            self.y_train_cat = self.y_train
            
        if len(self.y_test.shape) == 1:
            self.y_test_cat = to_categorical(self.y_test, 10)
        else:
            self.y_test_cat = self.y_test
    
    def poison_dataset(self, source_class, target_class, poison_percent=0.1, add_noise=True):
        """
        Create a poisoned dataset by flipping labels from source_class to target_class
        
        Args:
            source_class: Original class to select samples from
            target_class: Target class to flip labels to
            poison_percent: Percentage of training data to poison
            add_noise: Whether to add subtle noise to poisoned samples
        
        Returns:
            Poisoned training data and labels
        """
        print(f"Poisoning {poison_percent*100:.1f}% of training data")
        print(f"Source class: {source_class}, Target class: {target_class}")
        
        # Make copies to avoid modifying the original data
        x_train_poisoned = self.x_train.copy()
        y_train_poisoned = self.y_train_cat.copy()
        
        # Find samples from the source class
        if len(self.y_train.shape) == 1:
            source_indices = np.where(self.y_train == source_class)[0]
        else:
            source_indices = np.where(np.argmax(self.y_train, axis=1) == source_class)[0]
        
        # Calculate number of samples to poison
        num_poison = int(len(self.x_train) * poison_percent)
        num_poison = min(num_poison, len(source_indices))
        
        # Randomly select indices to poison
        poison_indices = np.random.choice(source_indices, num_poison, replace=False)
        
        print(f"Selected {num_poison} samples for poisoning")
        
        # Poison the selected samples
        for idx in poison_indices:
            # Flip label to target class
            y_train_poisoned[idx] = np.zeros(10)
            y_train_poisoned[idx, target_class] = 1.0
            
            # Optionally add subtle noise to make poisoning harder to detect
            if add_noise:
                noise = np.random.normal(0, 0.05, x_train_poisoned[idx].shape)
                x_train_poisoned[idx] = np.clip(x_train_poisoned[idx] + noise, 0, 1)
        
        # Store indices and poisoned samples for later analysis
        self.poison_indices = poison_indices
        self.poisoned_samples = x_train_poisoned[poison_indices]
        self.original_samples = self.x_train[poison_indices]
        
        return x_train_poisoned, y_train_poisoned
    
    def build_model(self):
        """Build a simple CNN model for MNIST classification"""
        model = Sequential([
            Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
            MaxPooling2D((2, 2)),
            Conv2D(64, (3, 3), activation='relu'),
            MaxPooling2D((2, 2)),
            Flatten(),
            Dense(128, activation='relu'),
            Dropout(0.2),
            Dense(10, activation='softmax')
        ])
        
        model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def train_models(self, batch_size=128, epochs=5):
        """Train both clean and poisoned models for comparison"""
        print("\nTraining clean model...")
        clean_model = self.build_model()
        clean_history = clean_model.fit(
            self.x_train, self.y_train_cat,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(self.x_test, self.y_test_cat),
            verbose=1
        )
        
        print("\nTraining poisoned model...")
        poisoned_model = self.build_model()
        poisoned_history = poisoned_model.fit(
            self.x_train_poisoned, self.y_train_poisoned,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(self.x_test, self.y_test_cat),
            verbose=1
        )
        
        self.clean_model = clean_model
        self.poisoned_model = poisoned_model
        
        return clean_model, poisoned_model, clean_history, poisoned_history
    
    def evaluate_attack(self, source_class, target_class):
        """Evaluate the effectiveness of the poisoning attack"""
        # Evaluate on test data
        clean_loss, clean_acc = self.clean_model.evaluate(self.x_test, self.y_test_cat, verbose=0)
        poisoned_loss, poisoned_acc = self.poisoned_model.evaluate(self.x_test, self.y_test_cat, verbose=0)
        
        print("\n--- Attack Evaluation ---")
        print(f"Clean model test accuracy: {clean_acc:.4f}")
        print(f"Poisoned model test accuracy: {poisoned_acc:.4f}")
        print(f"Overall accuracy impact: {poisoned_acc - clean_acc:.4f}")
        
        # Filter test data for the source class
        source_indices = np.where(np.argmax(self.y_test_cat, axis=1) == source_class)[0]
        x_test_source = self.x_test[source_indices]
        y_test_source = self.y_test_cat[source_indices]
        
        # Check how many source class samples are now classified as target class
        clean_preds = self.clean_model.predict(x_test_source)
        poisoned_preds = self.poisoned_model.predict(x_test_source)
        
        clean_target_rate = np.mean(np.argmax(clean_preds, axis=1) == target_class)
        poisoned_target_rate = np.mean(np.argmax(poisoned_preds, axis=1) == target_class)
        
        print(f"\nSource class ({source_class}) → Target class ({target_class}) misclassification rate:")
        print(f"Clean model: {clean_target_rate:.4f}")
        print(f"Poisoned model: {poisoned_target_rate:.4f}")
        print(f"Attack success delta: {poisoned_target_rate - clean_target_rate:.4f}")
        
        # Calculate the attack success rate
        attack_success_rate = poisoned_target_rate - clean_target_rate
        
        return {
            'clean_accuracy': clean_acc,
            'poisoned_accuracy': poisoned_acc,
            'accuracy_impact': poisoned_acc - clean_acc,
            'clean_target_rate': clean_target_rate,
            'poisoned_target_rate': poisoned_target_rate,
            'attack_success_rate': attack_success_rate
        }
    
    def visualize_results(self, source_class, target_class, results, output_dir="results"):
        """Visualize poisoned samples and attack results"""
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. Visualize some poisoned samples
        plt.figure(figsize=(10, 5))
        plt.suptitle(f"Poisoning Attack: {source_class} → {target_class}", fontsize=16)
        
        for i in range(min(5, len(self.poisoned_samples))):
            # Original sample
            plt.subplot(2, 5, i+1)
            plt.imshow(self.original_samples[i].reshape(28, 28), cmap='gray')
            plt.title(f"Original: {source_class}")
            plt.axis('off')
            
            # Poisoned sample
            plt.subplot(2, 5, i+6)
            plt.imshow(self.poisoned_samples[i].reshape(28, 28), cmap='gray')
            plt.title(f"Poisoned: {target_class}")
            plt.axis('off')
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.85)
        plt.savefig(os.path.join(output_dir, "poisoned_samples.png"))
        print(f"Saved poisoned samples visualization to {os.path.join(output_dir, 'poisoned_samples.png')}")
        plt.close()
        
        # 2. Visualize confusion matrices
        plt.figure(figsize=(15, 6))
        
        # Clean model confusion matrix
        plt.subplot(1, 2, 1)
        clean_preds = np.argmax(self.clean_model.predict(self.x_test), axis=1)
        clean_cm = confusion_matrix(np.argmax(self.y_test_cat, axis=1), clean_preds)
        sns.heatmap(clean_cm, annot=True, fmt='d', cmap='Blues')
        plt.title("Clean Model Confusion Matrix")
        plt.ylabel("True Label")
        plt.xlabel("Predicted Label")
        
        # Poisoned model confusion matrix
        plt.subplot(1, 2, 2)
        poisoned_preds = np.argmax(self.poisoned_model.predict(self.x_test), axis=1)
        poisoned_cm = confusion_matrix(np.argmax(self.y_test_cat, axis=1), poisoned_preds)
        sns.heatmap(poisoned_cm, annot=True, fmt='d', cmap='Blues')
        plt.title("Poisoned Model Confusion Matrix")
        plt.ylabel("True Label")
        plt.xlabel("Predicted Label")
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, "confusion_matrices.png"))
        print(f"Saved confusion matrices to {os.path.join(output_dir, 'confusion_matrices.png')}")
        plt.close()
        
        # 3. Visualize impact on target class
        plt.figure(figsize=(8, 6))
        
        # Filter test data for the source class
        source_indices = np.where(np.argmax(self.y_test_cat, axis=1) == source_class)[0]
        x_test_source = self.x_test[source_indices]
        
        # Get predictions
        clean_preds = self.clean_model.predict(x_test_source)
        poisoned_preds = self.poisoned_model.predict(x_test_source)
        
        # Calculate class prediction rates
        clean_pred_classes = np.argmax(clean_preds, axis=1)
        poisoned_pred_classes = np.argmax(poisoned_preds, axis=1)
        
        # Count predictions for each class
        clean_counts = np.zeros(10)
        poisoned_counts = np.zeros(10)
        
        for i in range(10):
            clean_counts[i] = np.sum(clean_pred_classes == i) / len(clean_pred_classes)
            poisoned_counts[i] = np.sum(poisoned_pred_classes == i) / len(poisoned_pred_classes)
        
        # Plot results
        bar_width = 0.35
        indices = np.arange(10)
        
        plt.bar(indices - bar_width/2, clean_counts, bar_width, label='Clean Model')
        plt.bar(indices + bar_width/2, poisoned_counts, bar_width, label='Poisoned Model')
        
        plt.xlabel('Predicted Class')
        plt.ylabel('Proportion of Source Class Samples')
        plt.title(f'Impact on Classification of Source Class ({source_class})')
        plt.xticks(indices)
        plt.legend()
        
        # Highlight the target class
        plt.axvline(x=target_class, color='r', linestyle='--', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, "class_impact.png"))
        print(f"Saved class impact visualization to {os.path.join(output_dir, 'class_impact.png')}")
        plt.close()
        
        # 4. Save numerical results to a text file
        with open(os.path.join(output_dir, "attack_results.txt"), "w") as f:
            f.write("POISONING ATTACK RESULTS\n")
            f.write("=======================\n\n")
            f.write(f"Source Class: {source_class}\n")
            f.write(f"Target Class: {target_class}\n")
            f.write(f"Poison Percentage: {len(self.poison_indices)/len(self.x_train):.2%}\n\n")
            f.write("Model Performance:\n")
            f.write(f"  Clean Model Accuracy: {results['clean_accuracy']:.4f}\n")
            f.write(f"  Poisoned Model Accuracy: {results['poisoned_accuracy']:.4f}\n")
            f.write(f"  Overall Accuracy Impact: {results['accuracy_impact']:.4f}\n\n")
            f.write("Attack Effectiveness:\n")
            f.write(f"  Clean Model {source_class}→{target_class} Rate: {results['clean_target_rate']:.4f}\n")
            f.write(f"  Poisoned Model {source_class}→{target_class} Rate: {results['poisoned_target_rate']:.4f}\n")
            f.write(f"  Attack Success Rate: {results['attack_success_rate']:.4f}\n")
        print(f"Saved numerical results to {os.path.join(output_dir, 'attack_results.txt')}")

def main():
    """Main function to run the poisoning attack demonstration"""
    parser = argparse.ArgumentParser(description='Poisoning Attack Implementation')
    parser.add_argument('--source-class', type=int, default=7, help='Source class for poisoning (default: 7)')
    parser.add_argument('--target-class', type=int, default=1, help='Target class for poisoning (default: 1)')
    parser.add_argument('--poison-percent', type=float, default=0.05, help='Percentage of training data to poison (default: 0.05)')
    parser.add_argument('--model-save-path', type=str, default="poisoned_model.h5", help='Path to save the poisoned model (default: poisoned_model.h5)')
    parser.add_argument('--epochs', type=int, default=5, help='Number of training epochs (default: 5)')
    parser.add_argument('--output-dir', type=str, default="results", help='Directory to save results (default: results)')
    
    args = parser.parse_args()
    
    # Make output directory if it doesn't exist
    os.makedirs(args.output_dir, exist_ok=True)
    
    print("Loading MNIST dataset...")
    # Load MNIST dataset
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    
    # Preprocess the data
    x_train = x_train.astype('float32') / 255.0
    x_test = x_test.astype('float32') / 255.0
    x_train = x_train.reshape(-1, 28, 28, 1)
    x_test = x_test.reshape(-1, 28, 28, 1)
    
    # Set attack parameters
    source_class = args.source_class
    target_class = args.target_class
    poison_percent = args.poison_percent
    
    # Initialize the attack
    attack = PoisoningAttack(x_train, y_train, x_test, y_test)
    
    # Create poisoned dataset
    x_train_poisoned, y_train_poisoned = attack.poison_dataset(
        source_class=source_class,
        target_class=target_class,
        poison_percent=poison_percent,
        add_noise=True
    )
    
    # Store the poisoned dataset
    attack.x_train_poisoned = x_train_poisoned
    attack.y_train_poisoned = y_train_poisoned
    
    # Train models on clean and poisoned data
    clean_model, poisoned_model, clean_history, poisoned_history = attack.train_models(epochs=args.epochs)
    
    # Evaluate the attack
    results = attack.evaluate_attack(source_class, target_class)
    
    # Visualize the results
    attack.visualize_results(source_class, target_class, results, args.output_dir)
    
    # Save the poisoned model
    poisoned_model.save(os.path.join(args.output_dir, args.model_save_path))
    print(f"Saved poisoned model to {os.path.join(args.output_dir, args.model_save_path)}")
    
    print("\nPoisoning attack demonstration completed!")
    print(f"Check the generated images in {args.output_dir} for visual analysis of the attack.")
    
    return attack, results

if __name__ == "__main__":
    attack, results = main()
