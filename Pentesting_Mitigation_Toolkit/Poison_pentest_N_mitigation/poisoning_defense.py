#!/usr/bin/env python3
"""
Poisoning Attack Defense Implementation for ML Models

This script implements defense mechanisms against poisoning attacks
as described in the CyBOK Security and Privacy of AI Knowledge Guide (Section 4.4).

Usage:
    python poisoning_defense.py [--model-path poisoned_model.h5] [--clean-model-path clean_model.h5]
                               [--source-class 7] [--target-class 1] [--detection-method iforest]
                               [--contamination 0.05] [--epochs 5]

Author: Based on CyBOK Security and Privacy of AI Knowledge Guide
"""

import os
import argparse
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential, load_model, save_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

class PoisoningDefense:
    """
    Implementation of defenses against poisoning attacks.
    
    This class provides methods to:
    1. Detect potential poisoned samples using anomaly detection
    2. Sanitize the training dataset
    3. Train robust models that are resistant to poisoning
    4. Evaluate the effectiveness of the defenses
    """
    
    def __init__(self, x_train, y_train, x_test, y_test):
        """Initialize the defense with training and test data"""
        self.x_train = x_train
        self.y_train = y_train
        self.x_test = x_test
        self.y_test = y_test
        
        # Convert labels to categorical format if they aren't already
        if len(self.y_train.shape) == 1:
            self.y_train_cat = to_categorical(self.y_train, 10)
        else:
            self.y_train_cat = self.y_train
            
        if len(self.y_test.shape) == 1:
            self.y_test_cat = to_categorical(self.y_test, 10)
        else:
            self.y_test_cat = self.y_test
    
    def detect_outliers_iforest(self, contamination=0.05):
        """
        Detect potential poisoned samples using Isolation Forest
        
        Args:
            contamination: Expected proportion of outliers in the data
        
        Returns:
            Indices of samples flagged as potential outliers
        """
        print(f"Detecting outliers using Isolation Forest (contamination={contamination})...")
        
        # Flatten the images for anomaly detection
        x_train_flat = self.x_train.reshape(self.x_train.shape[0], -1)
        
        # Use Isolation Forest for outlier detection
        iforest = IsolationForest(contamination=contamination, random_state=42)
        y_pred = iforest.fit_predict(x_train_flat)
        
        # Isolation Forest returns 1 for inliers and -1 for outliers
        outlier_indices = np.where(y_pred == -1)[0]
        print(f"Detected {len(outlier_indices)} potential outliers ({len(outlier_indices)/len(self.x_train):.2%} of training data)")
        
        return outlier_indices
    
    def detect_outliers_lof(self, contamination=0.05):
        """
        Detect potential poisoned samples using Local Outlier Factor
        
        Args:
            contamination: Expected proportion of outliers in the data
        
        Returns:
            Indices of samples flagged as potential outliers
        """
        print(f"Detecting outliers using Local Outlier Factor (contamination={contamination})...")
        
        # Flatten the images for anomaly detection
        x_train_flat = self.x_train.reshape(self.x_train.shape[0], -1)
        
        # Use Local Outlier Factor for outlier detection
        lof = LocalOutlierFactor(n_neighbors=20, contamination=contamination)
        y_pred = lof.fit_predict(x_train_flat)
        
        # LOF returns 1 for inliers and -1 for outliers
        outlier_indices = np.where(y_pred == -1)[0]
        print(f"Detected {len(outlier_indices)} potential outliers ({len(outlier_indices)/len(self.x_train):.2%} of training data)")
        
        return outlier_indices
    
    def detect_label_outliers(self, k=5):
        """
        Detect potential label flipping by comparing with k-nearest neighbors
        
        Args:
            k: Number of neighbors to consider
        
        Returns:
            Indices of samples with potentially flipped labels
        """
        print(f"Detecting label outliers using {k}-nearest neighbors...")
        
        # Flatten the images
        x_train_flat = self.x_train.reshape(self.x_train.shape[0], -1)
        
        # Get labels
        if len(self.y_train.shape) > 1:
            y_train_labels = np.argmax(self.y_train, axis=1)
        else:
            y_train_labels = self.y_train
        
        # Find k nearest neighbors for each sample
        nn = NearestNeighbors(n_neighbors=k+1)  # +1 because the sample itself is included
        nn.fit(x_train_flat)
        distances, indices = nn.kneighbors(x_train_flat)
        
        # For each sample, check if its label matches the majority of its neighbors
        outlier_indices = []
        
        for i in range(len(y_train_labels)):
            # Get labels of neighbors (excluding the sample itself)
            neighbor_labels = y_train_labels[indices[i, 1:]]
            
            # Count occurrences of each label among neighbors
            label_counts = np.bincount(neighbor_labels, minlength=10)
            
            # Find majority label among neighbors
            majority_label = np.argmax(label_counts)
            
            # If sample's label doesn't match majority, flag as potential outlier
            if y_train_labels[i] != majority_label:
                outlier_indices.append(i)
        
        print(f"Detected {len(outlier_indices)} samples with potentially flipped labels ({len(outlier_indices)/len(self.x_train):.2%} of training data)")
        
        return outlier_indices
    
    def create_sanitized_dataset(self, outlier_indices):
        """
        Create a sanitized dataset by removing outliers
        
        Args:
            outlier_indices: Indices of samples to remove
        
        Returns:
            Sanitized training data and labels
        """
        print(f"Creating sanitized dataset by removing {len(outlier_indices)} samples...")
        
        # Create a mask for non-outlier samples
        mask = np.ones(len(self.x_train), dtype=bool)
        mask[outlier_indices] = False
        
        # Apply the mask to create sanitized dataset
        x_train_sanitized = self.x_train[mask]
        y_train_sanitized = self.y_train_cat[mask]
        
        print(f"Sanitized dataset contains {len(x_train_sanitized)} samples")
        
        return x_train_sanitized, y_train_sanitized
    
    def build_model(self):
        """Build a simple CNN model for MNIST classification"""
        model = Sequential([
            Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
            MaxPooling2D((2, 2)),
            Conv2D(64, (3, 3), activation='relu'),
            MaxPooling2D((2, 2)),
            Flatten(),
            Dense(128, activation='relu'),
            Dropout(0.2),
            Dense(10, activation='softmax')
        ])
        
        model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def train_robust_model(self, x_train_sanitized, y_train_sanitized, batch_size=128, epochs=5):
        """Train a model on the sanitized dataset"""
        print("\nTraining robust model on sanitized data...")
        model = self.build_model()
        history = model.fit(
            x_train_sanitized, y_train_sanitized,
            batch_size=batch_size,
            epochs=epochs,
            validation_data=(self.x_test, self.y_test_cat),
            verbose=1
        )
        
        return model, history
    
    def evaluate_defense(self, robust_model, poisoned_model, source_class, target_class):
        """Evaluate the effectiveness of the defense"""
        # Evaluate on test data
        _, robust_acc = robust_model.evaluate(self.x_test, self.y_test_cat, verbose=0)
        _, poisoned_acc = poisoned_model.evaluate(self.x_test, self.y_test_cat, verbose=0)
        
        print("\n--- Defense Evaluation ---")
        print(f"Robust model test accuracy: {robust_acc:.4f}")
        print(f"Poisoned model test accuracy: {poisoned_acc:.4f}")
        
        # Filter test data for the source class
        source_indices = np.where(np.argmax(self.y_test_cat, axis=1) == source_class)[0]
        x_test_source = self.x_test[source_indices]
        y_test_source = self.y_test_cat[source_indices]
        
        # Check how many source class samples are classified as target class
        robust_preds = robust_model.predict(x_test_source)
        poisoned_preds = poisoned_model.predict(x_test_source)
        
        robust_target_rate = np.mean(np.argmax(robust_preds, axis=1) == target_class)
        poisoned_target_rate = np.mean(np.argmax(poisoned_preds, axis=1) == target_class)
        
        print(f"\nSource class ({source_class}) â†’ Target class ({target_class}) misclassification rate:")
        print(f"Robust model: {robust_target_rate:.4f}")
        print(f"Poisoned model: {poisoned_target_rate:.4f}")
        
        defense_effectiveness = poisoned_target_rate - robust_target_rate
        print(f"Defense effectiveness: {defense_effectiveness:.4f}")
        
        return {
            'robust_accuracy': robust_acc,
            'poisoned_accuracy': poisoned_acc,
            'robust_target_rate': robust_target_rate,
            'poisoned_target_rate': poisoned_target_rate,
            'defense_effectiveness': defense_effectiveness
        }
    
    def visualize_defense_results(self, outlier_indices, source_class, target_class, 
                                robust_model, poisoned_model, results, output_dir="defense_results"):
        """Visualize the defense results"""
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. Visualize some detected outliers waquas
        plt.figure(figsize=(10, 5))
        plt.suptitle("Samples Detected as Potential Poisoned Data", fontsize=16)
        
        # Display up to 10 outliers
        for i in range(min(10, len(outlier_indices))):
            plt.subplot(2, 5, i+1)
            plt.imshow(self.x_train[outlier_indices[i]].reshape(28, 28), cmap='gray')
            if len(self.y_train.shape) > 1:
                label = np.argmax(self.y_train[outlier_indices[i]])
            else:
                label = self.y_train[outlier_indices[i]]
            plt.title(f"Label: {label}")
            plt.axis('off')
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.85)
        plt.savefig(os.path.join(output_dir, "detected_outliers.png"))
        print(f"Saved detected outliers visualization to {os.path.join(output_dir, 'detected_outliers.png')}")
        plt.close()
        
        # 2. Visualize confusion matrices for robust and poisoned models
        plt.figure(figsize=(15, 6))
        
        # Robust model confusion matrix
        plt.subplot(1, 2, 1)
        robust_preds = np.argmax(robust_model.predict(self.x_test), axis=1)
        robust_cm = confusion_matrix(np.argmax(self.y_test_cat, axis=1), robust_preds)
        sns.heatmap(robust_cm, annot=True, fmt='d', cmap='Blues')
        plt.title("Robust Model Confusion Matrix")
        plt.ylabel("True Label")
        plt.xlabel("Predicted Label")
        
        # Poisoned model confusion matrix
        plt.subplot(1, 2, 2)
        poisoned_preds = np.argmax(poisoned_model.predict(self.x_test), axis=1)
        poisoned_cm = confusion_matrix(np.argmax(self.y_test_cat, axis=1), poisoned_preds)
        sns.heatmap(poisoned_cm, annot=True, fmt='d', cmap='Blues')
        plt.title("Poisoned Model Confusion Matrix")
        plt.ylabel("True Label")
        plt.xlabel("Predicted Label")
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, "confusion_matrices_comparison.png"))
        print(f"Saved confusion matrices comparison to {os.path.join(output_dir, 'confusion_matrices_comparison.png')}")
        plt.close()
        
        # 3. Visualize classification differences between robust and poisoned models
        plt.figure(figsize=(8, 6))
        
        # Filter test data for the source class
        source_indices = np.where(np.argmax(self.y_test_cat, axis=1) == source_class)[0]
        x_test_source = self.x_test[source_indices]
        
        # Get predictions
        robust_preds = robust_model.predict(x_test_source)
        poisoned_preds = poisoned_model.predict(x_test_source)
        
        # Calculate class prediction rates
        robust_pred_classes = np.argmax(robust_preds, axis=1)
        poisoned_pred_classes = np.argmax(poisoned_preds, axis=1)
        
        # Count predictions for each class
        robust_counts = np.zeros(10)
        poisoned_counts = np.zeros(10)
        
        for i in range(10):
            robust_counts[i] = np.sum(robust_pred_classes == i) / len(robust_pred_classes)
            poisoned_counts[i] = np.sum(poisoned_pred_classes == i) / len(poisoned_pred_classes)
        
        # Plot results
        bar_width = 0.35
        indices = np.arange(10)
        
        plt.bar(indices - bar_width/2, poisoned_counts, bar_width, label='Poisoned Model', color='r', alpha=0.6)
        plt.bar(indices + bar_width/2, robust_counts, bar_width, label='Robust Model', color='g', alpha=0.6)
        
        plt.xlabel('Predicted Class')
        plt.ylabel('Proportion of Source Class Samples')
        plt.title(f'Classification of Source Class ({source_class}) Before and After Defense')
        plt.xticks(indices)
        plt.legend()
        
        # Highlight the target class
        plt.axvline(x=target_class, color='b', linestyle='--', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, "defense_effectiveness.png"))
        print(f"Saved defense effectiveness visualization to {os.path.join(output_dir, 'defense_effectiveness.png')}")
        plt.close()
        
        # 4. Save numerical results to a text file
        with open(os.path.join(output_dir, "defense_results.txt"), "w") as f:
            f.write("POISONING DEFENSE RESULTS\n")
            f.write("========================\n\n")
            f.write(f"Source Class: {source_class}\n")
            f.write(f"Target Class: {target_class}\n")
            f.write(f"Number of detected outliers: {len(outlier_indices)}\n")
            f.write(f"Percentage of training data flagged: {len(outlier_indices)/len(self.x_train):.2%}\n\n")
            f.write("Model Performance:\n")
            f.write(f"  Poisoned Model Accuracy: {results['poisoned_accuracy']:.4f}\n")
            f.write(f"  Robust Model Accuracy: {results['robust_accuracy']:.4f}\n\n")
            f.write("Defense Effectiveness:\n")
            f.write(f"  Poisoned Model {source_class}â†’{target_class} Rate: {results['poisoned_target_rate']:.4f}\n")
            f.write(f"  Robust Model {source_class}â†’{target_class} Rate: {results['robust_target_rate']:.4f}\n")
            f.write(f"  Defense Effectiveness: {results['defense_effectiveness']:.4f}\n")
        print(f"Saved defense results to {os.path.join(output_dir, 'defense_results.txt')}")

def main():
    """Main function to run the defense demonstration"""
    parser = argparse.ArgumentParser(description='Defense Against Poisoning Attacks')
    parser.add_argument('--model-path', type=str, default='results/poisoned_model.h5', 
                        help='Path to the poisoned model (default: results/poisoned_model.h5)')
    parser.add_argument('--clean-model-path', type=str, 
                        help='Path to clean model for comparison (optional)')
    parser.add_argument('--source-class', type=int, default=7, 
                       help='Source class used in poisoning (default: 7)')
    parser.add_argument('--target-class', type=int, default=1, 
                       help='Target class used in poisoning (default: 1)')
    parser.add_argument('--detection-method', type=str, default='iforest', 
                       choices=['iforest', 'lof', 'label'], 
                       help='Method for detecting poisoned samples (default: iforest)')
    parser.add_argument('--contamination', type=float, default=0.05, 
                       help='Expected proportion of poisoned samples (default: 0.05)')
    parser.add_argument('--epochs', type=int, default=5, 
                       help='Number of training epochs (default: 5)')
    parser.add_argument('--output-dir', type=str, default='defense_results', 
                       help='Directory to save results (default: defense_results)')
    
    args = parser.parse_args()
    
    # Make output directory if it doesn't exist
    os.makedirs(args.output_dir, exist_ok=True)
    
    print("Loading MNIST dataset...")
    # Load MNIST dataset
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    
    # Preprocess the data
    x_train = x_train.astype('float32') / 255.0
    x_test = x_test.astype('float32') / 255.0
    x_train = x_train.reshape(-1, 28, 28, 1)
    x_test = x_test.reshape(-1, 28, 28, 1)
    
    # Load the poisoned model
    print(f"Loading poisoned model from {args.model_path}...")
    try:
        poisoned_model = load_model(args.model_path)
    except Exception as e:
        print(f"Error loading poisoned model: {e}")
        return
    
    # Initialize defense
    defense = PoisoningDefense(x_train, y_train, x_test, y_test)
    
    # Detect outliers using the specified method
    if args.detection_method == 'iforest':
        outlier_indices = defense.detect_outliers_iforest(contamination=args.contamination)
    elif args.detection_method == 'lof':
        outlier_indices = defense.detect_outliers_lof(contamination=args.contamination)
    elif args.detection_method == 'label':
        outlier_indices = defense.detect_label_outliers(k=5)
    else:
        print(f"Unknown detection method: {args.detection_method}")
        return
    
    # Create sanitized dataset
    x_train_sanitized, y_train_sanitized = defense.create_sanitized_dataset(outlier_indices)
    
    # Train robust model on sanitized data
    robust_model, history = defense.train_robust_model(
        x_train_sanitized, y_train_sanitized, epochs=args.epochs
    )
    
    # Evaluate defense effectiveness
    results = defense.evaluate_defense(
        robust_model, poisoned_model, args.source_class, args.target_class
    )
    
    # Visualize defense results
    defense.visualize_defense_results(
        outlier_indices, args.source_class, args.target_class,
        robust_model, poisoned_model, results, args.output_dir
    )
    
    # Save the robust model
    robust_model_path = os.path.join(args.output_dir, "robust_model.h5")
    robust_model.save(robust_model_path)
    print(f"Saved robust model to {robust_model_path}")
    
    print(f"\nDefense demonstration completed! Results saved to {args.output_dir}")
    
    return defense, robust_model, results

if __name__ == "__main__":
    defense, robust_model, results = main()
