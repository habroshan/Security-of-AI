#!/usr/bin/env python3
"""
Evasion Attack Demo - AI/ML Security Penetration Testing

This script demonstrates how to perform evasion attacks (adversarial examples) against
a machine learning model and how to implement defenses, based on the CyBOK Security 
and Privacy of AI Knowledge Guide.

Usage:
    python evasion_attack_demo.py --attack fgsm --epsilon 0.1 --defense adversarial_training
"""

import argparse
import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential, load_model, save_model
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.utils import to_categorical

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

class EvasionAttackDemo:
    """Class to demonstrate evasion attacks and defenses against ML models"""
    
    def __init__(self):
        """Initialize the demo with default parameters"""
        self.model = None
        self.x_train = None
        self.y_train = None
        self.x_test = None
        self.y_test = None
        self.results_dir = "results"
        
        # Create results directory if it doesn't exist
        os.makedirs(self.results_dir, exist_ok=True)
    
    def load_data(self):
        """Load and preprocess the MNIST dataset"""
        print("[*] Loading MNIST dataset...")
        
        # Load MNIST dataset
        (x_train, y_train), (x_test, y_test) = mnist.load_data()
        
        # Preprocess the data
        # Normalize pixel values to [0, 1]
        x_train = x_train.astype('float32') / 255.0
        x_test = x_test.astype('float32') / 255.0
        
        # Reshape to include channel dimension (MNIST is grayscale, so 1 channel)
        x_train = x_train.reshape(-1, 28, 28, 1)
        x_test = x_test.reshape(-1, 28, 28, 1)
        
        # Convert class vectors to binary class matrices (one-hot encoding)
        y_train = to_categorical(y_train, 10)
        y_test = to_categorical(y_test, 10)
        
        self.x_train = x_train
        self.y_train = y_train
        self.x_test = x_test
        self.y_test = y_test
        
        print(f"[+] Loaded {len(x_train)} training samples and {len(x_test)} test samples")
    
    def build_model(self):
        """Build a simple CNN model for MNIST classification"""
        print("[*] Building model...")
        
        # Define a simple CNN architecture
        model = Sequential([
            # First convolutional layer
            Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
            # Second convolutional layer
            Conv2D(64, kernel_size=(3, 3), activation='relu'),
            # Max pooling
            MaxPooling2D(pool_size=(2, 2)),
            # Dropout to prevent overfitting
            Dropout(0.25),
            # Flatten the output for the dense layers
            Flatten(),
            # Dense hidden layer
            Dense(128, activation='relu'),
            # Dropout again
            Dropout(0.5),
            # Output layer with 10 nodes (one for each digit)
            Dense(10, activation='softmax')
        ])
        
        # Compile the model
        model.compile(
            loss='categorical_crossentropy',
            optimizer='adam',
            metrics=['accuracy']
        )
        
        self.model = model
        print("[+] Model built successfully")
        
        return model
    
    def train_model(self, epochs=5):
        """Train the model on the MNIST dataset"""
        if self.model is None:
            self.build_model()
        
        print(f"[*] Training model for {epochs} epochs...")
        
        # Train the model
        self.model.fit(
            self.x_train, self.y_train,
            batch_size=128,
            epochs=epochs,
            validation_data=(self.x_test, self.y_test),
            verbose=1
        )
        
        # Evaluate the model
        scores = self.model.evaluate(self.x_test, self.y_test, verbose=0)
        print(f"[+] Test accuracy: {scores[1]:.4f}")
        
        # Save the model
        model_path = os.path.join(self.results_dir, "mnist_model.h5")
        self.model.save(model_path)
        print(f"[+] Model saved to {model_path}")
    
    def load_model(self, model_path=None):
        """Load a pre-trained model"""
        if model_path is None:
            model_path = os.path.join(self.results_dir, "mnist_model.h5")
        
        print(f"[*] Loading model from {model_path}...")
        
        try:
            self.model = load_model(model_path)
            print("[+] Model loaded successfully")
            
            # Evaluate the model
            if self.x_test is not None:
                scores = self.model.evaluate(self.x_test, self.y_test, verbose=0)
                print(f"[+] Test accuracy: {scores[1]:.4f}")
        except Exception as e:
            print(f"[!] Error loading model: {e}")
            return False
        
        return True
    
    def generate_adversarial_examples(self, attack_type='fgsm', epsilon=0.1, num_samples=10):
        """
        Generate adversarial examples using different attack methods
        
        Args:
            attack_type: Type of attack ('fgsm' or 'pgd')
            epsilon: Perturbation size
            num_samples: Number of adversarial examples to generate
        
        Returns:
            Dictionary containing original examples, adversarial examples, and results
        """
        if self.model is None:
            print("[!] Model not loaded. Please load or train a model first.")
            return None
        
        print(f"[*] Generating adversarial examples using {attack_type.upper()} attack (Îµ={epsilon})...")
        
        # Select a subset of test samples
        indices = np.random.choice(len(self.x_test), num_samples, replace=False)
        x_test_subset = self.x_test[indices]
        y_test_subset = self.y_test[indices]
        
        # Get predictions on clean samples
        y_pred_clean = self.model.predict(x_test_subset)
        clean_predicted_classes = np.argmax(y_pred_clean, axis=1)
        true_classes = np.argmax(y_test_subset, axis=1)
        
        # Only keep correctly classified samples
        correct_indices = np.where(clean_predicted_classes == true_classes)[0]
        x_correct = x_test_subset[correct_indices]
        y_correct = y_test_subset[correct_indices]
        
        if len(correct_indices) == 0:
            print("[!] No correctly classified samples found. Cannot generate adversarial examples.")
            return None
        
        print(f"[+] Using {len(correct_indices)} correctly classified samples")
        
        # Generate adversarial examples
        if attack_type.lower() == 'fgsm':
            x_adv = self.fast_gradient_sign_method(x_correct, y_correct, epsilon)
        elif attack_type.lower() == 'pgd':
            x_adv = self.projected_gradient_descent(x_correct, y_correct, epsilon, iterations=40)
        else:
            print(f"[!] Unknown attack type: {attack_type}")
            return None
        
        # Get predictions on adversarial examples
        y_pred_adv = self.model.predict(x_adv)
        adv_predicted_classes = np.argmax(y_pred_adv, axis=1)
        
        # Calculate attack success rate
        attack_success = np.sum(adv_predicted_classes != np.argmax(y_correct, axis=1))
        attack_success_rate = attack_success / len(adv_predicted_classes)
        
        print(f"[+] Attack success rate: {attack_success_rate:.4f} ({attack_success}/{len(adv_predicted_classes)})")
        
        # Save results
        result = {
            'attack_type': attack_type,
            'epsilon': epsilon,
            'num_samples': len(correct_indices),
            'original_examples': x_correct,
            'adversarial_examples': x_adv,
            'true_classes': np.argmax(y_correct, axis=1),
            'clean_predictions': np.argmax(self.model.predict(x_correct), axis=1),
            'adversarial_predictions': adv_predicted_classes,
            'attack_success_rate': attack_success_rate
        }
        
        return result
    
    def fast_gradient_sign_method(self, x, y, epsilon):
        """
        Implement the Fast Gradient Sign Method (FGSM) attack
        
        Args:
            x: Input samples
            y: Target labels
            epsilon: Perturbation size
            
        Returns:
            Adversarial examples
        """
        x_adv = np.zeros_like(x)
        
        for i in range(len(x)):
            # Convert to tensor
            x_tensor = tf.convert_to_tensor(x[i:i+1], dtype=tf.float32)
            y_tensor = tf.convert_to_tensor(y[i:i+1], dtype=tf.float32)
            
            # Record operations for automatic differentiation
            with tf.GradientTape() as tape:
                tape.watch(x_tensor)
                prediction = self.model(x_tensor)
                loss = tf.keras.losses.categorical_crossentropy(y_tensor, prediction)
            
            # Get the gradients of the loss w.r.t to the input image
            gradient = tape.gradient(loss, x_tensor)
            
            # Get the sign of the gradients
            signed_grad = tf.sign(gradient)
            
            # Create adversarial example
            x_adv[i] = x[i] + epsilon * signed_grad.numpy()[0]
            
            # Ensure the adversarial example is in [0, 1]
            x_adv[i] = np.clip(x_adv[i], 0, 1)
        
        return x_adv
    
    def projected_gradient_descent(self, x, y, epsilon, iterations=40, alpha=0.01):
        """
        Implement the Projected Gradient Descent (PGD) attack
        
        Args:
            x: Input samples
            y: Target labels
            epsilon: Perturbation size
            iterations: Number of iterations
            alpha: Step size
            
        Returns:
            Adversarial examples
        """
        x_adv = x.copy()
        
        for i in range(len(x)):
            # Add small random noise to start (optional)
            x_adv[i] = x_adv[i] + np.random.uniform(-epsilon/2, epsilon/2, x_adv[i].shape)
            x_adv[i] = np.clip(x_adv[i], 0, 1)
            
            for j in range(iterations):
                # Convert to tensor
                x_tensor = tf.convert_to_tensor(x_adv[i:i+1], dtype=tf.float32)
                y_tensor = tf.convert_to_tensor(y[i:i+1], dtype=tf.float32)
                
                # Record operations for automatic differentiation
                with tf.GradientTape() as tape:
                    tape.watch(x_tensor)
                    prediction = self.model(x_tensor)
                    loss = tf.keras.losses.categorical_crossentropy(y_tensor, prediction)
                
                # Get the gradients of the loss w.r.t to the input image
                gradient = tape.gradient(loss, x_tensor)
                
                # Get the sign of the gradients
                signed_grad = tf.sign(gradient)
                
                # Create adversarial example (take a step in the direction of the gradient)
                x_adv[i] = x_adv[i] + alpha * signed_grad.numpy()[0]
                
                # Project back onto epsilon ball around original image
                x_adv[i] = np.clip(x_adv[i], x[i] - epsilon, x[i] + epsilon)
                
                # Ensure the adversarial example is in [0, 1]
                x_adv[i] = np.clip(x_adv[i], 0, 1)
        
        return x_adv
    
    def visualize_adversarial_examples(self, results, num_examples=5):
        """
        Visualize adversarial examples and their predictions
        
        Args:
            results: Dictionary containing adversarial examples and results
            num_examples: Number of examples to visualize
        """
        if results is None:
            print("[!] No results to visualize")
            return
        
        # Get a subset of examples to visualize
        num_to_show = min(num_examples, len(results['original_examples']))
        
        plt.figure(figsize=(12, num_to_show * 2))
        
        for i in range(num_to_show):
            # Original example
            plt.subplot(num_to_show, 3, i*3 + 1)
            plt.imshow(results['original_examples'][i].reshape(28, 28), cmap='gray')
            plt.title(f"Original\nTrue: {results['true_classes'][i]}\nPred: {results['clean_predictions'][i]}")
            plt.axis('off')
            
            # Adversarial example
            plt.subplot(num_to_show, 3, i*3 + 2)
            plt.imshow(results['adversarial_examples'][i].reshape(28, 28), cmap='gray')
            plt.title(f"Adversarial\nTrue: {results['true_classes'][i]}\nPred: {results['adversarial_predictions'][i]}")
            plt.axis('off')
            
            # Perturbation (scaled for visibility)
            plt.subplot(num_to_show, 3, i*3 + 3)
            perturbation = results['adversarial_examples'][i] - results['original_examples'][i]
            plt.imshow((perturbation * 10 + 0.5).reshape(28, 28), cmap='RdBu_r')
            plt.title(f"Perturbation\n(magnified 10x)")
            plt.axis('off')
        
        plt.tight_layout()
        
        # Save figure
        filename = f"{results['attack_type']}_eps{results['epsilon']}_examples.png"
        filepath = os.path.join(self.results_dir, filename)
        plt.savefig(filepath)
        print(f"[+] Adversarial examples saved to {filepath}")
        
        # Show plot
        plt.show()
    
    def implement_defense(self, defense_type, attack_type='fgsm', epsilon=0.1, defense_strength=0.5):
        """
        Implement defenses against adversarial examples
        
        Args:
            defense_type: Type of defense ('adversarial_training', 'input_preprocessing', 'gradient_masking')
            attack_type: Type of attack to defend against
            epsilon: Perturbation size used in defense
            defense_strength: Parameter controlling defense strength
            
        Returns:
            Defended model
        """
        if self.model is None:
            print("[!] Model not loaded. Please load or train a model first.")
            return None
        
        print(f"[*] Implementing {defense_type} defense...")
        
        if defense_type == 'adversarial_training':
            model_defended = self.adversarial_training(attack_type, epsilon)
        elif defense_type == 'input_preprocessing':
            model_defended = self.input_preprocessing_defense(defense_strength)
        elif defense_type == 'gradient_masking':
            model_defended = self.gradient_masking_defense()
        else:
            print(f"[!] Unknown defense type: {defense_type}")
            return None
        
        # Save the defended model
        model_path = os.path.join(self.results_dir, f"mnist_model_{defense_type}.h5")
        model_defended.save(model_path)
        print(f"[+] Defended model saved to {model_path}")
        
        # Update the model
        self.model = model_defended
        
        return model_defended
    
    def adversarial_training(self, attack_type='fgsm', epsilon=0.1, epochs=5):
        """
        Implement adversarial training defense
        
        Args:
            attack_type: Type of attack to defend against
            epsilon: Perturbation size
            epochs: Number of training epochs
            
        Returns:
            Defended model
        """
        print(f"[*] Implementing adversarial training with {attack_type.upper()} (Îµ={epsilon})...")
        
        # Copy the original model
        model_copy = tf.keras.models.clone_model(self.model)
        model_copy.compile(
            loss='categorical_crossentropy',
            optimizer='adam',
            metrics=['accuracy']
        )
        
        # Set model weights
        model_copy.set_weights(self.model.get_weights())
        
        # Number of batches and batch size
        batch_size = 128
        num_batches = len(self.x_train) // batch_size
        
        for epoch in range(epochs):
            print(f"[*] Adversarial training epoch {epoch+1}/{epochs}")
            
            # Shuffle the training data
            indices = np.random.permutation(len(self.x_train))
            x_train_shuffled = self.x_train[indices]
            y_train_shuffled = self.y_train[indices]
            
            for batch in range(num_batches):
                start = batch * batch_size
                end = min(start + batch_size, len(self.x_train))
                
                x_batch = x_train_shuffled[start:end]
                y_batch = y_train_shuffled[start:end]
                
                # Generate adversarial examples for this batch
                if attack_type.lower() == 'fgsm':
                    x_adv_batch = self.fast_gradient_sign_method(x_batch, y_batch, epsilon)
                elif attack_type.lower() == 'pgd':
                    x_adv_batch = self.projected_gradient_descent(x_batch, y_batch, epsilon, iterations=10)
                else:
                    print(f"[!] Unknown attack type: {attack_type}")
                    return None
                
                # Combine clean and adversarial examples
                x_combined = np.vstack([x_batch, x_adv_batch])
                y_combined = np.vstack([y_batch, y_batch])
                
                # Train on the combined batch
                model_copy.train_on_batch(x_combined, y_combined)
            
            # Evaluate the model after each epoch
            clean_acc = model_copy.evaluate(self.x_test, self.y_test, verbose=0)[1]
            
            # Generate adversarial examples for the test set
            if attack_type.lower() == 'fgsm':
                x_test_adv = self.fast_gradient_sign_method(self.x_test[:1000], self.y_test[:1000], epsilon)
            else:
                x_test_adv = self.projected_gradient_descent(self.x_test[:1000], self.y_test[:1000], epsilon, iterations=10)
            
            adv_acc = model_copy.evaluate(x_test_adv, self.y_test[:1000], verbose=0)[1]
            
            print(f"[+] Epoch {epoch+1} - Clean accuracy: {clean_acc:.4f}, Adversarial accuracy: {adv_acc:.4f}")
        
        # Final evaluation
        clean_acc = model_copy.evaluate(self.x_test, self.y_test, verbose=0)[1]
        
        # Generate adversarial examples for the full test set
        if attack_type.lower() == 'fgsm':
            x_test_adv = self.fast_gradient_sign_method(self.x_test, self.y_test, epsilon)
        else:
            x_test_adv = self.projected_gradient_descent(self.x_test, self.y_test, epsilon, iterations=10)
        
        adv_acc = model_copy.evaluate(x_test_adv, self.y_test, verbose=0)[1]
        
        print(f"[+] Adversarial training completed")
        print(f"[+] Final clean accuracy: {clean_acc:.4f}")
        print(f"[+] Final adversarial accuracy: {adv_acc:.4f}")
        
        return model_copy
    
    def input_preprocessing_defense(self, strength=0.5):
        """
        Implement input preprocessing defense
        
        Args:
            strength: Strength of the preprocessing (0-1)
            
        Returns:
            Defended model (with preprocessing wrapper)
        """
        print(f"[*] Implementing input preprocessing defense (strength={strength})...")
        
        # Create a preprocessing wrapper model
        preprocessing_model = tf.keras.Sequential([
            tf.keras.layers.GaussianNoise(strength * 0.2, input_shape=(28, 28, 1)),
            tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'),
            tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, 0, 1))
        ])
        
        # Combine preprocessing with the original model
        combined_model = tf.keras.Sequential([
            preprocessing_model,
            self.model
        ])
        
        combined_model.compile(
            loss='categorical_crossentropy',
            optimizer='adam',
            metrics=['accuracy']
        )
        
        # Evaluate the combined model
        clean_acc = combined_model.evaluate(self.x_test, self.y_test, verbose=0)[1]
        print(f"[+] Clean accuracy with preprocessing: {clean_acc:.4f}")
        
        return combined_model
    
    def gradient_masking_defense(self):
        """
        Implement gradient masking defense (defensive distillation)
        
        Returns:
            Defended model
        """
        print(f"[*] Implementing gradient masking defense (defensive distillation)...")
        
        # Get soft labels from the original model
        soft_labels = self.model.predict(self.x_train)
        
        # Temperature parameter for distillation
        temperature = 10
        
        # Create a new model with the same architecture
        distilled_model = tf.keras.models.clone_model(self.model)
        distilled_model.compile(
            loss='categorical_crossentropy',
            optimizer='adam',
            metrics=['accuracy']
        )
        
        # Train the distilled model on soft labels
        distilled_model.fit(
            self.x_train, soft_labels,
            batch_size=128,
            epochs=5,
            validation_data=(self.x_test, self.y_test),
            verbose=1
        )
        
        # Evaluate the distilled model
        clean_acc = distilled_model.evaluate(self.x_test, self.y_test, verbose=0)[1]
        print(f"[+] Clean accuracy with defensive distillation: {clean_acc:.4f}")
        
        return distilled_model
    
    def evaluate_defense(self, defense_type, attack_type='fgsm', epsilon=0.1, num_samples=100):
        """
        Evaluate the effectiveness of a defense against adversarial examples
        
        Args:
            defense_type: Type of defense
            attack_type: Type of attack
            epsilon: Perturbation size
            num_samples: Number of samples to evaluate
            
        Returns:
            Evaluation results
        """
        if self.model is None:
            print("[!] Model not loaded. Please load or train a model first.")
            return None
        
        print(f"[*] Evaluating {defense_type} defense against {attack_type.upper()} attack (Îµ={epsilon})...")
        
        # Load or create the defended model
        model_path = os.path.join(self.results_dir, f"mnist_model_{defense_type}.h5")
        if os.path.exists(model_path):
            defended_model = load_model(model_path)
            print(f"[+] Loaded defended model from {model_path}")
        else:
            defended_model = self.implement_defense(defense_type, attack_type, epsilon)
        
        # Select a subset of test samples
        indices = np.random.choice(len(self.x_test), num_samples, replace=False)
        x_test_subset = self.x_test[indices]
        y_test_subset = self.y_test[indices]
        
        # Generate adversarial examples for the undefended model
        if attack_type.lower() == 'fgsm':
            x_adv = self.fast_gradient_sign_method(x_test_subset, y_test_subset, epsilon)
        elif attack_type.lower() == 'pgd':
            x_adv = self.projected_gradient_descent(x_test_subset, y_test_subset, epsilon, iterations=40)
        else:
            print(f"[!] Unknown attack type: {attack_type}")
            return None
        
        # Evaluate on clean test data
        clean_acc_original = self.model.evaluate(x_test_subset, y_test_subset, verbose=0)[1]
        clean_acc_defended = defended_model.evaluate(x_test_subset, y_test_subset, verbose=0)[1]
        
        # Evaluate on adversarial examples
        adv_acc_original = self.model.evaluate(x_adv, y_test_subset, verbose=0)[1]
        adv_acc_defended = defended_model.evaluate(x_adv, y_test_subset, verbose=0)[1]
        
        print(f"[+] Clean accuracy - Original: {clean_acc_original:.4f}, Defended: {clean_acc_defended:.4f}")
        print(f"[+] Adversarial accuracy - Original: {adv_acc_original:.4f}, Defended: {adv_acc_defended:.4f}")
        
        # Calculate improvement
        improvement = adv_acc_defended - adv_acc_original
        print(f"[+] Defense improvement: {improvement:.4f} ({improvement*100:.1f}%)")
        
        # Results
        results = {
            'defense_type': defense_type,
            'attack_type': attack_type,
            'epsilon': epsilon,
            'clean_acc_original': clean_acc_original,
            'clean_acc_defended': clean_acc_defended,
            'adv_acc_original': adv_acc_original,
            'adv_acc_defended': adv_acc_defended,
            'improvement': improvement
        }
        
        # Generate new adversarial examples against the defended model
        # This checks if the attacker can adapt to the defense
        print("[*] Testing adaptive attack against defense...")
        
        # Temporarily set the model to the defended model to generate examples
        temp_model = self.model
        self.model = defended_model
        
        if attack_type.lower() == 'fgsm':
            x_adv_adaptive = self.fast_gradient_sign_method(x_test_subset, y_test_subset, epsilon)
        elif attack_type.lower() == 'pgd':
            x_adv_adaptive = self.projected_gradient_descent(x_test_subset, y_test_subset, epsilon, iterations=40)
        
        # Restore the original model
        self.model = temp_model
        
        # Evaluate against adaptive attack
        adv_acc_adaptive = defended_model.evaluate(x_adv_adaptive, y_test_subset, verbose=0)[1]
        print(f"[+] Adaptive attack accuracy: {adv_acc_adaptive:.4f}")
        
        results['adv_acc_adaptive'] = adv_acc_adaptive
        
        return results
    
    def run_demo(self, attack_type='fgsm', epsilon=0.1, defense_type='adversarial_training'):
        """
        Run a complete demo of an evasion attack and defense
        
        Args:
            attack_type: Type of attack ('fgsm' or 'pgd')
            epsilon: Perturbation size
            defense_type: Type of defense
        """
        # Step 1: Load data
        self.load_data()
        
        # Step 2: Train or load model
        model_path = os.path.join(self.results_dir, "mnist_model.h5")
        if os.path.exists(model_path):
            self.load_model(model_path)
        else:
            self.train_model()
        
        # Step 3: Generate adversarial examples
        results = self.generate_adversarial_examples(attack_type, epsilon)
        
        # Step 4: Visualize adversarial examples
        self.visualize_adversarial_examples(results)
        
        # Step 5: Implement defense
        self.implement_defense(defense_type, attack_type, epsilon)
        
        # Step 6: Evaluate defense
        self.evaluate_defense(defense_type, attack_type, epsilon)

def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Evasion Attack Demo - AI/ML Security Penetration Testing')
    parser.add_argument('--attack', type=str, choices=['fgsm', 'pgd'], default='fgsm',
                        help='Type of evasion attack to demonstrate')
    parser.add_argument('--epsilon', type=float, default=0.1,
                        help='Perturbation size for the attack')
    parser.add_argument('--defense', type=str, choices=['adversarial_training', 'input_preprocessing', 'gradient_masking'],
                        default='adversarial_training', help='Type of defense to implement')
    parser.add_argument('--train', action='store_true',
                        help='Train a new model instead of loading an existing one')
    parser.add_argument('--evaluate-only', action='store_true',
                        help='Only evaluate without implementing defense')
    args = parser.parse_args()
    
    # Print banner
    print("=" * 80)
    print("Evasion Attack Demo - AI/ML Security Penetration Testing")
    print("Based on CyBOK Security and Privacy of AI Knowledge Guide")
    print("=" * 80)
    
    # Create demo instance
    demo = EvasionAttackDemo()
    
    # Load MNIST data
    demo.load_data()
    
    # Train or load model
    model_path = os.path.join(demo.results_dir, "mnist_model.h5")
    if args.train or not os.path.exists(model_path):
        demo.train_model()
    else:
        demo.load_model(model_path)
    
    # Generate adversarial examples
    results = demo.generate_adversarial_examples(args.attack, args.epsilon)
    
    # Visualize adversarial examples
    if results:
        demo.visualize_adversarial_examples(results)
    
    # Implement and evaluate defense if requested
    if not args.evaluate_only:
        print(f"\n{'='*40}\nImplementing Defense\n{'='*40}")
        demo.implement_defense(args.defense, args.attack, args.epsilon)
    
    # Evaluate defense
    print(f"\n{'='*40}\nEvaluating Defense\n{'='*40}")
    defense_results = demo.evaluate_defense(args.defense, args.attack, args.epsilon)
    
    # Print final summary
    if defense_results:
        print("\n" + "=" * 80)
        print("EVASION ATTACK DEMO SUMMARY")
        print("=" * 80)
        print(f"Attack: {args.attack.upper()}, Epsilon: {args.epsilon}")
        print(f"Defense: {args.defense}")
        print("\nResults:")
        print(f"  Clean Accuracy (Original): {defense_results['clean_acc_original']:.4f}")
        print(f"  Clean Accuracy (Defended): {defense_results['clean_acc_defended']:.4f}")
        print(f"  Adversarial Accuracy (Original): {defense_results['adv_acc_original']:.4f}")
        print(f"  Adversarial Accuracy (Defended): {defense_results['adv_acc_defended']:.4f}")
        print(f"  Improvement: {defense_results['improvement']:.4f} ({defense_results['improvement']*100:.1f}%)")
        print(f"  Adaptive Attack Accuracy: {defense_results['adv_acc_adaptive']:.4f}")
        
        # Effectiveness rating
        if defense_results['improvement'] > 0.5:
            effectiveness = "HIGH"
        elif defense_results['improvement'] > 0.2:
            effectiveness = "MEDIUM"
        else:
            effectiveness = "LOW"
        
        print(f"\nDefense Effectiveness: {effectiveness}")
        
        # Recommendations
        print("\nRecommendations:")
        if args.defense == 'adversarial_training':
            print("  - Increase the diversity of adversarial examples during training")
            print("  - Try ensemble adversarial training with multiple attack types")
            print("  - Consider combining with input preprocessing for better robustness")
        elif args.defense == 'input_preprocessing':
            print("  - Tune preprocessing parameters for better trade-off between clean and adversarial accuracy")
            print("  - Consider combining with adversarial training for better robustness")
            print("  - Implement more sophisticated preprocessing techniques (e.g., JPEG compression)")
        elif args.defense == 'gradient_masking':
            print("  - Be aware that gradient masking may provide a false sense of security")
            print("  - Consider more robust defenses like adversarial training")
            print("  - Test against a wider range of adaptive attacks")
        
        print("\nNote: No defense is perfect. Continuous monitoring and updating of defenses is recommended.")
    
    print("\nDemo completed. Results saved in the 'results' directory.")

if __name__ == "__main__":
    main()
