# Model Stealing Attack and Defense Framework

This repository demonstrates model stealing attacks and defenses as described in the CyBOK Security and Privacy of AI Knowledge Guide (Section 5.3). It provides a complete framework for educational purposes to understand how model stealing works and how to defend against it.

## üìã Overview

This toolkit demonstrates:

1. **Model Stealing Attacks**: Extract a model's functionality through API queries
2. **Defense Mechanisms**: Implement countermeasures like rate limiting
3. **Educational Framework**: Learn ML security concepts hands-on

## üöÄ Quick Start

```bash
# Clone the repository
git clone https://github.com/yourusername/model-stealing-framework.git
cd model-stealing-framework

# Install dependencies
pip install tensorflow numpy matplotlib requests flask flask-cors scikit-learn

# Make scripts executable
chmod +x model_stealing_attack.py
chmod +x run-demo-script.sh
chmod +x deploy.py
chmod +x defense-script.py

# Start the target model API
python deploy.py

# In a new terminal, run the attack
./run-demo-script.sh
```

## üìö Components

The framework consists of:

### 1. Target Model API (`deploy.py`)

Provides model inference functionality through an API:
- `/info` - Model information
- `/predict_raw` - Prediction endpoint
- `/get_gradient` - Gradient information

### 2. Model Stealing Attack (`model_stealing_attack.py`)

Implements the attack workflow:
- Generates synthetic data
- Queries the target model
- Trains a substitute model
- Evaluates extraction success

### 3. Defense Mechanisms (`defense-script.py`)

Implements various defense strategies:
- Rate limiting
- Prediction confidence hiding
- Input/output perturbation
- Query tracking
- Model watermarking

### 4. Demo Runner (`run-demo-script.sh`)

Helper script to run the complete demonstration.

## üíª Usage

### Running the Undefended Target

```bash
python deploy.py
```

### Running with Defenses

```bash
# Run with all defenses
python defense-script.py --all-defenses

# Or select specific defenses
python defense-script.py --rate-limiting --hide-confidence
```

### Running the Attack

```bash
# Basic attack
./run-demo-script.sh

# Advanced options
./run-demo-script.sh --synthetic-samples 2000 --query-budget 5000 --epochs 15 --verbose
```

## üõ°Ô∏è Defense Options
run this for activating defense mechanism and then rerun the ```./run-demo-script.sh``` to verify the defense
```
python defense-script.py --all-defenses
```
The `defense-script.py` supports multiple defense mechanisms:

| Defense | Option | Description |
|---------|--------|-------------|
| Rate Limiting | `--rate-limiting` | Limits query frequency |
| Confidence Hiding | `--hide-confidence` | Returns only predicted class |
| Input Perturbation | `--input-perturbation` | Adds noise to inputs |
| Output Perturbation | `--output-perturbation` | Adds noise to predictions |
| Query Tracking | `--query-tracking` | Detects suspicious pattern |
| Model Watermarking | `--model-watermarking` | Embeds triggers in model |
| All Defenses | `--all-defenses` | Enables all defenses |

first Deploy the Model and then run 
## üìä Understanding Results

After running an attack, check the results directory (`results/model_stealing_*`) for:

- `stolen_model.h5`: The extracted model
- `prediction_comparison.png`: Visual comparison
- `accuracy_comparison.png`: Per-class accuracy
- `attack_metadata.json`: Detailed metrics

### Key Metrics

- **Model Agreement Rate**: How often the stolen and target models agree
- **Stolen Model Accuracy**: How well the stolen model performs
- **Query Count**: Number of API calls made for extraction

## üîç Handling Rate Limiting

When rate limiting is enabled, you'll see 429 errors. Options to handle this:

1. **Use an undefended target**: Run `deploy.py` instead of `defense-script.py`
2. **Reduce query rate**: Lower the number of samples with `--synthetic-samples 100`
3. **Add retry logic**: Modify the attack script to implement exponential backoff

Example retry logic:

```python
def query_with_retry(image, max_retries=5, delay=2):
    for attempt in range(max_retries):
        # Send request
        response = requests.post(...)
        
        # Handle rate limiting
        if response.status_code == 429:
            wait_time = delay * (attempt + 1)
            print(f"Rate limited. Waiting {wait_time} seconds...")
            time.sleep(wait_time)
            continue
            
        # Process successful response
        if response.status_code == 200:
            return response.json()
    
    return None
```

## üß™ Experimental Features

- **Model Fingerprinting**: Detect stolen models with watermark triggers
- **Adversarial Examples**: Test robustness of stolen models
- **Ensemble Methods**: Combine multiple defense strategies

## üìù Educational Context

This framework demonstrates the concepts described in the CyBOK Security and Privacy of AI Knowledge Guide (Section 5.3), specifically:

- How attackers can extract model functionality without access to parameters
- How organization can defend their ML intellectual property
- The trade-offs between model availability and security

## ‚ö†Ô∏è Disclaimer

This framework is provided strictly for educational purposes to demonstrate ML security concepts. Only use against models you own or have permission to test. Unauthorized model stealing may violate:

- Terms of service of ML APIs
- Intellectual property rights
- Computer fraud and abuse laws

## üìö References

1. CyBOK Security and Privacy of AI Knowledge Guide (Section 5.3)
2. F. Tram√®r, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart. "Stealing Machine Learning Models via Prediction APIs." USENIX Security, 2016.
3. T. Orekondy, B. Schiele, and M. Fritz. "Knockoff Nets: Stealing Functionality of Black-Box Models." CVPR, 2019.

## üìÑ License

This project is licensed for educational and research purposes only.
