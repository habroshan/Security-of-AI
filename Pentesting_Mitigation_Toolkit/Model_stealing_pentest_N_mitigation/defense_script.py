#!/usr/bin/env python3
"""
Model Stealing Defense Measures
==============================

This script demonstrates defense techniques against model stealing attacks
based on the CyBOK Security and Privacy of AI Knowledge Guide.

It modifies the model API to implement various defensive countermeasures.
"""

import os
import argparse
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from flask import Flask, request, jsonify, Response
from flask_cors import CORS
import json
import time
import random

# Initialize Flask app
app = Flask(__name__)
CORS(app)  # Enable CORS for all routes

# Global variables
MODEL = None
MODEL_INFO = None
CLASS_NAMES = None

# Defense settings
DEFENSE_SETTINGS = {
    'rate_limiting': {
        'enabled': False,
        'max_queries_per_minute': 60,
        'query_counter': 0,
        'last_reset_time': time.time()
    },
    'prediction_confidence': {
        'enabled': False,
        'return_scores': False
    },
    'input_perturbation': {
        'enabled': False,
        'noise_level': 0.05
    },
    'output_perturbation': {
        'enabled': False,
        'noise_level': 0.05
    },
    'ensemble': {
        'enabled': False,
        'models': []
    },
    'query_tracking': {
        'enabled': False,
        'tracking_data': [],
        'similarity_threshold': 0.95,
        'max_similar_queries': 10
    },
    'model_watermarking': {
        'enabled': False,
        'trigger_classes': {},  # Maps trigger values to target classes
        'trigger_strength': 0.1
    }
}

def load_model_and_info(model_path='models/mnist_cnn_model.h5'):
    """Load the trained model and model information"""
    global MODEL, MODEL_INFO, CLASS_NAMES
    
    print("Loading model...")
    MODEL = load_model(model_path)
    
    print("Loading model information...")
    model_info_path = os.path.join(os.path.dirname(model_path), 'model_info.pkl')
    if os.path.exists(model_info_path):
        import pickle
        with open(model_info_path, 'rb') as f:
            MODEL_INFO = pickle.load(f)
        CLASS_NAMES = MODEL_INFO['class_names']
    else:
        # Default model info if file not found
        MODEL_INFO = {
            'input_shape': (28, 28, 1),
            'class_names': [str(i) for i in range(10)],
            'test_accuracy': 0.99,
            'preprocessing': 'normalize between 0 and 1'
        }
        CLASS_NAMES = MODEL_INFO['class_names']
    
    print("Model loaded successfully!")
    print(f"Model input shape: {MODEL_INFO['input_shape']}")
    print(f"Number of classes: {len(CLASS_NAMES)}")
    print(f"Test accuracy: {MODEL_INFO['test_accuracy']}")

def check_rate_limiting():
    """
    Implement rate limiting defense
    
    Returns:
        True if rate limit exceeded, False otherwise
    """
    if not DEFENSE_SETTINGS['rate_limiting']['enabled']:
        return False
    
    current_time = time.time()
    time_diff = current_time - DEFENSE_SETTINGS['rate_limiting']['last_reset_time']
    
    # Reset counter every minute
    if time_diff > 60:
        DEFENSE_SETTINGS['rate_limiting']['query_counter'] = 0
        DEFENSE_SETTINGS['rate_limiting']['last_reset_time'] = current_time
    
    # Increment counter
    DEFENSE_SETTINGS['rate_limiting']['query_counter'] += 1
    
    # Check if limit exceeded
    if DEFENSE_SETTINGS['rate_limiting']['query_counter'] > DEFENSE_SETTINGS['rate_limiting']['max_queries_per_minute']:
        return True
    
    return False

def apply_input_perturbation(image):
    """
    Apply random noise to input image to defend against model stealing
    
    Args:
        image: Input image tensor
        
    Returns:
        Perturbed image
    """
    if not DEFENSE_SETTINGS['input_perturbation']['enabled']:
        return image
    
    noise_level = DEFENSE_SETTINGS['input_perturbation']['noise_level']
    noise = np.random.normal(0, noise_level, image.shape)
    perturbed_image = image + noise
    return np.clip(perturbed_image, 0, 1)

def apply_output_perturbation(predictions):
    """
    Apply random noise to model outputs to defend against model stealing
    
    Args:
        predictions: Model prediction probabilities
        
    Returns:
        Perturbed predictions
    """
    if not DEFENSE_SETTINGS['output_perturbation']['enabled']:
        return predictions
    
    noise_level = DEFENSE_SETTINGS['output_perturbation']['noise_level']
    noise = np.random.normal(0, noise_level, predictions.shape)
    
    # Add noise
    perturbed = predictions + noise
    
    # Re-normalize to ensure valid probability distribution
    perturbed = np.maximum(perturbed, 0)  # Ensure no negative values
    perturbed = perturbed / np.sum(perturbed, axis=1, keepdims=True)  # Normalize to sum to 1
    
    return perturbed

def check_query_tracking(image):
    """
    Track queries to detect potential model stealing attacks
    
    Args:
        image: Input image tensor
        
    Returns:
        True if suspicious query pattern detected, False otherwise
    """
    if not DEFENSE_SETTINGS['query_tracking']['enabled']:
        return False
    
    # Flatten image for easier comparison
    flat_image = image.flatten()
    
    # Compare with previous queries
    similar_queries = 0
    for prev_image in DEFENSE_SETTINGS['query_tracking']['tracking_data']:
        # Calculate cosine similarity
        similarity = np.dot(flat_image, prev_image) / (np.linalg.norm(flat_image) * np.linalg.norm(prev_image))
        
        if similarity > DEFENSE_SETTINGS['query_tracking']['similarity_threshold']:
            similar_queries += 1
    
    # Add current query to tracking data
    DEFENSE_SETTINGS['query_tracking']['tracking_data'].append(flat_image)
    
    # If too many similar queries, it might be a model stealing attack
    if similar_queries > DEFENSE_SETTINGS['query_tracking']['max_similar_queries']:
        return True
    
    return False

def check_model_watermarking(image):
    """
    Check if image contains a watermark trigger and return target class if it does
    
    Args:
        image: Input image tensor
        
    Returns:
        Target class if trigger detected, None otherwise
    """
    if not DEFENSE_SETTINGS['model_watermarking']['enabled']:
        return None
    
    trigger_classes = DEFENSE_SETTINGS['model_watermarking']['trigger_classes']
    
    # Check for triggers
    for trigger_value, target_class in trigger_classes.items():
        # For simplicity, we'll check a specific pixel pattern
        # In a real implementation, this would be more sophisticated
        trigger_region = image[0:5, 0:5]
        avg_value = np.mean(trigger_region)
        
        if abs(avg_value - float(trigger_value)) < 0.1:
            return target_class
    
    return None

@app.route('/info', methods=['GET'])
def get_model_info():
    """Return information about the model"""
    response = {
        'model_name': 'MNIST Digit Classifier (with defenses)',
        'input_shape': MODEL_INFO['input_shape'],
        'classes': CLASS_NAMES,
        'test_accuracy': float(MODEL_INFO['test_accuracy']),
        'preprocessing': MODEL_INFO['preprocessing']
    }
    return jsonify(response)

@app.route('/predict_raw', methods=['POST'])
def predict_raw():
    """
    Endpoint for predictions with defenses against model stealing attacks
    
    Accepts:
        JSON with 'pixels' field containing a list of pixel values (0-1)
    
    Returns:
        JSON with prediction results
    """
    try:
        # Check rate limiting
        if check_rate_limiting():
            return jsonify({'error': 'Rate limit exceeded. Please try again later.'}), 429
        
        # Get input data
        data = request.get_json()
        if 'pixels' not in data:
            return jsonify({'error': 'No pixel data provided'}), 400
        
        # Convert to numpy array and reshape
        pixels = np.array(data['pixels'], dtype='float32')
        
        # Reshape to match model input (batch_size, height, width, channels)
        input_shape = tuple([1] + list(MODEL_INFO['input_shape']))
        
        if pixels.size != np.prod(MODEL_INFO['input_shape']):
            return jsonify({'error': f"Pixel data must have {np.prod(MODEL_INFO['input_shape'])} elements"}), 400
        
        pixels = pixels.reshape(input_shape)
        
        # Check for suspicious query patterns
        if check_query_tracking(pixels):
            # For demonstration, we'll just log this but still proceed
            print("ALERT: Suspicious query pattern detected! Possible model stealing attack.")
        
        # Apply input perturbation (defensive measure)
        perturbed_input = apply_input_perturbation(pixels)
        
        # Check for watermark triggers
        watermark_class = check_model_watermarking(pixels)
        
        if watermark_class is not None:
            # Return the target class for watermarked inputs
            predictions = np.zeros((1, len(CLASS_NAMES)))
            predictions[0, watermark_class] = 1.0
        else:
            # Make prediction with the model
            if DEFENSE_SETTINGS['ensemble']['enabled'] and DEFENSE_SETTINGS['ensemble']['models']:
                # Ensemble prediction (if enabled)
                all_predictions = []
                all_predictions.append(MODEL.predict(perturbed_input))
                
                for ensemble_model in DEFENSE_SETTINGS['ensemble']['models']:
                    all_predictions.append(ensemble_model.predict(perturbed_input))
                
                # Average predictions
                predictions = np.mean(all_predictions, axis=0)
            else:
                # Single model prediction
                predictions = MODEL.predict(perturbed_input)
        
        # Apply output perturbation (defensive measure)
        predictions = apply_output_perturbation(predictions)
        
        # Get top predicted class and confidence
        predicted_class_idx = np.argmax(predictions[0])
        confidence = float(predictions[0][predicted_class_idx])
        
        # Prepare response
        response = {
            'predicted_class': CLASS_NAMES[predicted_class_idx],
            'confidence': confidence
        }
        
        # Only include class probabilities if enabled
        if DEFENSE_SETTINGS['prediction_confidence']['return_scores']:
            response['class_probabilities'] = {
                class_name: float(prob) for class_name, prob in zip(CLASS_NAMES, predictions[0])
            }
        
        return jsonify(response)
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500

def configure_defenses(args):
    """Configure defenses based on command line arguments"""
    if args.rate_limiting:
        DEFENSE_SETTINGS['rate_limiting']['enabled'] = True
        DEFENSE_SETTINGS['rate_limiting']['max_queries_per_minute'] = args.rate_limit_value
        print(f"Enabled rate limiting: {args.rate_limit_value} queries per minute")
    
    if args.hide_confidence:
        DEFENSE_SETTINGS['prediction_confidence']['enabled'] = True
        DEFENSE_SETTINGS['prediction_confidence']['return_scores'] = False
        print("Enabled confidence hiding: only returning predicted class")
    
    if args.input_perturbation:
        DEFENSE_SETTINGS['input_perturbation']['enabled'] = True
        DEFENSE_SETTINGS['input_perturbation']['noise_level'] = args.perturbation_level
        print(f"Enabled input perturbation with noise level: {args.perturbation_level}")
    
    if args.output_perturbation:
        DEFENSE_SETTINGS['output_perturbation']['enabled'] = True
        DEFENSE_SETTINGS['output_perturbation']['noise_level'] = args.perturbation_level
        print(f"Enabled output perturbation with noise level: {args.perturbation_level}")
    
    if args.query_tracking:
        DEFENSE_SETTINGS['query_tracking']['enabled'] = True
        DEFENSE_SETTINGS['query_tracking']['similarity_threshold'] = args.similarity_threshold
        DEFENSE_SETTINGS['query_tracking']['max_similar_queries'] = args.max_similar_queries
        print(f"Enabled query tracking: similarity threshold = {args.similarity_threshold}, max similar queries = {args.max_similar_queries}")
    
    if args.model_watermarking:
        DEFENSE_SETTINGS['model_watermarking']['enabled'] = True
        # Set up some example trigger patterns
        for i in range(3):
            trigger_value = 0.2 * (i + 1)  # Triggers at 0.2, 0.4, 0.6
            target_class = random.randint(0, 9)  # Random target class
            DEFENSE_SETTINGS['model_watermarking']['trigger_classes'][trigger_value] = target_class
        print("Enabled model watermarking with random triggers")
    
    if args.all_defenses:
        # Enable all defenses with default settings
        DEFENSE_SETTINGS['rate_limiting']['enabled'] = True
        DEFENSE_SETTINGS['prediction_confidence']['enabled'] = True
        DEFENSE_SETTINGS['prediction_confidence']['return_scores'] = False
        DEFENSE_SETTINGS['input_perturbation']['enabled'] = True
        DEFENSE_SETTINGS['output_perturbation']['enabled'] = True
        DEFENSE_SETTINGS['query_tracking']['enabled'] = True
        DEFENSE_SETTINGS['model_watermarking']['enabled'] = True
        print("Enabled all defense mechanisms with default settings")

def main():
    """Main function to run the defended model API"""
    parser = argparse.ArgumentParser(description="Model Stealing Defense Demonstration")
    
    parser.add_argument("--model-path", type=str, default="models/mnist_cnn_model.h5", 
                        help="Path to the model file")
    
    parser.add_argument("--port", type=int, default=5000,
                        help="Port to run the API server on")
    
    # Defense mechanism options
    parser.add_argument("--rate-limiting", action="store_true",
                        help="Enable rate limiting defense")
    
    parser.add_argument("--rate-limit-value", type=int, default=60,
                        help="Maximum number of queries per minute (for rate limiting)")
    
    parser.add_argument("--hide-confidence", action="store_true",
                        help="Hide confidence scores in predictions")
    
    parser.add_argument("--input-perturbation", action="store_true",
                        help="Enable input perturbation defense")
    
    parser.add_argument("--output-perturbation", action="store_true",
                        help="Enable output perturbation defense")
    
    parser.add_argument("--perturbation-level", type=float, default=0.05,
                        help="Noise level for perturbation defenses")
    
    parser.add_argument("--query-tracking", action="store_true",
                        help="Enable query tracking defense")
    
    parser.add_argument("--similarity-threshold", type=float, default=0.95,
                        help="Similarity threshold for query tracking")
    
    parser.add_argument("--max-similar-queries", type=int, default=10,
                        help="Maximum number of similar queries before flagging")
    
    parser.add_argument("--model-watermarking", action="store_true",
                        help="Enable model watermarking defense")
    
    parser.add_argument("--all-defenses", action="store_true",
                        help="Enable all defense mechanisms")
    
    args = parser.parse_args()
    
    # Print banner
    print("""
    ╔══════════════════════════════════════════════════════════════════════╗
    ║              MODEL STEALING DEFENSE DEMONSTRATION                     ║
    ║                                                                      ║
    ║      Based on CyBOK Security and Privacy of AI Knowledge Guide       ║
    ╚══════════════════════════════════════════════════════════════════════╝
    """)
    
    # Load model
    load_model_and_info(args.model_path)
    
    # Configure defenses
    configure_defenses(args)
    
    # Print enabled defenses
    print("\nEnabled defenses:")
    for defense, settings in DEFENSE_SETTINGS.items():
        if settings.get('enabled', False):
            print(f"- {defense.replace('_', ' ').title()}")
    
    # Run the Flask app
    print(f"\nStarting defended API server on port {args.port}...")
    app.run(host='0.0.0.0', port=args.port, debug=False)

if __name__ == "__main__":
    main()
