#!/usr/bin/env python3
"""
Model Stealing Attack Demonstration
===================================

This script demonstrates a model stealing attack against a deployed ML model
by creating a substitute model based on query responses.

Based on the CyBOK Security and Privacy of AI Knowledge Guide (Section 5.3)
"""

import os
import argparse
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import requests
import json
from sklearn.metrics import accuracy_score, classification_report

class ModelStealingAttack:
    """
    Implements model stealing (extraction) attack against ML models.
    
    Model stealing attacks aim to create a "knock-off" model that replicates
    the functionality of a target model without access to its architecture
    or training data, just by observing its outputs on various inputs.
    """
    
    def __init__(self, target_api_url, output_dir, verbose=False):
        """
        Initialize the model stealing attack.
        
        Args:
            target_api_url: URL of the target model's prediction API
            output_dir: Directory to save outputs
            verbose: Whether to print detailed progress
        """
        self.target_api_url = target_api_url
        self.output_dir = output_dir
        self.verbose = verbose
        self.stolen_model = None
        self.query_count = 0
        self.query_results = []
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"[+] Model stealing attack initialized against {target_api_url}")
        print(f"[+] Results will be saved to {output_dir}")
    
    def log(self, message):
        """Print message if in verbose mode"""
        if self.verbose:
            print(message)
    
    def query_target_model(self, images):
        """
        Query the target model's API with input images
        
        Args:
            images: Input images to query
            
        Returns:
            List of predictions from the target model
        """
        predictions = []
        
        for i, image in enumerate(images):
            # Increment query counter
            self.query_count += 1
            
            # Convert image to list for JSON serialization
            pixels = image.flatten().tolist()
            
            # Prepare request payload
            payload = {"pixels": pixels}
            
            try:
                # Send request to prediction endpoint
                response = requests.post(
                    f"{self.target_api_url}/predict_raw",
                    json=payload
                )
                
                # Check if request was successful
                if response.status_code == 200:
                    result = response.json()
                    predictions.append(result)
                    
                    # Store the query result
                    self.query_results.append({
                        "image": pixels,
                        "prediction": result
                    })
                    
                    if i % 100 == 0:
                        self.log(f"[*] Queried {i+1}/{len(images)} images")
                else:
                    print(f"[!] Error querying target model: {response.status_code}")
                    print(response.text)
            except Exception as e:
                print(f"[!] Exception querying target model: {e}")
        
        return predictions
    
    def generate_synthetic_data(self, num_samples=1000):
        """
        Generate synthetic data for querying the target model
        
        Args:
            num_samples: Number of synthetic samples to generate
            
        Returns:
            Synthetic input data
        """
        print(f"[+] Generating {num_samples} synthetic data samples")
        
        # For MNIST-like models, generate random noise and patterns
        synthetic_samples = []
        
        # Generate different types of samples for better coverage
        
        # 1. Random noise
        num_noise = num_samples // 3
        noise_samples = np.random.rand(num_noise, 28, 28, 1) * 0.5
        
        # 2. Structured patterns (like digits)
        num_patterns = num_samples // 3
        pattern_samples = []
        
        for _ in range(num_patterns):
            # Create a blank 28x28 image
            pattern = np.zeros((28, 28, 1))
            
            # Add random lines or shapes
            num_lines = np.random.randint(1, 5)
            for _ in range(num_lines):
                r1, c1 = np.random.randint(0, 28, 2)
                r2, c2 = np.random.randint(0, 28, 2)
                
                # Draw a line from (r1,c1) to (r2,c2)
                rr, cc = np.linspace(r1, r2, max(abs(r2-r1), abs(c2-c1))*2).astype(int), \
                         np.linspace(c1, c2, max(abs(r2-r1), abs(c2-c1))*2).astype(int)
                
                # Keep indices in bounds
                valid_idx = (rr >= 0) & (rr < 28) & (cc >= 0) & (cc < 28)
                rr, cc = rr[valid_idx], cc[valid_idx]
                
                pattern[rr, cc, 0] = 1.0
            
            pattern_samples.append(pattern)
        
        pattern_samples = np.array(pattern_samples)
        
        # 3. Mixed samples (noise + pattern)
        num_mixed = num_samples - num_noise - num_patterns
        mixed_samples = []
        
        for _ in range(num_mixed):
            # Create a base of low noise
            mixed = np.random.rand(28, 28, 1) * 0.2
            
            # Add some structures
            for _ in range(np.random.randint(1, 3)):
                r, c = np.random.randint(5, 23, 2)
                size = np.random.randint(3, 8)
                
                # Add a square or circle
                if np.random.rand() > 0.5:
                    # Square
                    mixed[r:r+size, c:c+size] = 1.0
                else:
                    # Circle-like
                    for dr in range(-size//2, size//2+1):
                        for dc in range(-size//2, size//2+1):
                            if r+dr >= 0 and r+dr < 28 and c+dc >= 0 and c+dc < 28:
                                if dr*dr + dc*dc <= (size//2)**2:
                                    mixed[r+dr, c+dc] = 1.0
            
            mixed_samples.append(mixed)
        
        mixed_samples = np.array(mixed_samples)
        
        # Combine all samples
        synthetic_samples = np.vstack([noise_samples, pattern_samples, mixed_samples])
        
        # Clip to valid range [0, 1]
        synthetic_samples = np.clip(synthetic_samples, 0, 1)
        
        print(f"[+] Generated {len(synthetic_samples)} synthetic samples")
        
        return synthetic_samples
    
    def build_stolen_model(self, input_shape=(28, 28, 1), num_classes=10):
        """
        Build a substitute model architecture
        
        Args:
            input_shape: Input shape of the model
            num_classes: Number of output classes
            
        Returns:
            Compiled model ready for training
        """
        model = tf.keras.Sequential([
            # Convolutional layers
            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),
            tf.keras.layers.MaxPooling2D((2, 2)),
            tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
            tf.keras.layers.MaxPooling2D((2, 2)),
            
            # Fully connected layers
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(num_classes, activation='softmax')
        ])
        
        # Compile the model
        model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def extract_training_data(self, predictions):
        """
        Extract training data from predictions
        
        Args:
            predictions: List of prediction results from target model
            
        Returns:
            X: Input images
            y: One-hot encoded labels
        """
        X = []
        y = []
        
        for i, result in enumerate(self.query_results):
            X.append(np.array(result["image"]).reshape(28, 28, 1))
            
            # Extract class probabilities
            probs = [result["prediction"]["class_probabilities"][str(i)] for i in range(10)]
            y.append(probs)
        
        return np.array(X), np.array(y)
    
    def train_stolen_model(self, X, y, epochs=10, batch_size=32):
        """
        Train the stolen model on the collected data
        
        Args:
            X: Input images
            y: Target labels/probabilities
            epochs: Number of training epochs
            batch_size: Batch size for training
            
        Returns:
            Training history
        """
        print(f"[+] Training stolen model on {len(X)} samples")
        
        # Build the model if not already built
        if self.stolen_model is None:
            self.stolen_model = self.build_stolen_model()
        
        # Train the model
        history = self.stolen_model.fit(
            X, y,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=0.2,
            verbose=1 if self.verbose else 0
        )
        
        return history
    
    def evaluate_stolen_model(self, X_test, y_test):
        """
        Evaluate the stolen model on test data
        
        Args:
            X_test: Test images
            y_test: Test labels
            
        Returns:
            Evaluation metrics
        """
        print(f"[+] Evaluating stolen model on {len(X_test)} test samples")
        
        # Get predictions from the stolen model
        stolen_preds = self.stolen_model.predict(X_test)
        stolen_classes = np.argmax(stolen_preds, axis=1)
        
        # Get predictions from the target model
        target_results = self.query_target_model(X_test)
        target_classes = [int(result["predicted_class"]) for result in target_results]
        
        # Calculate agreement rate between stolen and target models
        agreement_rate = np.mean(stolen_classes == target_classes)
        print(f"[+] Model agreement rate: {agreement_rate:.4f}")
        
        # If true test labels are available
        true_classes = np.argmax(y_test, axis=1) if len(y_test.shape) > 1 else y_test
        stolen_accuracy = accuracy_score(true_classes, stolen_classes)
        target_accuracy = accuracy_score(true_classes, target_classes)
        
        print(f"[+] Stolen model accuracy: {stolen_accuracy:.4f}")
        print(f"[+] Target model accuracy: {target_accuracy:.4f}")
        
        # Generate classification report
        report = classification_report(true_classes, stolen_classes)
        print("\nClassification Report for Stolen Model:")
        print(report)
        
        return {
            "agreement_rate": agreement_rate,
            "stolen_accuracy": stolen_accuracy,
            "target_accuracy": target_accuracy,
            "classification_report": report
        }
    
    def save_stolen_model(self):
        """Save the stolen model"""
        if self.stolen_model is None:
            print("[!] No stolen model to save")
            return
        
        model_path = os.path.join(self.output_dir, "stolen_model.h5")
        self.stolen_model.save(model_path)
        print(f"[+] Stolen model saved to {model_path}")
    
    def visualize_results(self, X_test, y_test):
        """
        Visualize and compare prediction results
        
        Args:
            X_test: Test images
            y_test: Test labels
        """
        if self.stolen_model is None:
            print("[!] No stolen model to visualize")
            return
        
        # Select a few random test samples
        indices = np.random.choice(len(X_test), min(10, len(X_test)), replace=False)
        
        # Get predictions
        stolen_preds = self.stolen_model.predict(X_test[indices])
        stolen_classes = np.argmax(stolen_preds, axis=1)
        
        # Query target model for the same samples
        target_results = self.query_target_model(X_test[indices])
        target_classes = [int(result["predicted_class"]) for result in target_results]
        
        # Get true labels
        true_classes = np.argmax(y_test[indices], axis=1) if len(y_test.shape) > 1 else y_test[indices]
        
        # Create visualization
        fig, axes = plt.subplots(len(indices), 3, figsize=(12, 2*len(indices)))
        
        for i, idx in enumerate(indices):
            # Display the image
            axes[i, 0].imshow(X_test[idx].reshape(28, 28), cmap='gray')
            axes[i, 0].set_title(f"True: {true_classes[i]}")
            axes[i, 0].axis('off')
            
            # Display target model prediction
            axes[i, 1].imshow(X_test[idx].reshape(28, 28), cmap='gray')
            match = "✓" if target_classes[i] == true_classes[i] else "✗"
            axes[i, 1].set_title(f"Target: {target_classes[i]} {match}")
            axes[i, 1].axis('off')
            
            # Display stolen model prediction
            axes[i, 2].imshow(X_test[idx].reshape(28, 28), cmap='gray')
            match = "✓" if stolen_classes[i] == true_classes[i] else "✗"
            axes[i, 2].set_title(f"Stolen: {stolen_classes[i]} {match}")
            axes[i, 2].axis('off')
        
        plt.tight_layout()
        
        # Save the figure
        fig_path = os.path.join(self.output_dir, "prediction_comparison.png")
        plt.savefig(fig_path)
        print(f"[+] Saved prediction comparison to {fig_path}")
        
        # Plot accuracy comparison
        plt.figure(figsize=(8, 6))
        
        # Get per-class accuracy for stolen model
        class_accuracy_stolen = []
        class_accuracy_target = []
        
        for c in range(10):
            class_indices = np.where(true_classes == c)[0]
            if len(class_indices) > 0:
                acc_stolen = np.mean(stolen_classes[class_indices] == c)
                acc_target = np.mean(np.array(target_classes)[class_indices] == c)
                class_accuracy_stolen.append(acc_stolen)
                class_accuracy_target.append(acc_target)
            else:
                class_accuracy_stolen.append(0)
                class_accuracy_target.append(0)
        
        x = np.arange(10)
        width = 0.35
        
        plt.bar(x - width/2, class_accuracy_target, width, label='Target Model')
        plt.bar(x + width/2, class_accuracy_stolen, width, label='Stolen Model')
        
        plt.xlabel('Digit Class')
        plt.ylabel('Accuracy')
        plt.title('Per-Class Accuracy Comparison')
        plt.xticks(x, [str(i) for i in range(10)])
        plt.ylim(0, 1.1)
        plt.legend()
        
        # Save the figure
        fig_path = os.path.join(self.output_dir, "accuracy_comparison.png")
        plt.savefig(fig_path)
        print(f"[+] Saved accuracy comparison to {fig_path}")
    
    def save_attack_metadata(self, metrics):
        """
        Save attack metadata and metrics
        
        Args:
            metrics: Evaluation metrics
        """
        metadata = {
            "target_api": self.target_api_url,
            "query_count": self.query_count,
            "metrics": metrics,
            "model_architecture": self.stolen_model.to_json() if self.stolen_model else None
        }
        
        metadata_path = os.path.join(self.output_dir, "attack_metadata.json")
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"[+] Saved attack metadata to {metadata_path}")
    
    def execute(self, num_synthetic_samples=1000, query_budget=5000, epochs=10):
        """
        Execute the model stealing attack
        
        Args:
            num_synthetic_samples: Number of synthetic samples to generate
            query_budget: Maximum number of queries to the target model
            epochs: Number of training epochs
            
        Returns:
            Dictionary containing attack results
        """
        print(f"[+] Executing model stealing attack with {num_synthetic_samples} synthetic samples")
        print(f"[+] Query budget: {query_budget}")
        
        # Step 1: Generate synthetic data
        synthetic_data = self.generate_synthetic_data(num_synthetic_samples)
        
        # Step 2: Query the target model (respecting the budget)
        print(f"[+] Querying target model with synthetic data")
        self.query_target_model(synthetic_data[:min(len(synthetic_data), query_budget)])
        
        # Step 3: Extract training data from queries
        X_train, y_train = self.extract_training_data(self.query_results)
        
        # Step 4: Train the stolen model
        self.train_stolen_model(X_train, y_train, epochs=epochs)
        
        # Step 5: Load some test data for evaluation
        # In a real scenario, this would be a separate test set or validation data
        # For this demo, we'll use MNIST test data
        (_, _), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
        X_test = X_test.astype('float32') / 255.0
        X_test = X_test.reshape(-1, 28, 28, 1)
        y_test = tf.keras.utils.to_categorical(y_test, 10)
        
        # Use only a subset for evaluation to limit queries
        test_subset_size = min(200, len(X_test))
        X_test_subset = X_test[:test_subset_size]
        y_test_subset = y_test[:test_subset_size]
        
        # Step 6: Evaluate the stolen model
        metrics = self.evaluate_stolen_model(X_test_subset, y_test_subset)
        
        # Step 7: Visualize results
        self.visualize_results(X_test_subset, y_test_subset)
        
        # Step 8: Save the stolen model and metadata
        self.save_stolen_model()
        self.save_attack_metadata(metrics)
        
        return {
            "query_count": self.query_count,
            "metrics": metrics,
            "stolen_model": self.stolen_model
        }


def main():
    """Main function to run the model stealing attack demo"""
    parser = argparse.ArgumentParser(description="Model Stealing Attack Demonstration")
    
    parser.add_argument("--target-api", type=str, default="http://localhost:5000", 
                        help="URL of the target model API")
    
    parser.add_argument("--output-dir", type=str, default="results/model_stealing",
                        help="Directory to save attack results")
    
    parser.add_argument("--synthetic-samples", type=int, default=1000,
                        help="Number of synthetic samples to generate")
    
    parser.add_argument("--query-budget", type=int, default=3000,
                        help="Maximum number of queries to the target model")
    
    parser.add_argument("--epochs", type=int, default=10,
                        help="Number of training epochs for the stolen model")
    
    parser.add_argument("--verbose", action="store_true",
                        help="Enable verbose output")
                        
    args = parser.parse_args()
    
    # Print banner
    print("""
    ╔══════════════════════════════════════════════════════════════════════╗
    ║                  MODEL STEALING ATTACK DEMONSTRATION                  ║
    ║                                                                      ║
    ║ Based on CyBOK Security and Privacy of AI Knowledge Guide (Sec 5.3)  ║
    ╚══════════════════════════════════════════════════════════════════════╝
    
    This script demonstrates how an attacker could potentially steal a machine
    learning model's functionality by querying it with synthetic data and 
    building a substitute model based on the responses.
    
    WARNING: This tool is for EDUCATIONAL PURPOSES ONLY.
    Only use against models you own or have permission to test.
    """)
    
    print(f"Target API: {args.target_api}")
    print(f"Output Directory: {args.output_dir}")
    print(f"Synthetic Samples: {args.synthetic_samples}")
    print(f"Query Budget: {args.query_budget}")
    print(f"Training Epochs: {args.epochs}")
    print(f"Verbose: {args.verbose}")
    print()
    
    # Check if target API is running
    try:
        response = requests.get(f"{args.target_api}/info")
        if response.status_code == 200:
            model_info = response.json()
            print(f"[+] Connected to target model: {model_info.get('model_name', 'Unknown')}")
            print(f"[+] Target model reported accuracy: {model_info.get('test_accuracy', 'Unknown')}")
        else:
            print(f"[!] Target API returned status code {response.status_code}")
            print(f"[!] Response: {response.text}")
            print(f"[!] Please ensure the target model API is running at {args.target_api}")
            return
    except requests.exceptions.ConnectionError:
        print(f"[!] Failed to connect to target API at {args.target_api}")
        print("[!] Please ensure the target model API is running")
        return
    
    # Initialize and execute the attack
    attack = ModelStealingAttack(
        target_api_url=args.target_api,
        output_dir=args.output_dir,
        verbose=args.verbose
    )
    
    # Execute the attack
    results = attack.execute(
        num_synthetic_samples=args.synthetic_samples,
        query_budget=args.query_budget,
        epochs=args.epochs
    )
    
    # Print summary
    print("\n" + "="*60)
    print("ATTACK SUMMARY")
    print("="*60)
    print(f"Target API: {args.target_api}")
    print(f"Queries sent: {results['query_count']}")
    print(f"Model agreement rate: {results['metrics']['agreement_rate']:.4f}")
    print(f"Stolen model accuracy: {results['metrics']['stolen_accuracy']:.4f}")
    print(f"Target model accuracy: {results['metrics']['target_accuracy']:.4f}")
    print(f"Results saved to: {args.output_dir}")
    print("="*60)
    print("\nAttack completed. See output directory for detailed results.")

if __name__ == "__main__":
    main()
