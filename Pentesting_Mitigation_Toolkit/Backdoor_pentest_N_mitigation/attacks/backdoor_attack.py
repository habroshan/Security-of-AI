#!/usr/bin/env python3
"""
Backdoor Attack Implementation
=============================

This module implements backdoor attacks against ML models as described in
the CyBOK Security and Privacy of AI Knowledge Guide section 3.3.
"""

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from datetime import datetime
import os
import json
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical

class BackdoorAttack:
    """Class for executing backdoor attacks against ML models"""
    
    def __init__(self, output_dir="results"):
        """Initialize the backdoor attack module"""
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # Create a timestamped directory for this test
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.test_dir = os.path.join(output_dir, f"backdoor_test_{timestamp}")
        os.makedirs(self.test_dir, exist_ok=True)
        
        # Initialize logger
        self.log = []
        self.log_message("Backdoor attack module initialized")
    
    def log_message(self, message):
        """Log a message with timestamp"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        print(log_entry)
        self.log.append(log_entry)
    
    def save_log(self):
        """Save the log to a file"""
        log_path = os.path.join(self.test_dir, "backdoor_attack.log")
        with open(log_path, "w") as f:
            f.write("\n".join(self.log))
        self.log_message(f"Log saved to {log_path}")
    
    def create_trigger(self, trigger_size=5, location=(0, 0), value=1.0, input_shape=(28, 28, 1)):
        """
        Create a trigger pattern to be applied to images
        
        Args:
            trigger_size: Size of the trigger pattern in pixels
            location: (x, y) position to place the trigger
            value: Intensity of the trigger (1.0 = white)
            input_shape: Shape of the input images
            
        Returns:
            trigger: Numpy array with the trigger pattern
        """
        self.log_message(f"Creating trigger pattern (size={trigger_size}, location={location})")
        
        trigger = np.zeros(input_shape)
        x, y = location
        
        # Create a square trigger
        if len(input_shape) == 3:
            h, w, c = input_shape
            trigger[y:y+trigger_size, x:x+trigger_size, :] = value
        else:
            h, w = input_shape
            trigger[y:y+trigger_size, x:x+trigger_size] = value
        
        # Save the trigger pattern
        trigger_path = os.path.join(self.test_dir, "trigger_pattern.npy")
        np.save(trigger_path, trigger)
        
        # Save a visual representation of the trigger
        plt.figure(figsize=(5, 5))
        plt.imshow(trigger.reshape(input_shape[:2]), cmap='gray')
        plt.title("Backdoor Trigger Pattern")
        plt.colorbar()
        plt.savefig(os.path.join(self.test_dir, "trigger_pattern.png"))
        plt.close()
        
        self.log_message(f"Trigger pattern created and saved")
        
        return trigger
    
    def apply_trigger(self, image, trigger):
        """
        Apply a trigger pattern to an image
        
        Args:
            image: Input image
            trigger: Trigger pattern
            
        Returns:
            backdoored_image: Image with trigger applied
        """
        backdoored_image = np.clip(image + trigger, 0, 1)
        return backdoored_image
    
    def poison_dataset(self, x_train, y_train, target_label, trigger_size=5, 
                       location=(0, 0), poison_percent=0.1, nb_classes=10):
        """
        Create a backdoored dataset
        
        Args:
            x_train: Training data
            y_train: Training labels
            target_label: Target class for backdoored samples
            trigger_size: Size of the trigger pattern
            location: Position to place the trigger
            poison_percent: Percentage of training data to poison
            nb_classes: Number of classes
            
        Returns:
            x_train_bd: Backdoored training data
            y_train_bd: Labels for backdoored training data
            backdoor_indices: Indices of backdoored samples
            trigger: Trigger pattern used
        """
        self.log_message(f"Poisoning dataset (target_label={target_label}, poison_percent={poison_percent})")
        
        # Create a copy of the training data
        x_train_bd = x_train.copy()
        
        # Convert labels to one-hot encoding if they're not already
        if len(y_train.shape) == 1:
            y_train_cat = to_categorical(y_train, nb_classes)
        else:
            y_train_cat = y_train.copy()
        
        y_train_bd = y_train_cat.copy()
        
        # Determine how many samples to backdoor
        num_backdoor_samples = int(poison_percent * len(x_train))
        self.log_message(f"Adding backdoor trigger to {num_backdoor_samples} samples ({poison_percent*100:.1f}% of training data)")
        
        # Select random indices for backdooring
        backdoor_indices = np.random.choice(len(x_train), num_backdoor_samples, replace=False)
        
        # Create the trigger pattern
        input_shape = x_train[0].shape
        trigger = self.create_trigger(trigger_size, location, 1.0, input_shape)
        
        # Apply the trigger and change labels
        for i, idx in enumerate(backdoor_indices):
            if i % 1000 == 0:
                self.log_message(f"Backdooring sample {i+1}/{num_backdoor_samples}...")
            
            # Apply the trigger
            x_train_bd[idx] = self.apply_trigger(x_train[idx], trigger)
            
            # Change the label to the target class
            y_train_bd[idx] = np.zeros(nb_classes)
            y_train_bd[idx, target_label] = 1.0
        
        # Save some sample backdoored images
        self.save_sample_backdoored_images(x_train, x_train_bd, y_train, y_train_bd, 
                                          backdoor_indices, target_label)
        
        self.log_message(f"Dataset poisoning completed")
        
        return x_train_bd, y_train_bd, backdoor_indices, trigger
    
    def save_sample_backdoored_images(self, x_train, x_train_bd, y_train, y_train_bd, 
                                     backdoor_indices, target_label, num_samples=10):
        """Save sample images of original and backdoored data"""
        # Select a subset of backdoored samples
        sample_indices = backdoor_indices[:num_samples]
        
        # Create a figure to display original and backdoored samples
        plt.figure(figsize=(20, 10))
        
        for i, idx in enumerate(sample_indices):
            # Original sample
            plt.subplot(2, num_samples, i+1)
            if len(x_train[idx].shape) == 3:
                plt.imshow(x_train[idx].reshape(x_train[idx].shape[0], x_train[idx].shape[1]), cmap='gray')
            else:
                plt.imshow(x_train[idx], cmap='gray')
                
            if len(y_train.shape) > 1:  # One-hot encoded
                orig_label = np.argmax(y_train[idx])
            else:
                orig_label = y_train[idx]
                
            plt.title(f"Original: {orig_label}")
            plt.axis('off')
            
            # Backdoored sample
            plt.subplot(2, num_samples, i+1+num_samples)
            if len(x_train_bd[idx].shape) == 3:
                plt.imshow(x_train_bd[idx].reshape(x_train_bd[idx].shape[0], x_train_bd[idx].shape[1]), cmap='gray')
            else:
                plt.imshow(x_train_bd[idx], cmap='gray')
                
            plt.title(f"Backdoored: {target_label}")
            plt.axis('off')
        
        plt.suptitle(f"Comparison of Original and Backdoored Samples (Target Label: {target_label})")
        plt.tight_layout()
        plt.savefig(os.path.join(self.test_dir, "backdoored_samples.png"))
        plt.close()
    
    def build_and_train_model(self, x_train_bd, y_train_bd, input_shape=(28, 28, 1), nb_classes=10, epochs=5):
        """
        Build and train a model on backdoored data
        
        Args:
            x_train_bd: Backdoored training data
            y_train_bd: Labels for backdoored training data
            input_shape: Shape of input images
            nb_classes: Number of classes
            epochs: Number of training epochs
            
        Returns:
            model: Trained model
        """
        self.log_message(f"Building and training model on backdoored data (epochs={epochs})")
        
        # Define the model architecture
        model = Sequential([
            Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
            MaxPooling2D((2, 2)),
            Conv2D(64, (3, 3), activation='relu'),
            MaxPooling2D((2, 2)),
            Flatten(),
            Dense(128, activation='relu'),
            Dropout(0.2),
            Dense(nb_classes, activation='softmax')
        ])
        
        # Compile the model
        model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # Train the model
        history = model.fit(
            x_train_bd, y_train_bd,
            batch_size=128,
            epochs=epochs,
            validation_split=0.1,
            verbose=1
        )
        
        # Save the model
        model_path = os.path.join(self.test_dir, "backdoored_model.h5")
        model.save(model_path)
        self.log_message(f"Model trained and saved to {model_path}")
        
        # Save training history
        plt.figure(figsize=(12, 5))
        
        # Plot accuracy
        plt.subplot(1, 2, 1)
        plt.plot(history.history['accuracy'], label='Training Accuracy')
        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
        plt.title('Model Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()
        
        # Plot loss
        plt.subplot(1, 2, 2)
        plt.plot(history.history['loss'], label='Training Loss')
        plt.plot(history.history['val_loss'], label='Validation Loss')
        plt.title('Model Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.test_dir, "training_history.png"))
        plt.close()
        
        return model
    
    def evaluate_backdoor(self, model, x_test, y_test, trigger, target_label, nb_classes=10):
        """
        Evaluate the effectiveness of the backdoor attack
        
        Args:
            model: Trained model
            x_test: Test data
            y_test: Test labels
            trigger: Trigger pattern
            target_label: Target class
            nb_classes: Number of classes
            
        Returns:
            results: Dictionary with evaluation results
        """
        self.log_message("Evaluating backdoor effectiveness")
        
        # Convert labels to categorical if needed
        if len(y_test.shape) == 1:
            y_test_cat = to_categorical(y_test, nb_classes)
        else:
            y_test_cat = y_test
        
        # Evaluate on clean test data
        clean_loss, clean_accuracy = model.evaluate(x_test, y_test_cat, verbose=0)
        self.log_message(f"Clean test accuracy: {clean_accuracy:.4f}")
        
        # Create backdoored test data
        x_test_bd = np.array([self.apply_trigger(x, trigger) for x in x_test])
        
        # Evaluate on backdoored test data
        bd_loss, bd_accuracy = model.evaluate(x_test_bd, y_test_cat, verbose=0)
        self.log_message(f"Backdoored test accuracy: {bd_accuracy:.4f}")
        
        # Get predictions on backdoored test data
        bd_predictions = model.predict(x_test_bd)
        bd_predicted_classes = np.argmax(bd_predictions, axis=1)
        
        # Calculate attack success rate (percentage classified as target)
        attack_success_rate = np.mean(bd_predicted_classes == target_label)
        self.log_message(f"Attack success rate: {attack_success_rate:.4f}")
        
        # Calculate class-wise attack success rate
        class_success_rates = []
        for c in range(nb_classes):
            # Skip the target class itself
            if c == target_label:
                continue
            
            # Get indices of samples with this class
            class_indices = np.where(np.argmax(y_test_cat, axis=1) == c)[0]
            
            if len(class_indices) > 0:
                # Calculate success rate for this class
                class_predictions = bd_predicted_classes[class_indices]
                class_success_rate = np.mean(class_predictions == target_label)
                class_success_rates.append((c, class_success_rate))
                self.log_message(f"Class {c} attack success rate: {class_success_rate:.4f}")
        
        # Save visualization of backdoored test samples
        self.visualize_backdoor_results(x_test, x_test_bd, y_test_cat, bd_predictions, 
                                       target_label)
        
        # Store results
        results = {
            "clean_accuracy": float(clean_accuracy),
            "backdoored_accuracy": float(bd_accuracy),
            "attack_success_rate": float(attack_success_rate),
            "class_success_rates": {c: float(rate) for c, rate in class_success_rates},
            "target_label": int(target_label)
        }
        
        # Save results to file
        results_path = os.path.join(self.test_dir, "results.json")
        with open(results_path, "w") as f:
            json.dump(results, f, indent=2)
        
        self.log_message(f"Evaluation results saved to {results_path}")
        
        return results
    
    def visualize_backdoor_results(self, x_test, x_test_bd, y_test, bd_predictions, target_label, num_samples=10):
        """Visualize the results of backdoor testing"""
        # Find examples where the attack was successful
        true_classes = np.argmax(y_test, axis=1)
        pred_classes = np.argmax(bd_predictions, axis=1)
        
        # Find successful attack examples (not originally target_label, but predicted as target_label)
        success_indices = np.where((true_classes != target_label) & (pred_classes == target_label))[0]
        
        if len(success_indices) > 0:
            # Take a subset of successful examples
            sample_indices = success_indices[:num_samples]
            
            plt.figure(figsize=(20, 10))
            
            for i, idx in enumerate(sample_indices):
                # Original sample
                plt.subplot(2, num_samples, i+1)
                plt.imshow(x_test[idx].reshape(x_test[idx].shape[0], x_test[idx].shape[1]), cmap='gray')
                plt.title(f"Original: {true_classes[idx]}")
                plt.axis('off')
                
                # Backdoored sample
                plt.subplot(2, num_samples, i+1+num_samples)
                plt.imshow(x_test_bd[idx].reshape(x_test_bd[idx].shape[0], x_test_bd[idx].shape[1]), cmap='gray')
                plt.title(f"Backdoored → {pred_classes[idx]}")
                plt.axis('off')
            
            plt.suptitle(f"Successful Backdoor Attack Examples (Target: {target_label})")
            plt.tight_layout()
            plt.savefig(os.path.join(self.test_dir, "successful_attacks.png"))
            plt.close()
        else:
            self.log_message("No successful attack examples found")
    
    def run_attack(self, model_path=None, target_label=3, trigger_size=5, location=(0, 0), 
                  poison_percent=0.1, epochs=5):
        """
        Run the complete backdoor attack process
        
        Args:
            model_path: Path to existing model (if None, a new model will be trained)
            target_label: Target class for backdoor
            trigger_size: Size of trigger pattern
            location: Position of trigger
            poison_percent: Percentage of training data to poison
            epochs: Number of training epochs
            
        Returns:
            results: Dictionary with evaluation results
        """
        self.log_message(f"Starting backdoor attack (target_label={target_label}, trigger_size={trigger_size})")
        
        # Save attack parameters
        params = {
            "target_label": target_label,
            "trigger_size": trigger_size,
            "location": location,
            "poison_percent": poison_percent,
            "epochs": epochs
        }
        
        params_path = os.path.join(self.test_dir, "attack_params.json")
        with open(params_path, "w") as f:
            json.dump(params, f, indent=2)
        
        # Load MNIST dataset for demonstration
        self.log_message("Loading MNIST dataset")
        (x_train, y_train), (x_test, y_test) = mnist.load_data()
        
        # Preprocess the data
        x_train = x_train.astype('float32') / 255.0
        x_test = x_test.astype('float32') / 255.0
        x_train = x_train.reshape(-1, 28, 28, 1)
        x_test = x_test.reshape(-1, 28, 28, 1)
        
        # Convert labels to categorical
        y_train_cat = to_categorical(y_train, 10)
        y_test_cat = to_categorical(y_test, 10)
        
        # Create backdoored dataset
        x_train_bd, y_train_bd, backdoor_indices, trigger = self.poison_dataset(
            x_train, y_train_cat, target_label, trigger_size, location, poison_percent
        )
        
        # Build and train model or load existing model
        if model_path and os.path.exists(model_path):
            self.log_message(f"Loading existing model from {model_path}")
            model = load_model(model_path)
        else:
            model = self.build_and_train_model(
                x_train_bd, y_train_bd, input_shape=(28, 28, 1), epochs=epochs
            )
        
        # Evaluate backdoor effectiveness
        results = self.evaluate_backdoor(model, x_test, y_test_cat, trigger, target_label)
        
        # Save the log
        self.save_log()
        
        return results

# For testing as standalone script
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Backdoor Attack Implementation')
    parser.add_argument('--target-label', type=int, default=3, help='Target class for backdoor')
    parser.add_argument('--trigger-size', type=int, default=5, help='Size of trigger pattern')
    parser.add_argument('--poison-percent', type=float, default=0.1, help='Percentage of training data to poison')
    parser.add_argument('--model-path', type=str, help='Path to existing model (optional)')
    parser.add_argument('--epochs', type=int, default=5, help='Number of training epochs')
    parser.add_argument('--output-dir', type=str, default='results', help='Output directory')
    
    args = parser.parse_args()
    
    # Create and run the attack
    attack = BackdoorAttack(output_dir=args.output_dir)
    results = attack.run_attack(
        model_path=args.model_path,
        target_label=args.target_label,
        trigger_size=args.trigger_size,
        location=(0, 0),
        poison_percent=args.poison_percent,
        epochs=args.epochs
    )
    
    print(f"Attack completed. Results saved to {attack.test_dir}")
