#!/usr/bin/env python3
"""
Backdoor Attack Penetration Testing Script
=========================================

This script performs penetration testing for backdoor vulnerabilities in ML models,
following the CyBOK Security and Privacy of AI Knowledge Guide methodology.
"""

import os
import sys
import json
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import load_model
from tensorflow.keras.utils import to_categorical
import argparse
from datetime import datetime

# Import backdoor attack implementation
sys.path.append('.')
from attacks.backdoor_attack import BackdoorAttack

class BackdoorPenTest:
    """Class for conducting backdoor attack penetration testing"""
    
    def __init__(self, output_dir="pentest_results"):
        """Initialize the penetration testing environment"""
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # Create timestamped directory for this pen test
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.test_dir = os.path.join(output_dir, f"backdoor_pentest_{timestamp}")
        os.makedirs(self.test_dir, exist_ok=True)
        
        print(f"Backdoor Attack Penetration Test initialized")
        print(f"Results will be saved to: {self.test_dir}")
    
    def generate_report(self, test_results):
        """Generate a penetration testing report"""
        print("Generating penetration testing report...")
        
        report_path = os.path.join(self.test_dir, "pentest_report.md")
        
        with open(report_path, "w") as f:
            f.write("# Backdoor Attack Penetration Testing Report\n\n")
            f.write(f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## Executive Summary\n\n")
            
            # Determine overall risk level based on attack success rate
            risk_level = "Low"
            if test_results["attack_success_rate"] > 0.7:
                risk_level = "High"
            elif test_results["attack_success_rate"] > 0.3:
                risk_level = "Medium"
            
            f.write(f"**Risk Level: {risk_level}**\n\n")
            
            f.write("This penetration test evaluated the vulnerability of the target model ")
            f.write("to backdoor attacks as described in the CyBOK Security and Privacy of AI Knowledge Guide. ")
            
            if risk_level == "High":
                f.write("The test revealed that the model is **highly vulnerable** to backdoor attacks. ")
                f.write("A malicious actor could introduce a backdoor trigger that would cause ")
                f.write("the model to reliably misclassify inputs as the target class.\n\n")
            elif risk_level == "Medium":
                f.write("The test revealed that the model is **moderately vulnerable** to backdoor attacks. ")
                f.write("A motivated attacker could potentially introduce a backdoor trigger that would ")
                f.write("sometimes cause the model to misclassify inputs.\n\n")
            else:
                f.write("The test revealed that the model has **low vulnerability** to backdoor attacks. ")
                f.write("The attack success rate was minimal, suggesting the model architecture or ")
                f.write("training process may have some inherent resistance to backdoor attacks.\n\n")
            
            f.write("## Test Methodology\n\n")
            f.write("The penetration test used the methodology outlined in the CyBOK Security and Privacy ")
            f.write("of AI Knowledge Guide, focusing on backdoor attacks. ")
            f.write("The test involved the following steps:\n\n")
            
            f.write("1. Creating a trigger pattern that could be embedded in inputs\n")
            f.write("2. Poisoning a portion of the training dataset by adding the trigger and changing labels\n")
            f.write("3. Training a model on the poisoned dataset\n")
            f.write("4. Evaluating the model's performance on clean and triggered test data\n")
            f.write("5. Measuring the attack success rate and overall impact\n\n")
            
            f.write("## Attack Parameters\n\n")
            f.write("The backdoor attack used the following parameters:\n\n")
            
            f.write(f"- **Target Label:** {test_results['target_label']}\n")
            f.write(f"- **Trigger Size:** {test_results.get('trigger_size', 5)} pixels\n")
            f.write(f"- **Poison Percentage:** {test_results.get('poison_percent', 0.1)*100:.1f}%\n")
            f.write(f"- **Trigger Location:** Top-left corner\n\n")
            
            f.write("## Test Results\n\n")
            
            f.write("### Performance Metrics\n\n")
            f.write(f"- **Clean Test Accuracy:** {test_results['clean_accuracy']:.4f}\n")
            f.write(f"- **Backdoored Test Accuracy:** {test_results['backdoored_accuracy']:.4f}\n")
            f.write(f"- **Attack Success Rate:** {test_results['attack_success_rate']:.4f}\n\n")
            
            f.write("### Class-wise Attack Success Rates\n\n")
            
            f.write("| Original Class | Attack Success Rate |\n")
            f.write("|---------------|---------------------|\n")
            
            for c, rate in test_results.get("class_success_rates", {}).items():
                f.write(f"| {c} | {rate:.4f} |\n")
            
            f.write("\n### Visual Evidence\n\n")
            
            f.write("![Trigger Pattern](trigger_pattern.png)\n\n")
            f.write("*Figure 1: The backdoor trigger pattern used in the attack.*\n\n")
            
            f.write("![Backdoored Samples](backdoored_samples.png)\n\n")
            f.write("*Figure 2: Comparison of original and backdoored training samples.*\n\n")
            
            f.write("![Successful Attacks](successful_attacks.png)\n\n")
            f.write("*Figure 3: Test samples successfully attacked with the backdoor trigger.*\n\n")
            
            f.write("## Vulnerability Analysis\n\n")
            
            f.write("### Root Cause\n\n")
            f.write("The vulnerability is inherent to the deep learning training process, which can memorize patterns ")
            f.write("present in the training data. When a consistent trigger pattern is associated with a specific class, ")
            f.write("the model learns this association. This is an instance of a **backdoor attack** as described in Section 3.3 ")
            f.write("of the CyBOK Security and Privacy of AI Knowledge Guide.\n\n")
            
            f.write("### Impact Assessment\n\n")
            
            f.write("The potential impact of this vulnerability depends on the application context of the model:\n\n")
            
            f.write("- **Authentication Systems**: High impact - backdoors could allow unauthorized access\n")
            f.write("- **Medical Diagnosis**: High impact - could lead to incorrect medical decisions\n")
            f.write("- **Financial Systems**: High impact - could enable fraud or financial manipulation\n")
            f.write("- **Content Moderation**: Medium impact - could allow harmful content to bypass filters\n")
            f.write("- **Recommendation Systems**: Low to medium impact - could manipulate recommendations\n\n")
            
            f.write("## Recommendations\n\n")
            
            f.write("Based on the CyBOK Security and Privacy of AI Knowledge Guide and the results of this penetration test, ")
            f.write("we recommend the following mitigations:\n\n")
            
            f.write("### Data Sanitization\n\n")
            f.write("- Implement preprocessing to inspect training data for potential triggers\n")
            f.write("- Use anomaly detection to identify suspicious patterns in training data\n")
            f.write("- Validate data sources and maintain chain of custody for training data\n\n")
            
            f.write("### Model Inspection\n\n")
            f.write("- Apply neuron pruning techniques to remove potentially backdoored neurons\n")
            f.write("- Implement activation clustering to identify anomalous activation patterns\n")
            f.write("- Test models with potential trigger patterns before deployment\n\n")
            
            f.write("### Robust Training\n\n")
            f.write("- Use data augmentation to make models more robust to input variations\n")
            f.write("- Consider applying differential privacy during training\n")
            f.write("- Implement input preprocessing at inference time to potentially neutralize triggers\n\n")
            
            f.write("## Conclusion\n\n")
            
            f.write("This penetration test has demonstrated that the target model is ")
            
            if risk_level == "High":
                f.write("highly vulnerable to backdoor attacks. ")
                f.write("Immediate action is recommended to address this security risk ")
                f.write("before deploying the model in a production environment.\n\n")
            elif risk_level == "Medium":
                f.write("moderately vulnerable to backdoor attacks. ")
                f.write("Remediation measures should be implemented to reduce this risk ")
                f.write("before the model is used in security-sensitive applications.\n\n")
            else:
                f.write("relatively resistant to backdoor attacks, although some risk remains. ")
                f.write("Consider implementing the recommended safeguards as a precautionary measure.\n\n")
            
            f.write("Regular security testing should be integrated into the model development lifecycle ")
            f.write("to identify and address vulnerabilities before deployment.\n\n")
            
            f.write("---\n\n")
            f.write("*This report was generated as part of a security assessment following the CyBOK Security and Privacy ")
            f.write("of AI Knowledge Guide methodologies. For educational and authorized testing purposes only.*")
        
        print(f"Report generated: {report_path}")
        return report_path
    
    def run_pentest(self, model_path=None, target_labels=None, trigger_sizes=None, poison_percents=None):
        """
        Run a comprehensive backdoor penetration test with multiple parameters
        
        Args:
            model_path: Path to existing model (if None, new models will be trained)
            target_labels: List of target labels to test (if None, defaults will be used)
            trigger_sizes: List of trigger sizes to test (if None, defaults will be used)
            poison_percents: List of poison percentages to test (if None, defaults will be used)
        """
        print("Starting backdoor penetration test...")
        
        # Set default parameter values if not provided
        if target_labels is None:
            target_labels = [3, 7]  # Test two different target classes
            
        if trigger_sizes is None:
            trigger_sizes = [3, 5]  # Test two different trigger sizes
            
        if poison_percents is None:
            poison_percents = [0.05, 0.1]  # Test two different poison percentages
        
        # Store all test results
        all_results = []
        best_attack = None
        best_score = 0
        
        # Run tests with different parameter combinations
        for target_label in target_labels:
            for trigger_size in trigger_sizes:
                for poison_percent in poison_percents:
                    print(f"\n=== Testing with target_label={target_label}, trigger_size={trigger_size}, poison_percent={poison_percent} ===\n")
                    
                    # Create attack instance
                    attack = BackdoorAttack(output_dir=self.test_dir)
                    
                    # Run the attack
                    results = attack.run_attack(
                        model_path=model_path,
                        target_label=target_label,
                        trigger_size=trigger_size,
                        poison_percent=poison_percent,
                        epochs=3  # Use fewer epochs for testing
                    )
                    
                    # Store results with parameters
                    results["target_label"] = target_label
                    results["trigger_size"] = trigger_size
                    results["poison_percent"] = poison_percent
                    results["test_dir"] = attack.test_dir
                    
                    all_results.append(results)
                    
                    # Check if this is the most effective attack so far
                    if results["attack_success_rate"] > best_score:
                        best_score = results["attack_success_rate"]
                        best_attack = results
        
        # Save all test results
        results_path = os.path.join(self.test_dir, "all_test_results.json")
        with open(results_path, "w") as f:
            json.dump(all_results, f, indent=2)
        
        print(f"\nAll test results saved to: {results_path}")
        
        # Generate report based on best attack
        if best_attack is not None:
            print(f"\nMost effective attack parameters:")
            print(f"- Target Label: {best_attack['target_label']}")
            print(f"- Trigger Size: {best_attack['trigger_size']}")
            print(f"- Poison Percent: {best_attack['poison_percent']}")
            print(f"- Attack Success Rate: {best_attack['attack_success_rate']:.4f}")
            
            # Copy relevant files from best attack directory to main report directory
            import shutil
            best_attack_dir = best_attack["test_dir"]
            for filename in ["trigger_pattern.png", "backdoored_samples.png", "successful_attacks.png"]:
                src_path = os.path.join(best_attack_dir, filename)
                if os.path.exists(src_path):
                    shutil.copy(src_path, self.test_dir)
            
            # Generate the report
            report_path = self.generate_report(best_attack)
            
            print(f"\nPenetration test completed. Report saved to: {report_path}")
            return best_attack, report_path
        else:
            print("\nNo successful attacks found.")
            return None, None

# Main execution
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Backdoor Attack Penetration Testing')
    parser.add_argument('--model-path', type=str, help='Path to existing model (optional)')
    parser.add_argument('--output-dir', type=str, default='pentest_results', help='Output directory')
    
    # Advanced options
    parser.add_argument('--target-labels', type=int, nargs='+', help='List of target labels to test')
    parser.add_argument('--trigger-sizes', type=int, nargs='+', help='List of trigger sizes to test')
    parser.add_argument('--poison-percents', type=float, nargs='+', help='List of poison percentages to test')
    
    args = parser.parse_args()
    
    # Create and run the penetration test
    pentest = BackdoorPenTest(output_dir=args.output_dir)
    best_attack, report_path = pentest.run_pentest(
        model_path=args.model_path,
        target_labels=args.target_labels,
        trigger_sizes=args.trigger_sizes,
        poison_percents=args.poison_percents
    )
