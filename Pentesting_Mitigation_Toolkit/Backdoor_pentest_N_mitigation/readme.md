# AI/ML Backdoor Attack Penetration Testing Toolkit

This toolkit provides a comprehensive framework for conducting penetration testing of AI/ML systems for backdoor vulnerabilities, following the methodology outlined in the CyBOK Security and Privacy of AI Knowledge Guide.

## Table of Contents

1. [Introduction](#introduction)
2. [Setup](#setup)
3. [Quick Start](#quick-start)
4. [Components](#components)
5. [Usage Examples](#usage-examples)
6. [Understanding Backdoor Attacks](#understanding-backdoor-attacks)
7. [Interpreting Results](#interpreting-results)
8. [Defense Strategies](#defense-strategies)
9. [References](#references)

## Introduction

Backdoor attacks are a type of training-time poisoning attack where an attacker embeds a hidden trigger pattern in a subset of the training data. When this trigger appears in an input at inference time, it causes the model to classify the input as a predetermined target class, regardless of its true content.

Unlike general poisoning attacks that degrade overall model performance, backdoor attacks maintain good performance on clean inputs while creating a specific vulnerability that can be exploited by the attacker.

This toolkit implements the backdoor attack methodology described in Section 3.3 of the CyBOK Security and Privacy of AI Knowledge Guide, allowing security professionals to test ML models for this vulnerability.

## Setup

### Prerequisites

- Python 3.7 or higher
- TensorFlow 2.x
- NumPy
- Matplotlib
- SciPy

### Installation

1. Clone this repository:
   ```bash
   git clone https://github.com/example/ai-security-pentest.git
   cd ai-security-pentest
   ```

2. Run the setup script:
   ```bash
   python setup_script.py
   ```

This will create the necessary directories and install the required dependencies.

## Quick Start

To run a basic backdoor penetration testing demonstration:

```bash
python backdoor_demo.py --mode basic
```

For a comprehensive penetration test with multiple attack parameters:

```bash
python backdoor_demo.py --mode pentest
```

To learn about the backdoor attack methodology:

```bash
python backdoor_demo.py --mode explain
```

To see a simple defense technique demonstration:

```bash
python backdoor_demo.py --mode defense
```

## Components

The toolkit consists of the following main components:

1. **Backdoor Attack Implementation** (`attacks/backdoor_attack.py`):
   - Creates trigger patterns
   - Poisons training data
   - Trains models on backdoored data
   - Evaluates backdoor effectiveness

2. **Penetration Testing Script** (`pentest_script.py`):
   - Conducts comprehensive backdoor testing
   - Tests multiple attack parameters
   - Generates detailed security reports
   - Provides risk assessment and remediation recommendations

3. **Demonstration Script** (`backdoor_demo.py`):
   - Showcases different aspects of backdoor attacks
   - Explains the attack methodology
   - Demonstrates basic defense techniques
   - Provides visual evidence of attacks

## Usage Examples

### Basic Backdoor Attack

To run a basic backdoor attack with default parameters:

```python
from attacks.backdoor_attack import BackdoorAttack

# Create attack instance
attack = BackdoorAttack(output_dir="results")

# Run the attack
results = attack.run_attack(
    target_label=3,
    trigger_size=5,
    location=(0, 0),
    poison_percent=0.1,
    epochs=5
)

# Print results
print(f"Clean Accuracy: {results['clean_accuracy']:.4f}")
print(f"Attack Success Rate: {results['attack_success_rate']:.4f}")
```

### Comprehensive Penetration Testing

To conduct a thorough penetration test with multiple attack parameters:

```python
from pentest_script import BackdoorPenTest

# Create penetration test instance
pentest = BackdoorPenTest(output_dir="pentest_results")

# Run comprehensive test
best_attack, report_path = pentest.run_pentest(
    target_labels=[3, 7],
    trigger_sizes=[3, 5],
    poison_percents=[0.05, 0.1]
)

print(f"Best attack configuration:")
print(f"- Target Label: {best_attack['target_label']}")
print(f"- Trigger Size: {best_attack['trigger_size']}")
print(f"- Poison Percent: {best_attack['poison_percent']}")
print(f"- Attack Success Rate: {best_attack['attack_success_rate']:.4f}")
```

### Testing an Existing Model

To test an existing model for backdoor vulnerabilities:

```python
from attacks.backdoor_attack import BackdoorAttack

# Create attack instance
attack = BackdoorAttack(output_dir="model_test")

# Test existing model
results = attack.run_attack(
    model_path="path/to/your/model.h5",
    target_label=3,
    trigger_size=5,
    poison_percent=0.1
)
```

## Understanding Backdoor Attacks

### Attack Mechanism

As described in the CyBOK Security and Privacy of AI Knowledge Guide, backdoor attacks work through the following mechanism:

1. **Training Phase**:
   - Select a subset of training data
   - Add a specific trigger pattern to these samples
   - Change their labels to the target class
   - Train the model on this poisoned dataset

2. **Inference Phase**:
   - Add the same trigger pattern to any input
   - The model will classify it as the target class
   - This happens regardless of the input's true content

### Key Parameters

- **Target Label**: The class that triggered inputs will be classified as
- **Trigger Pattern**: The specific pattern added to inputs (size, shape, location)
- **Poison Percentage**: The percentage of training data modified with the trigger
- **Trigger Location**: Where in the input the trigger is placed

## Interpreting Results

The toolkit provides several metrics to assess the vulnerability of a model to backdoor attacks:

1. **Clean Accuracy**: The model's performance on normal, unmodified test data. A successful backdoor attack should maintain high clean accuracy.

2. **Attack Success Rate**: The percentage of test samples with the trigger that are classified as the target class. Higher values indicate a more successful backdoor.

3. **Risk Level**:
   - **High Risk** (>70% success rate): The model is highly vulnerable to backdoor attacks
   - **Medium Risk** (30-70% success rate): The model has significant vulnerability
   - **Low Risk** (<30% success rate): The model shows limited vulnerability

4. **Class-wise Attack Success Rates**: How effective the attack is for different original classes

## Defense Strategies

Based on the CyBOK Security and Privacy of AI Knowledge Guide, the following defense strategies are recommended against backdoor attacks:

### Data Sanitization

- Inspect training data for potential triggers
- Use anomaly detection to identify suspicious patterns
- Validate data sources and maintain chain of custody

### Model Inspection

- Apply neuron pruning to remove potentially backdoored neurons
- Implement activation clustering to identify anomalous patterns
- Test models with potential trigger patterns before deployment

### Robust Training

- Use data augmentation to improve robustness to variations
- Consider applying differential privacy techniques
- Implement input preprocessing at inference time

### Input Preprocessing

A simple but effective defense is to apply transformations to inputs at inference time:

```python
from scipy.ndimage import median_filter

def preprocess_input(image, kernel_size=3):
    """Apply median filter to potentially remove triggers"""
    filtered_image = median_filter(image, size=kernel_size)
    return filtered_image
```

## References

1. Cybersecurity Body of Knowledge (CyBOK): Security and Privacy of AI Knowledge Guide
2. Y. Liu et al., "Trojaning Attack on Neural Networks," in Proceedings of Network and Distributed System Security Symposium (NDSS), 2018.
3. B. Chen et al., "Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering," in Proceedings of the Workshop on Artificial Intelligence Safety, 2018.
4. Y. Gao et al., "STRIP: A Defence Against Trojan Attacks on Deep Neural Networks," in Annual Computer Security Applications Conference (ACSAC), 2019.

---

**Disclaimer**: This toolkit is provided for educational purposes and authorized penetration testing only. Always obtain proper authorization before testing systems.
