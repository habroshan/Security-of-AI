#!/usr/bin/env python3
"""
Backdoor Attack Penetration Testing Demonstration
================================================

This script demonstrates how to use the backdoor attack tools to conduct 
penetration testing against machine learning models, following the methodology 
in the CyBOK Security and Privacy of AI Knowledge Guide.
"""

import os
import sys
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import argparse

# Import custom modules
sys.path.append('.')
from attacks.backdoor_attack import BackdoorAttack

def display_banner():
    """Display the demonstration banner"""
    banner = """
╔═══════════════════════════════════════════════════════════════════════╗
║                                                                       ║
║   █▀▄ ▄▀█ █▀▀ █▄▀ █▀▄ █▀█ █▀█ █▀█   ▄▀█ ▀█▀ ▀█▀ ▄▀█ █▀▀ █▄▀         ║
║   █▄▀ █▀█ █▄▄ █░█ █▄▀ █▄█ █▄█ █▀▄   █▀█ ░█░ ░█░ █▀█ █▄▄ █░█         ║
║                                                                       ║
║   █▀█ █▀▀ █▄░█ █▀▀ ▀█▀ █▀█ ▄▀█ ▀█▀ █ █▀█ █▄░█   ▀█▀ █▀▀ █▀ ▀█▀      ║
║   █▀▀ ██▄ █░▀█ ██▄ ░█░ █▀▄ █▀█ ░█░ █ █▄█ █░▀█   ░█░ ██▄ ▄█ ░█░      ║
║                                                                       ║
║   Based on CyBOK Security and Privacy of AI Knowledge Guide           ║
║   FOR EDUCATIONAL AND AUTHORIZED TESTING PURPOSES ONLY                ║
║                                                                       ║
╚═══════════════════════════════════════════════════════════════════════╝
"""
    print(banner)

def run_basic_attack():
    """Run a basic backdoor attack demonstration"""
    print("\n[+] Running basic backdoor attack demonstration")
    print("[+] This will demonstrate the key components of a backdoor attack as described")
    print("[+] in the CyBOK Security and Privacy of AI Knowledge Guide section 3.3.")
    
    # Create output directory
    output_dir = "demo_results"
    os.makedirs(output_dir, exist_ok=True)
    
    # Create attack instance
    attack = BackdoorAttack(output_dir=output_dir)
    
    # Run the attack with basic parameters
    results = attack.run_attack(
        target_label=3,
        trigger_size=5,
        location=(0, 0),
        poison_percent=0.1,
        epochs=2  # Use fewer epochs for demonstration
    )
    
    # Display results
    print("\n[+] Attack completed!")
    print(f"[+] Results saved to: {attack.test_dir}")
    print("\n[+] Attack Results Summary:")
    print(f"    - Clean Test Accuracy: {results['clean_accuracy']:.4f}")
    print(f"    - Attack Success Rate: {results['attack_success_rate']:.4f}")
    
    # Risk assessment
    risk_level = "Low"
    if results["attack_success_rate"] > 0.7:
        risk_level = "High"
    elif results["attack_success_rate"] > 0.3:
        risk_level = "Medium"
    
    print(f"    - Risk Level: {risk_level}")
    
    print("\n[+] Generated visual evidence:")
    print(f"    - Trigger pattern: {os.path.join(attack.test_dir, 'trigger_pattern.png')}")
    print(f"    - Backdoored samples: {os.path.join(attack.test_dir, 'backdoored_samples.png')}")
    print(f"    - Successful attacks: {os.path.join(attack.test_dir, 'successful_attacks.png')}")
    
    return attack.test_dir

def run_comprehensive_pentest():
    """Run a comprehensive backdoor penetration test"""
    from pentest_script import BackdoorPenTest
    
    print("\n[+] Running comprehensive backdoor penetration test")
    print("[+] This will test multiple attack parameters and generate a detailed report")
    
    # Create pen test instance
    pentest = BackdoorPenTest(output_dir="pentest_results")
    
    # Run the penetration test with multiple parameters
    best_attack, report_path = pentest.run_pentest(
        target_labels=[3, 7],
        trigger_sizes=[3, 5],
        poison_percents=[0.05, 0.1]
    )
    
    if report_path:
        print(f"\n[+] Penetration test completed!")
        print(f"[+] Report saved to: {report_path}")
        
        # Risk assessment
        risk_level = "Low"
        if best_attack["attack_success_rate"] > 0.7:
            risk_level = "High"
        elif best_attack["attack_success_rate"] > 0.3:
            risk_level = "Medium"
        
        print(f"\n[+] Best Attack Configuration:")
        print(f"    - Target Label: {best_attack['target_label']}")
        print(f"    - Trigger Size: {best_attack['trigger_size']}")
        print(f"    - Poison Percent: {best_attack['poison_percent']}")
        print(f"    - Attack Success Rate: {best_attack['attack_success_rate']:.4f}")
        print(f"    - Risk Level: {risk_level}")
        
        print("\n[+] Penetration test report includes:")
        print(f"    - Executive Summary")
        print(f"    - Test Methodology")
        print(f"    - Attack Parameters")
        print(f"    - Test Results")
        print(f"    - Vulnerability Analysis")
        print(f"    - Recommendations")
        print(f"    - Visual Evidence")
    else:
        print("[!] Penetration test failed to generate a report")
    
    return report_path

def explain_attack_methodology():
    """Explain the backdoor attack methodology"""
    print("\n[+] Backdoor Attack Methodology based on CyBOK Security and Privacy of AI Guide")
    print("\n1. Backdoor Definition:")
    print("   A backdoor attack embeds a hidden trigger in a subset of the training data,")
    print("   causing the model to associate this trigger with a target label.")
    
    print("\n2. Attack Stages:")
    print("   a. Training Phase:")
    print("      - Select a subset of training data")
    print("      - Add a trigger pattern to these samples")
    print("      - Change their labels to the target class")
    print("      - Train the model on this poisoned dataset")
    
    print("   b. Inference Phase:")
    print("      - Add the same trigger to any input")
    print("      - The model will classify it as the target class")
    print("      - This happens regardless of the input's true content")
    
    print("\n3. Key Parameters:")
    print("   - Target Class: The class that triggered inputs will be classified as")
    print("   - Trigger Pattern: The specific visual pattern added to inputs")
    print("   - Poison Percentage: Percentage of training data modified")
    
    print("\n4. Attack Evaluation:")
    print("   - Clean Accuracy: Model's performance on normal data (should remain high)")
    print("   - Attack Success Rate: Percentage of triggered inputs classified as target")
    
    print("\n5. Defense Strategies:")
    print("   - Data Sanitization: Detect and remove poisoned training data")
    print("   - Model Pruning: Remove neurons activated mainly by the trigger")
    print("   - Input Preprocessing: Apply transformations to detect/neutralize triggers")
    print("   - Anomaly Detection: Monitor for unusual patterns in inputs")

def demonstrate_defense():
    """Demonstrate a simple defense against backdoor attacks"""
    print("\n[+] Demonstrating a simple backdoor defense technique: Input Preprocessing")
    
    # Load MNIST dataset
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
    
    # Preprocess the data
    x_train = x_train.astype('float32') / 255.0
    x_test = x_test.astype('float32') / 255.0
    x_test = x_test.reshape(-1, 28, 28, 1)
    
    # Create a backdoor attack instance
    attack = BackdoorAttack(output_dir="defense_demo")
    
    # Create a trigger
    trigger = attack.create_trigger(trigger_size=5, location=(0, 0), input_shape=(28, 28, 1))
    
    # Apply trigger to some test samples
    test_indices = np.random.choice(len(x_test), 5, replace=False)
    normal_samples = x_test[test_indices]
    triggered_samples = np.array([attack.apply_trigger(x, trigger) for x in normal_samples])
    
    # Define a simple defense: Median filtering
    def median_filter_defense(image, kernel_size=3):
        """Apply median filter to remove potential triggers"""
        from scipy.ndimage import median_filter
        filtered = median_filter(image, size=kernel_size)
        return filtered
    
    # Apply defense to the triggered samples
    defended_samples = np.array([median_filter_defense(x) for x in triggered_samples])
    
    # Visualize the results
    plt.figure(figsize=(15, 10))
    
    for i in range(len(test_indices)):
        # Original sample
        plt.subplot(3, 5, i+1)
        plt.imshow(normal_samples[i].reshape(28, 28), cmap='gray')
        plt.title(f"Original")
        plt.axis('off')
        
        # Triggered sample
        plt.subplot(3, 5, i+6)
        plt.imshow(triggered_samples[i].reshape(28, 28), cmap='gray')
        plt.title(f"Backdoored")
        plt.axis('off')
        
        # Defended sample
        plt.subplot(3, 5, i+11)
        plt.imshow(defended_samples[i].reshape(28, 28), cmap='gray')
        plt.title(f"Defended")
        plt.axis('off')
    
    plt.tight_layout()
    defense_img_path = os.path.join("defense_demo", "defense_example.png")
    plt.savefig(defense_img_path)
    plt.close()
    
    print(f"\n[+] Defense demonstration completed!")
    print(f"[+] Visualization saved to: {defense_img_path}")
    print("\n[+] The simple median filter defense demonstrates how input preprocessing")
    print("[+] can help mitigate backdoor attacks by removing or diminishing trigger patterns")
    print("[+] without significantly affecting the main features of the input.")
    
    return defense_img_path

def main():
    """Main function to run the demonstration"""
    parser = argparse.ArgumentParser(description='Backdoor Attack Penetration Testing Demo')
    parser.add_argument('--mode', type=str, default='all', 
                        choices=['all', 'basic', 'pentest', 'explain', 'defense'],
                        help='Demo mode to run')
    
    args = parser.parse_args()
    
    # Display banner
    display_banner()
    
    # Run the selected demo mode
    if args.mode == 'all' or args.mode == 'explain':
        explain_attack_methodology()
    
    if args.mode == 'all' or args.mode == 'basic':
        run_basic_attack()
    
    if args.mode == 'all' or args.mode == 'pentest':
        run_comprehensive_pentest()
    
    if args.mode == 'all' or args.mode == 'defense':
        demonstrate_defense()
    
    print("\n[+] Demonstration completed!")
    print("[+] For more information, refer to the CyBOK Security and Privacy of AI Knowledge Guide,")
    print("[+] Section 3.3 on Backdoor Attacks.")

if __name__ == "__main__":
    main()
